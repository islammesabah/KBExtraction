# Nvidia Container and Python versions
NVIDIA_CONTAINER_VERSION=24.04
PYTHON_VERSION=3.10
CUDA_VERSION=12.4

# Pytorch
TORCH_SHOW_SDP_KERNELS=1

# Hugging Face configuration
HF_TOKEN=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
HF_HOME=/netscratch/abuali/_cache/huggingface
# NEW: persistent venv and pip cache on /netscratch
VENV_DIR=/netscratch/abuali/envs/KBExtract
PIP_CACHE_DIR=/netscratch/abuali/pip-cache
TRANSFORMERS_CACHE=$HF_HOME/transformers

# ======== Local environment variables for TrustifAI project ========
# HF_API_TOKEN=        # leave empty for now, unless you have a Hugging Face token
USE_CUDA=0           # 0 = force CPU mode (my Intel Arc Graphics GPU in ThinkPad laptop)
PEFT_MODEL_PATH=Graph_Structuring/fine-tuned-mistral
MODEL_ID=TinyLlama/TinyLlama-1.1B-Chat-v1.0

# ===== Neo4j connection details =======
# Priyabanta Neo4j Aura (secure scheme uses TLS)
# NEO4J_URI=neo4j+s://62f54917.databases.neo4j.io
# NEO4J_USERNAME=neo4j
# NEO4J_PASSWORD=yqyKyCuAv1wQzH7OG7LsTyEsEVzSYXjfXyvX0FALLJ0

# Faris Neo4j Aura instance (secure scheme uses TLS)
NEO4J_URI=neo4j+s://15010e8e.databases.neo4j.io
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD=uQXFCUdWgjg89V5xMKfzBETh2PLkZSrJta8ezkgvTZM
NEO4J_DATABASE=neo4j
AURA_INSTANCEID=15010e8e
AURA_INSTANCENAME="Free instance"

# === Neo4j Graph Schema Definition ===
GRAPH_NODE_LABEL=Node
GRAPH_REL_TYPE_PROPERTY=type

# ===== Other settings =====
SUPPRESS_WARNINGS=1

# ===== LLM Access Settings =====
# MODEL_BACKEND=http
# MODEL_SERVICE_URL=http://serv-3306.kl.dfki.de:8000/v1/chat/completions
# # MODEL_SERVICE_URL_FARIS=http://serv-4100.kl.dfki.de:8000/v1/chat/completions
# MODEL_SERVICE_NAME=llama3.3-70b-instruct-fp8
# REQUEST_TIMEOUT=30
# REQUEST_RETRIES=2

# MODEL_BACKEND=hf_local
# HF_LOCAL_MODEL=TinyLlama/TinyLlama-1.1B-Chat-v1.0
# HF_DEVICE=cpu
# HF_MAX_NEW_TOKENS=256

# --- use Groq ---
MODEL_BACKEND=groq
GROQ_API_KEY=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
# choose one (quality vs cost):
# GROQ_MODEL=llama-3.3-70b-versatile   # best results
GROQ_MODEL=llama-3.1-8b-instant        # cheaper/faster, still fine

# === Debugging on SLURM containers ===
DEBUG_PORT=5678