{
  "meta": {
    "created_at": "20260127_192321Z",
    "keyword": "explainability",
    "num_documents": 49,
    "num_topics": 3,
    "matched_topic_ids": [],
    "match_type_by_topic": {},
    "matched_synonyms": {},
    "generated_synonyms": [
      "interpretability",
      "transparency",
      "accountability",
      "understandability",
      "clarity",
      "non-obfuscation",
      "explain",
      "interpret",
      "analyze",
      "understand"
    ]
  },
  "topic_info": [
    {
      "Topic": -1,
      "Count": 3,
      "Name": "-1_et_al_yang_2021",
      "Representation": [
        "et",
        "al",
        "yang",
        "2021",
        "metrics",
        "vxai",
        "desiderata",
        "2019",
        "narrow",
        "text",
        "sources",
        "2024",
        "listed",
        "classification",
        "frameworks",
        "2020"
      ],
      "Representative_Docs": [
        "3.2 Proposed Framework of Desiderata\nBuilding on the desiderata frameworks established above and our findings on VXAI metrics, we propose a set of seven desiderata to serve as a categorization scheme for VXAI.\nOur goal is to offer a principled yet practical structure that enables consistent classification while avoiding the limitations of prior frameworks. These are either too narrow to accommodate relevant metrics or too broad and include properties beyond explainability. While properties such as fairness are often measured using XAI methods, we consider them beyond the scope of VXAI, because they assess the model's behavior rather than the explanation itself.",
        "1 Introduction\nThere exist a number of surveys and guidelines that address human-centered evaluations (Hoffman et al., 2018; Miller, 2019; Chromik & Schuessler, 2020; Holzinger et al., 2020; Franklin & Lagnado, 2022; Hsiao et al., 2021; Jesus et al., 2021; Langer et al., 2021; Mohseni et al., 2021; van der Waa et al., 2021; Silva et al., 2023). Unfortunately, the range of reviews dedicated to functionality-grounded evaluation is still limited. This is despite the advantage of offering objective, quantitative metrics without requiring human experiments, which can save both time and cost (Doshi-Velez & Kim, 2017; Samek et al., 2019; Zhou et al., 2021). Most existing studies are narrow and restricted to a specific application domain (Giuste et al., 2022; Arreche et al., 2024), including cybersecurity (Pawlicki et al., 2024), medical image classification (Patrício et al., 2023; Chaddad et al., 2024), electronic health record data",
        "3 Desiderata of XAI\n& Müller, Reported Metrics = 16 sources in text (metrics not listed) 40 sources in text. Yang et al., Primary VXAI Focus = ✓. Yang et al., Primarily Functionality- Grounded VXAI = ✓. Yang et al., (Semi-)Systematic Review = ✓. Yang et al., Date ↓ = Aug 2019. Yang et al., Desiderata = Generalizability, Fidelity, Persuasibility. Yang et al., Limited to = . Yang et al., Reported Metrics = (metrics not listed)\n† Co-12: Correctness, Output-Completeness, Consistency, Continuity, Contrastivity, Covariate Complexity, Compactness, Composition, Confidence, Context, Coherence, Controllability"
      ]
    },
    {
      "Topic": 0,
      "Count": 33,
      "Name": "0_al_explanation_et_xai",
      "Representation": [
        "al",
        "explanation",
        "et",
        "xai",
        "desiderata",
        "evaluation",
        "metrics",
        "2024",
        "explanations",
        "model",
        "2023",
        "explanans",
        "2021",
        "human",
        "grounded",
        "framework"
      ],
      "Representative_Docs": [
        "2 Related Work\nThere are five reviews, that we consider most similar to this work, as they present systematic functionality-grounded VXAI surveys. Le et al. (2023) and Nauta et al. (2023) both categorize the identified metrics based on a scheme of 12 properties, namely the Co-12 framework, which we discuss in more detail in Section 3. However, Le et al. (2023) restrict their analysis to metrics available through public XAI or VXAI toolkits (e.g., Quantus (Hedström et al., 2023)) and do not report on metrics introduced in the literature but not implemented in such libraries. Although there is some overlap with the study by Nauta et al. (2023), particularly in the inclusion of some identical metrics, their review also incorporates studies that merely apply VXAI metrics rather than introducing them. In contrast, our work provides detailed descriptions and categorizations of each identified metric. The review by Kadir et al. (2023) covers a broad range of domains and explanation types 2 and reports a wide variety of metrics. It also groups several metrics by method, a strategy shared by",
        "1 Introduction\nHowever, XAI is not a silver bullet. Van der Waa et al. (2021) point out, that humans tend to trust predictions more readily when an explanation is provided, often without carefully examining the explanation itself. This lack of critical scrutiny can lead to unwarranted trust, especially when decisions are taken based on incorrect or misleading explanations (Eiband et al., 2019; Jesus et al., 2021). To make matters worse, different XAI algorithms may result in conflicting explanations for the same model and sample (Krishna et al., 2022). Therefore, simply providing any explanation is not sufficient, but it is important to assess the quality of the explanation at hand (Sovrano et al., 2021). Unfortunately, while there is a plethora of XAI methods, evaluation of explanations is still an immature research area (Ribera & Lapedriza, 2019), with many studies relying on the notion of a good explanation as 'You'll know it when you see it', providing anecdotal evidence (i.e. small-scale qualitative validation) (Doshi-Velez & Kim, 2017; Nauta et al., 2023; Saarela &",
        "1 Introduction\n(Payrovnaziri et al., 2020), data and knowledge engineering (Li et al., 2020b), or timeseries classification (Theissler et al., 2022). Others focus on particular XAI approaches, such as visual explanations in CNNs (Mohamed et al., 2022) or instance-based explanations (Bayrak & Bach, 2024). Moreover, many surveys dedicate only limited attention to evaluation metrics, with their primary focus placed on the XAI methods themselves (Carvalho et al., 2019; Ding et al., 2022; Minh et al., 2022; Mohamed et al., 2022; Ali et al., 2023; Clement et al., 2023; Patrício et al., 2023; Chaddad et al., 2024; Gongane et al., 2024; Xua & Yang, 2024). By contrast, this review focuses exclusively on functionality-grounded evaluation across domains and is applicable to a wide range of XAI approaches."
      ]
    },
    {
      "Topic": 1,
      "Count": 13,
      "Name": "1_et_al_vxai_metrics",
      "Representation": [
        "et",
        "al",
        "vxai",
        "metrics",
        "desiderata",
        "systematic",
        "primarily",
        "semi",
        "date",
        "primary",
        "limited",
        "review",
        "focus",
        "functionality",
        "reported",
        "grounded"
      ],
      "Representative_Docs": [
        "3 Desiderata of XAI\nal., Desiderata = Correctness, Comprehensibility, Stability. Alangari et al., Limited to = . Alangari et al., Reported Metrics = 59 metrics. Le et al., Primary VXAI Focus = ✓. Le et al., Primarily Functionality- Grounded VXAI = ✓. Le et al., (Semi-)Systematic Review = ✓. Le et al., Date ↓ = Aug 2023. Le et al., Desiderata = Co-12 †. Le et al., Limited to = . Le et al., Reported Metrics = 86 metrics from 17 toolkits. Salih et al., Primary VXAI Focus = ✓. Salih et al., Primarily Functionality- Grounded VXAI = . Salih et al., (Semi-)Systematic Review = ✓. Salih et al., Date ↓ = Aug 2023. Salih et al., Desiderata = . Salih et al., Limited to = Cardiology. Salih et al., Reported Metrics = 27 metrics. Kadir et al., Primary VXAI Focus = ✓.",
        "3 Desiderata of XAI\net al., Primarily Functionality- Grounded VXAI = ✓. Bodria et al., (Semi-)Systematic Review = ✓. Bodria et al., Date ↓ = Nov 2021. Bodria et al., Desiderata = . Bodria et al., Limited to = . Bodria et al., Reported Metrics = 6 metrics. Sovrano et al., Primary VXAI Focus = ✓. Sovrano et al., Primarily Functionality- Grounded VXAI = . Sovrano et al., (Semi-)Systematic Review = . Sovrano et al., Date ↓ = Oct 2021. Sovrano et al., Desiderata = Similarity, Exactness, Fruitfulness. Sovrano et al., Limited to = . Sovrano et al., Reported Metrics = 22 metrics. Ras et al., Primary VXAI Focus = . Ras et al., Primarily Functionality- Grounded VXAI = . Ras et al., (Semi-)Systematic Review = . Ras et al., Date ↓ = Sep 2021. Ras et al., Desiderata = . Ras et",
        "3 Desiderata of XAI\net al., Primarily Functionality- Grounded VXAI = ✓. Bommer et al., (Semi-)Systematic Review = . Bommer et al., Date ↓ = Mar 2024. Bommer et al., Desiderata = Robustness, Faithfulness, Complexity, Lo- calization, Randomization. Bommer et al., Limited to = Climate Science. Bommer et al., Reported Metrics = 10 metrics. Li et al., Primary VXAI Focus = ✓. Li et al., Primarily Functionality- Grounded VXAI = ✓. Li et al., (Semi-)Systematic Review = . Li et al., Date ↓ = Dec 2023. Li et al., Desiderata = Faithfulness. Li et al., Limited to = Feature Attributions. Li et al., Reported Metrics = 6 metrics. Alangari et al., Primary VXAI Focus = ✓. Alangari et al., Primarily Functionality- Grounded VXAI = . Alangari et al., (Semi-)Systematic Review = . Alangari et al., Date ↓ = Aug 2023. Alangari et"
      ]
    }
  ],
  "document_info": [
    {
      "Document": "Unifying VXAI: A Systematic Review and Framework for the Evaluation of Explainable AI\nDavid Dembinsky\ndavid.dembinsky@dfki.de\nGerman Research Center for Artificial Intelligence\nAdriano Lucieri\nadriano.lucieri@dfki.de\nGerman Research Center for Artificial Intelligence\nStanislav Frolov\nstanislav.frolov@dfki.de\nGerman Research Center for Artificial Intelligence\nHiba Najjar\nhiba.najjar@dfki.de\nGerman Research Center for Artificial Intelligence\nKo Watanabe\nko.watanabe@dfki.de\nGerman Research Center for Artificial Intelligence\nAndreas Dengel\nandreas.dengel@dfki.de\nGerman Research Center for Artificial Intelligence",
      "Topic": 0,
      "Name": "0_al_explanation_et_xai",
      "Representation": [
        "al",
        "explanation",
        "et",
        "xai",
        "desiderata",
        "evaluation",
        "metrics",
        "2024",
        "explanations",
        "model",
        "2023",
        "explanans",
        "2021",
        "human",
        "grounded",
        "framework"
      ],
      "Representative_Docs": [
        "2 Related Work\nThere are five reviews, that we consider most similar to this work, as they present systematic functionality-grounded VXAI surveys. Le et al. (2023) and Nauta et al. (2023) both categorize the identified metrics based on a scheme of 12 properties, namely the Co-12 framework, which we discuss in more detail in Section 3. However, Le et al. (2023) restrict their analysis to metrics available through public XAI or VXAI toolkits (e.g., Quantus (Hedström et al., 2023)) and do not report on metrics introduced in the literature but not implemented in such libraries. Although there is some overlap with the study by Nauta et al. (2023), particularly in the inclusion of some identical metrics, their review also incorporates studies that merely apply VXAI metrics rather than introducing them. In contrast, our work provides detailed descriptions and categorizations of each identified metric. The review by Kadir et al. (2023) covers a broad range of domains and explanation types 2 and reports a wide variety of metrics. It also groups several metrics by method, a strategy shared by",
        "1 Introduction\nHowever, XAI is not a silver bullet. Van der Waa et al. (2021) point out, that humans tend to trust predictions more readily when an explanation is provided, often without carefully examining the explanation itself. This lack of critical scrutiny can lead to unwarranted trust, especially when decisions are taken based on incorrect or misleading explanations (Eiband et al., 2019; Jesus et al., 2021). To make matters worse, different XAI algorithms may result in conflicting explanations for the same model and sample (Krishna et al., 2022). Therefore, simply providing any explanation is not sufficient, but it is important to assess the quality of the explanation at hand (Sovrano et al., 2021). Unfortunately, while there is a plethora of XAI methods, evaluation of explanations is still an immature research area (Ribera & Lapedriza, 2019), with many studies relying on the notion of a good explanation as 'You'll know it when you see it', providing anecdotal evidence (i.e. small-scale qualitative validation) (Doshi-Velez & Kim, 2017; Nauta et al., 2023; Saarela &",
        "1 Introduction\n(Payrovnaziri et al., 2020), data and knowledge engineering (Li et al., 2020b), or timeseries classification (Theissler et al., 2022). Others focus on particular XAI approaches, such as visual explanations in CNNs (Mohamed et al., 2022) or instance-based explanations (Bayrak & Bach, 2024). Moreover, many surveys dedicate only limited attention to evaluation metrics, with their primary focus placed on the XAI methods themselves (Carvalho et al., 2019; Ding et al., 2022; Minh et al., 2022; Mohamed et al., 2022; Ali et al., 2023; Clement et al., 2023; Patrício et al., 2023; Chaddad et al., 2024; Gongane et al., 2024; Xua & Yang, 2024). By contrast, this review focuses exclusively on functionality-grounded evaluation across domains and is applicable to a wide range of XAI approaches."
      ],
      "Top_n_words": "al - explanation - et - xai - desiderata - evaluation - metrics - 2024 - explanations - model - 2023 - explanans - 2021 - human - grounded - framework",
      "Probability": 0.8504276036869494,
      "Representative_document": false
    },
    {
      "Document": "Abstract\nModern AI systems frequently rely on opaque black-box models, most notably Deep Neural Networks, whose performance stems from complex architectures with millions of learned parameters. While powerful, their complexity poses a major challenge to trustworthiness, particularly due to a lack of transparency. Explainable AI (XAI) addresses this issue by providing human-understandable explanations of model behavior. However, to ensure their usefulness and trustworthiness, such explanations must be rigorously evaluated. Despite the growing number of XAI methods, the field lacks standardized evaluation protocols and consensus on appropriate metrics. To address this gap, we conduct a systematic literature review following the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines and introduce a unified framework for the eValuation of XAI (VXAI) . We identify 362 relevant publications and aggregate their contributions into 41 functionally similar metric groups. In addition, we propose a three-dimensional categorization scheme spanning explanation type, evaluation contextuality, and explanation quality desiderata. Our framework provides the most comprehensive and structured overview of VXAI to date. It supports systematic metric selection, promotes comparability across methods, and offers a flexible foundation for future extensions.",
      "Topic": 0,
      "Name": "0_al_explanation_et_xai",
      "Representation": [
        "al",
        "explanation",
        "et",
        "xai",
        "desiderata",
        "evaluation",
        "metrics",
        "2024",
        "explanations",
        "model",
        "2023",
        "explanans",
        "2021",
        "human",
        "grounded",
        "framework"
      ],
      "Representative_Docs": [
        "2 Related Work\nThere are five reviews, that we consider most similar to this work, as they present systematic functionality-grounded VXAI surveys. Le et al. (2023) and Nauta et al. (2023) both categorize the identified metrics based on a scheme of 12 properties, namely the Co-12 framework, which we discuss in more detail in Section 3. However, Le et al. (2023) restrict their analysis to metrics available through public XAI or VXAI toolkits (e.g., Quantus (Hedström et al., 2023)) and do not report on metrics introduced in the literature but not implemented in such libraries. Although there is some overlap with the study by Nauta et al. (2023), particularly in the inclusion of some identical metrics, their review also incorporates studies that merely apply VXAI metrics rather than introducing them. In contrast, our work provides detailed descriptions and categorizations of each identified metric. The review by Kadir et al. (2023) covers a broad range of domains and explanation types 2 and reports a wide variety of metrics. It also groups several metrics by method, a strategy shared by",
        "1 Introduction\nHowever, XAI is not a silver bullet. Van der Waa et al. (2021) point out, that humans tend to trust predictions more readily when an explanation is provided, often without carefully examining the explanation itself. This lack of critical scrutiny can lead to unwarranted trust, especially when decisions are taken based on incorrect or misleading explanations (Eiband et al., 2019; Jesus et al., 2021). To make matters worse, different XAI algorithms may result in conflicting explanations for the same model and sample (Krishna et al., 2022). Therefore, simply providing any explanation is not sufficient, but it is important to assess the quality of the explanation at hand (Sovrano et al., 2021). Unfortunately, while there is a plethora of XAI methods, evaluation of explanations is still an immature research area (Ribera & Lapedriza, 2019), with many studies relying on the notion of a good explanation as 'You'll know it when you see it', providing anecdotal evidence (i.e. small-scale qualitative validation) (Doshi-Velez & Kim, 2017; Nauta et al., 2023; Saarela &",
        "1 Introduction\n(Payrovnaziri et al., 2020), data and knowledge engineering (Li et al., 2020b), or timeseries classification (Theissler et al., 2022). Others focus on particular XAI approaches, such as visual explanations in CNNs (Mohamed et al., 2022) or instance-based explanations (Bayrak & Bach, 2024). Moreover, many surveys dedicate only limited attention to evaluation metrics, with their primary focus placed on the XAI methods themselves (Carvalho et al., 2019; Ding et al., 2022; Minh et al., 2022; Mohamed et al., 2022; Ali et al., 2023; Clement et al., 2023; Patrício et al., 2023; Chaddad et al., 2024; Gongane et al., 2024; Xua & Yang, 2024). By contrast, this review focuses exclusively on functionality-grounded evaluation across domains and is applicable to a wide range of XAI approaches."
      ],
      "Top_n_words": "al - explanation - et - xai - desiderata - evaluation - metrics - 2024 - explanations - model - 2023 - explanans - 2021 - human - grounded - framework",
      "Probability": 1.0,
      "Representative_document": false
    },
    {
      "Document": "1 Introduction\nExplainable AI (XAI) is a research area of growing interest to both AI researchers and practitioners. It aims to alleviate the black-box issue of current deep-learning models, which can reach stunning performances at the expense of their interpretability (Vilone & Longo, 2021). Government-affiliated initiatives, such as the European Union High-Level Expert Group on AI (2019), the U.S. National",
      "Topic": 0,
      "Name": "0_al_explanation_et_xai",
      "Representation": [
        "al",
        "explanation",
        "et",
        "xai",
        "desiderata",
        "evaluation",
        "metrics",
        "2024",
        "explanations",
        "model",
        "2023",
        "explanans",
        "2021",
        "human",
        "grounded",
        "framework"
      ],
      "Representative_Docs": [
        "2 Related Work\nThere are five reviews, that we consider most similar to this work, as they present systematic functionality-grounded VXAI surveys. Le et al. (2023) and Nauta et al. (2023) both categorize the identified metrics based on a scheme of 12 properties, namely the Co-12 framework, which we discuss in more detail in Section 3. However, Le et al. (2023) restrict their analysis to metrics available through public XAI or VXAI toolkits (e.g., Quantus (Hedström et al., 2023)) and do not report on metrics introduced in the literature but not implemented in such libraries. Although there is some overlap with the study by Nauta et al. (2023), particularly in the inclusion of some identical metrics, their review also incorporates studies that merely apply VXAI metrics rather than introducing them. In contrast, our work provides detailed descriptions and categorizations of each identified metric. The review by Kadir et al. (2023) covers a broad range of domains and explanation types 2 and reports a wide variety of metrics. It also groups several metrics by method, a strategy shared by",
        "1 Introduction\nHowever, XAI is not a silver bullet. Van der Waa et al. (2021) point out, that humans tend to trust predictions more readily when an explanation is provided, often without carefully examining the explanation itself. This lack of critical scrutiny can lead to unwarranted trust, especially when decisions are taken based on incorrect or misleading explanations (Eiband et al., 2019; Jesus et al., 2021). To make matters worse, different XAI algorithms may result in conflicting explanations for the same model and sample (Krishna et al., 2022). Therefore, simply providing any explanation is not sufficient, but it is important to assess the quality of the explanation at hand (Sovrano et al., 2021). Unfortunately, while there is a plethora of XAI methods, evaluation of explanations is still an immature research area (Ribera & Lapedriza, 2019), with many studies relying on the notion of a good explanation as 'You'll know it when you see it', providing anecdotal evidence (i.e. small-scale qualitative validation) (Doshi-Velez & Kim, 2017; Nauta et al., 2023; Saarela &",
        "1 Introduction\n(Payrovnaziri et al., 2020), data and knowledge engineering (Li et al., 2020b), or timeseries classification (Theissler et al., 2022). Others focus on particular XAI approaches, such as visual explanations in CNNs (Mohamed et al., 2022) or instance-based explanations (Bayrak & Bach, 2024). Moreover, many surveys dedicate only limited attention to evaluation metrics, with their primary focus placed on the XAI methods themselves (Carvalho et al., 2019; Ding et al., 2022; Minh et al., 2022; Mohamed et al., 2022; Ali et al., 2023; Clement et al., 2023; Patrício et al., 2023; Chaddad et al., 2024; Gongane et al., 2024; Xua & Yang, 2024). By contrast, this review focuses exclusively on functionality-grounded evaluation across domains and is applicable to a wide range of XAI approaches."
      ],
      "Top_n_words": "al - explanation - et - xai - desiderata - evaluation - metrics - 2024 - explanations - model - 2023 - explanans - 2021 - human - grounded - framework",
      "Probability": 0.9165155618800245,
      "Representative_document": false
    },
    {
      "Document": "1 Introduction\nInstitute of Standards and Technology (2023), and the DARPA initiative (Gunning & Aha, 2019), identified XAI as a crucial part of Trustworthy AI. Especially as it helps AI systems in serving the 'right to explain' its decisions (Goodman & Flaxman, 2017) and fosters user trust through understanding of the system (Morandini et al., 2023). XAI already plays a fundamental role in making high-stakes AI systems more trustworthy (Saarela & Podgorelec, 2024; Xua & Yang, 2024), with broad applications in areas such as healthcare, finance, autonomous driving, natural disaster detection, energy management, military and remote sensing (Adadi & Berrada, 2018; Markus et al., 2021; Saraswat et al., 2022; Kadir et al., 2023; Hosain et al., 2024; Höhl et al., 2024). Furthermore, explainability is used to help with other dimensions of trustworthiness like privacy, robustness, or fairness (Doshi-Velez & Kim, 2017; Yang et al., 2019; Arrieta et al., 2020;",
      "Topic": 0,
      "Name": "0_al_explanation_et_xai",
      "Representation": [
        "al",
        "explanation",
        "et",
        "xai",
        "desiderata",
        "evaluation",
        "metrics",
        "2024",
        "explanations",
        "model",
        "2023",
        "explanans",
        "2021",
        "human",
        "grounded",
        "framework"
      ],
      "Representative_Docs": [
        "2 Related Work\nThere are five reviews, that we consider most similar to this work, as they present systematic functionality-grounded VXAI surveys. Le et al. (2023) and Nauta et al. (2023) both categorize the identified metrics based on a scheme of 12 properties, namely the Co-12 framework, which we discuss in more detail in Section 3. However, Le et al. (2023) restrict their analysis to metrics available through public XAI or VXAI toolkits (e.g., Quantus (Hedström et al., 2023)) and do not report on metrics introduced in the literature but not implemented in such libraries. Although there is some overlap with the study by Nauta et al. (2023), particularly in the inclusion of some identical metrics, their review also incorporates studies that merely apply VXAI metrics rather than introducing them. In contrast, our work provides detailed descriptions and categorizations of each identified metric. The review by Kadir et al. (2023) covers a broad range of domains and explanation types 2 and reports a wide variety of metrics. It also groups several metrics by method, a strategy shared by",
        "1 Introduction\nHowever, XAI is not a silver bullet. Van der Waa et al. (2021) point out, that humans tend to trust predictions more readily when an explanation is provided, often without carefully examining the explanation itself. This lack of critical scrutiny can lead to unwarranted trust, especially when decisions are taken based on incorrect or misleading explanations (Eiband et al., 2019; Jesus et al., 2021). To make matters worse, different XAI algorithms may result in conflicting explanations for the same model and sample (Krishna et al., 2022). Therefore, simply providing any explanation is not sufficient, but it is important to assess the quality of the explanation at hand (Sovrano et al., 2021). Unfortunately, while there is a plethora of XAI methods, evaluation of explanations is still an immature research area (Ribera & Lapedriza, 2019), with many studies relying on the notion of a good explanation as 'You'll know it when you see it', providing anecdotal evidence (i.e. small-scale qualitative validation) (Doshi-Velez & Kim, 2017; Nauta et al., 2023; Saarela &",
        "1 Introduction\n(Payrovnaziri et al., 2020), data and knowledge engineering (Li et al., 2020b), or timeseries classification (Theissler et al., 2022). Others focus on particular XAI approaches, such as visual explanations in CNNs (Mohamed et al., 2022) or instance-based explanations (Bayrak & Bach, 2024). Moreover, many surveys dedicate only limited attention to evaluation metrics, with their primary focus placed on the XAI methods themselves (Carvalho et al., 2019; Ding et al., 2022; Minh et al., 2022; Mohamed et al., 2022; Ali et al., 2023; Clement et al., 2023; Patrício et al., 2023; Chaddad et al., 2024; Gongane et al., 2024; Xua & Yang, 2024). By contrast, this review focuses exclusively on functionality-grounded evaluation across domains and is applicable to a wide range of XAI approaches."
      ],
      "Top_n_words": "al - explanation - et - xai - desiderata - evaluation - metrics - 2024 - explanations - model - 2023 - explanans - 2021 - human - grounded - framework",
      "Probability": 1.0,
      "Representative_document": false
    },
    {
      "Document": "1 Introduction\nDas & Rad, 2020; Markus et al., 2021; Rawal et al., 2021; Agarwal et al., 2022b).",
      "Topic": 1,
      "Name": "1_et_al_vxai_metrics",
      "Representation": [
        "et",
        "al",
        "vxai",
        "metrics",
        "desiderata",
        "systematic",
        "primarily",
        "semi",
        "date",
        "primary",
        "limited",
        "review",
        "focus",
        "functionality",
        "reported",
        "grounded"
      ],
      "Representative_Docs": [
        "3 Desiderata of XAI\nal., Desiderata = Correctness, Comprehensibility, Stability. Alangari et al., Limited to = . Alangari et al., Reported Metrics = 59 metrics. Le et al., Primary VXAI Focus = ✓. Le et al., Primarily Functionality- Grounded VXAI = ✓. Le et al., (Semi-)Systematic Review = ✓. Le et al., Date ↓ = Aug 2023. Le et al., Desiderata = Co-12 †. Le et al., Limited to = . Le et al., Reported Metrics = 86 metrics from 17 toolkits. Salih et al., Primary VXAI Focus = ✓. Salih et al., Primarily Functionality- Grounded VXAI = . Salih et al., (Semi-)Systematic Review = ✓. Salih et al., Date ↓ = Aug 2023. Salih et al., Desiderata = . Salih et al., Limited to = Cardiology. Salih et al., Reported Metrics = 27 metrics. Kadir et al., Primary VXAI Focus = ✓.",
        "3 Desiderata of XAI\net al., Primarily Functionality- Grounded VXAI = ✓. Bodria et al., (Semi-)Systematic Review = ✓. Bodria et al., Date ↓ = Nov 2021. Bodria et al., Desiderata = . Bodria et al., Limited to = . Bodria et al., Reported Metrics = 6 metrics. Sovrano et al., Primary VXAI Focus = ✓. Sovrano et al., Primarily Functionality- Grounded VXAI = . Sovrano et al., (Semi-)Systematic Review = . Sovrano et al., Date ↓ = Oct 2021. Sovrano et al., Desiderata = Similarity, Exactness, Fruitfulness. Sovrano et al., Limited to = . Sovrano et al., Reported Metrics = 22 metrics. Ras et al., Primary VXAI Focus = . Ras et al., Primarily Functionality- Grounded VXAI = . Ras et al., (Semi-)Systematic Review = . Ras et al., Date ↓ = Sep 2021. Ras et al., Desiderata = . Ras et",
        "3 Desiderata of XAI\net al., Primarily Functionality- Grounded VXAI = ✓. Bommer et al., (Semi-)Systematic Review = . Bommer et al., Date ↓ = Mar 2024. Bommer et al., Desiderata = Robustness, Faithfulness, Complexity, Lo- calization, Randomization. Bommer et al., Limited to = Climate Science. Bommer et al., Reported Metrics = 10 metrics. Li et al., Primary VXAI Focus = ✓. Li et al., Primarily Functionality- Grounded VXAI = ✓. Li et al., (Semi-)Systematic Review = . Li et al., Date ↓ = Dec 2023. Li et al., Desiderata = Faithfulness. Li et al., Limited to = Feature Attributions. Li et al., Reported Metrics = 6 metrics. Alangari et al., Primary VXAI Focus = ✓. Alangari et al., Primarily Functionality- Grounded VXAI = . Alangari et al., (Semi-)Systematic Review = . Alangari et al., Date ↓ = Aug 2023. Alangari et"
      ],
      "Top_n_words": "et - al - vxai - metrics - desiderata - systematic - primarily - semi - date - primary - limited - review - focus - functionality - reported - grounded",
      "Probability": 1.0,
      "Representative_document": false
    },
    {
      "Document": "1 Introduction\nHowever, XAI is not a silver bullet. Van der Waa et al. (2021) point out, that humans tend to trust predictions more readily when an explanation is provided, often without carefully examining the explanation itself. This lack of critical scrutiny can lead to unwarranted trust, especially when decisions are taken based on incorrect or misleading explanations (Eiband et al., 2019; Jesus et al., 2021). To make matters worse, different XAI algorithms may result in conflicting explanations for the same model and sample (Krishna et al., 2022). Therefore, simply providing any explanation is not sufficient, but it is important to assess the quality of the explanation at hand (Sovrano et al., 2021). Unfortunately, while there is a plethora of XAI methods, evaluation of explanations is still an immature research area (Ribera & Lapedriza, 2019), with many studies relying on the notion of a good explanation as 'You'll know it when you see it', providing anecdotal evidence (i.e. small-scale qualitative validation) (Doshi-Velez & Kim, 2017; Nauta et al., 2023; Saarela &",
      "Topic": 0,
      "Name": "0_al_explanation_et_xai",
      "Representation": [
        "al",
        "explanation",
        "et",
        "xai",
        "desiderata",
        "evaluation",
        "metrics",
        "2024",
        "explanations",
        "model",
        "2023",
        "explanans",
        "2021",
        "human",
        "grounded",
        "framework"
      ],
      "Representative_Docs": [
        "2 Related Work\nThere are five reviews, that we consider most similar to this work, as they present systematic functionality-grounded VXAI surveys. Le et al. (2023) and Nauta et al. (2023) both categorize the identified metrics based on a scheme of 12 properties, namely the Co-12 framework, which we discuss in more detail in Section 3. However, Le et al. (2023) restrict their analysis to metrics available through public XAI or VXAI toolkits (e.g., Quantus (Hedström et al., 2023)) and do not report on metrics introduced in the literature but not implemented in such libraries. Although there is some overlap with the study by Nauta et al. (2023), particularly in the inclusion of some identical metrics, their review also incorporates studies that merely apply VXAI metrics rather than introducing them. In contrast, our work provides detailed descriptions and categorizations of each identified metric. The review by Kadir et al. (2023) covers a broad range of domains and explanation types 2 and reports a wide variety of metrics. It also groups several metrics by method, a strategy shared by",
        "1 Introduction\nHowever, XAI is not a silver bullet. Van der Waa et al. (2021) point out, that humans tend to trust predictions more readily when an explanation is provided, often without carefully examining the explanation itself. This lack of critical scrutiny can lead to unwarranted trust, especially when decisions are taken based on incorrect or misleading explanations (Eiband et al., 2019; Jesus et al., 2021). To make matters worse, different XAI algorithms may result in conflicting explanations for the same model and sample (Krishna et al., 2022). Therefore, simply providing any explanation is not sufficient, but it is important to assess the quality of the explanation at hand (Sovrano et al., 2021). Unfortunately, while there is a plethora of XAI methods, evaluation of explanations is still an immature research area (Ribera & Lapedriza, 2019), with many studies relying on the notion of a good explanation as 'You'll know it when you see it', providing anecdotal evidence (i.e. small-scale qualitative validation) (Doshi-Velez & Kim, 2017; Nauta et al., 2023; Saarela &",
        "1 Introduction\n(Payrovnaziri et al., 2020), data and knowledge engineering (Li et al., 2020b), or timeseries classification (Theissler et al., 2022). Others focus on particular XAI approaches, such as visual explanations in CNNs (Mohamed et al., 2022) or instance-based explanations (Bayrak & Bach, 2024). Moreover, many surveys dedicate only limited attention to evaluation metrics, with their primary focus placed on the XAI methods themselves (Carvalho et al., 2019; Ding et al., 2022; Minh et al., 2022; Mohamed et al., 2022; Ali et al., 2023; Clement et al., 2023; Patrício et al., 2023; Chaddad et al., 2024; Gongane et al., 2024; Xua & Yang, 2024). By contrast, this review focuses exclusively on functionality-grounded evaluation across domains and is applicable to a wide range of XAI approaches."
      ],
      "Top_n_words": "al - explanation - et - xai - desiderata - evaluation - metrics - 2024 - explanations - model - 2023 - explanans - 2021 - human - grounded - framework",
      "Probability": 0.877620147218145,
      "Representative_document": true
    },
    {
      "Document": "1 Introduction\nPodgorelec, 2024). Especially in computer vision tasks, evaluation through qualitative inspection of a few examples can be appealing (Ibrahim & Shafiq, 2023). However, unstructured qualitative examination leads to highly subjective results, as humans struggle at judging the value of XAI explanations (Adebayo et al., 2018; Buçinca et al., 2020; Hase & Bansal, 2020). In addition, such evaluations risk cherry-picking favorable examples and offer no reliable foundation for comparing different explanation methods across studies or practitioners. For the same given explanation, human ratings vary depending on both the task itself (Franklin & Lagnado, 2022) and the participant's cultural background (Peters & Carman, 2024). Evaluation is further complicated by the lack of ground-truth for the explanations, as this would require knowledge about the model's internal reasoning process (Samek et al., 2019; Markus et al., 2021; Samek et al., 2021; Bommer et al., 2024; Ortigossa et al., 2024).",
      "Topic": 0,
      "Name": "0_al_explanation_et_xai",
      "Representation": [
        "al",
        "explanation",
        "et",
        "xai",
        "desiderata",
        "evaluation",
        "metrics",
        "2024",
        "explanations",
        "model",
        "2023",
        "explanans",
        "2021",
        "human",
        "grounded",
        "framework"
      ],
      "Representative_Docs": [
        "2 Related Work\nThere are five reviews, that we consider most similar to this work, as they present systematic functionality-grounded VXAI surveys. Le et al. (2023) and Nauta et al. (2023) both categorize the identified metrics based on a scheme of 12 properties, namely the Co-12 framework, which we discuss in more detail in Section 3. However, Le et al. (2023) restrict their analysis to metrics available through public XAI or VXAI toolkits (e.g., Quantus (Hedström et al., 2023)) and do not report on metrics introduced in the literature but not implemented in such libraries. Although there is some overlap with the study by Nauta et al. (2023), particularly in the inclusion of some identical metrics, their review also incorporates studies that merely apply VXAI metrics rather than introducing them. In contrast, our work provides detailed descriptions and categorizations of each identified metric. The review by Kadir et al. (2023) covers a broad range of domains and explanation types 2 and reports a wide variety of metrics. It also groups several metrics by method, a strategy shared by",
        "1 Introduction\nHowever, XAI is not a silver bullet. Van der Waa et al. (2021) point out, that humans tend to trust predictions more readily when an explanation is provided, often without carefully examining the explanation itself. This lack of critical scrutiny can lead to unwarranted trust, especially when decisions are taken based on incorrect or misleading explanations (Eiband et al., 2019; Jesus et al., 2021). To make matters worse, different XAI algorithms may result in conflicting explanations for the same model and sample (Krishna et al., 2022). Therefore, simply providing any explanation is not sufficient, but it is important to assess the quality of the explanation at hand (Sovrano et al., 2021). Unfortunately, while there is a plethora of XAI methods, evaluation of explanations is still an immature research area (Ribera & Lapedriza, 2019), with many studies relying on the notion of a good explanation as 'You'll know it when you see it', providing anecdotal evidence (i.e. small-scale qualitative validation) (Doshi-Velez & Kim, 2017; Nauta et al., 2023; Saarela &",
        "1 Introduction\n(Payrovnaziri et al., 2020), data and knowledge engineering (Li et al., 2020b), or timeseries classification (Theissler et al., 2022). Others focus on particular XAI approaches, such as visual explanations in CNNs (Mohamed et al., 2022) or instance-based explanations (Bayrak & Bach, 2024). Moreover, many surveys dedicate only limited attention to evaluation metrics, with their primary focus placed on the XAI methods themselves (Carvalho et al., 2019; Ding et al., 2022; Minh et al., 2022; Mohamed et al., 2022; Ali et al., 2023; Clement et al., 2023; Patrício et al., 2023; Chaddad et al., 2024; Gongane et al., 2024; Xua & Yang, 2024). By contrast, this review focuses exclusively on functionality-grounded evaluation across domains and is applicable to a wide range of XAI approaches."
      ],
      "Top_n_words": "al - explanation - et - xai - desiderata - evaluation - metrics - 2024 - explanations - model - 2023 - explanans - 2021 - human - grounded - framework",
      "Probability": 0.7499813513452699,
      "Representative_document": false
    },
    {
      "Document": "1 Introduction\nBecause evaluation is still not performed consistently and seldom systematically (Adadi & Berrada, 2018; Lipton, 2018; Payrovnaziri et al., 2020; Messina et al., 2022; Lopes et al., 2022; De Camargo et al., 2023; Kadir et al., 2023; Nauta et al., 2023; Mohamed et al., 2024; Naveed et al., 2024; Saarela & Podgorelec, 2024; Salih et al., 2024a), the community frequently calls to develop comprehensive and unified evaluation standards (Pinto & Paquette, 2024; Saarela & Podgorelec, 2024; Xua & Yang, 2024). A central motivation behind such efforts is to enable the comparison of explanations and asses whether explainability is achieved (Markus et al., 2021; Zhou et al., 2021)",
      "Topic": 0,
      "Name": "0_al_explanation_et_xai",
      "Representation": [
        "al",
        "explanation",
        "et",
        "xai",
        "desiderata",
        "evaluation",
        "metrics",
        "2024",
        "explanations",
        "model",
        "2023",
        "explanans",
        "2021",
        "human",
        "grounded",
        "framework"
      ],
      "Representative_Docs": [
        "2 Related Work\nThere are five reviews, that we consider most similar to this work, as they present systematic functionality-grounded VXAI surveys. Le et al. (2023) and Nauta et al. (2023) both categorize the identified metrics based on a scheme of 12 properties, namely the Co-12 framework, which we discuss in more detail in Section 3. However, Le et al. (2023) restrict their analysis to metrics available through public XAI or VXAI toolkits (e.g., Quantus (Hedström et al., 2023)) and do not report on metrics introduced in the literature but not implemented in such libraries. Although there is some overlap with the study by Nauta et al. (2023), particularly in the inclusion of some identical metrics, their review also incorporates studies that merely apply VXAI metrics rather than introducing them. In contrast, our work provides detailed descriptions and categorizations of each identified metric. The review by Kadir et al. (2023) covers a broad range of domains and explanation types 2 and reports a wide variety of metrics. It also groups several metrics by method, a strategy shared by",
        "1 Introduction\nHowever, XAI is not a silver bullet. Van der Waa et al. (2021) point out, that humans tend to trust predictions more readily when an explanation is provided, often without carefully examining the explanation itself. This lack of critical scrutiny can lead to unwarranted trust, especially when decisions are taken based on incorrect or misleading explanations (Eiband et al., 2019; Jesus et al., 2021). To make matters worse, different XAI algorithms may result in conflicting explanations for the same model and sample (Krishna et al., 2022). Therefore, simply providing any explanation is not sufficient, but it is important to assess the quality of the explanation at hand (Sovrano et al., 2021). Unfortunately, while there is a plethora of XAI methods, evaluation of explanations is still an immature research area (Ribera & Lapedriza, 2019), with many studies relying on the notion of a good explanation as 'You'll know it when you see it', providing anecdotal evidence (i.e. small-scale qualitative validation) (Doshi-Velez & Kim, 2017; Nauta et al., 2023; Saarela &",
        "1 Introduction\n(Payrovnaziri et al., 2020), data and knowledge engineering (Li et al., 2020b), or timeseries classification (Theissler et al., 2022). Others focus on particular XAI approaches, such as visual explanations in CNNs (Mohamed et al., 2022) or instance-based explanations (Bayrak & Bach, 2024). Moreover, many surveys dedicate only limited attention to evaluation metrics, with their primary focus placed on the XAI methods themselves (Carvalho et al., 2019; Ding et al., 2022; Minh et al., 2022; Mohamed et al., 2022; Ali et al., 2023; Clement et al., 2023; Patrício et al., 2023; Chaddad et al., 2024; Gongane et al., 2024; Xua & Yang, 2024). By contrast, this review focuses exclusively on functionality-grounded evaluation across domains and is applicable to a wide range of XAI approaches."
      ],
      "Top_n_words": "al - explanation - et - xai - desiderata - evaluation - metrics - 2024 - explanations - model - 2023 - explanans - 2021 - human - grounded - framework",
      "Probability": 1.0,
      "Representative_document": false
    },
    {
      "Document": "1 Introduction\nOne of the most prevalent taxonomies reported in the literature (Vilone & Longo, 2021; Zhou et al., 2021; Elkhawaga et al., 2023), and illustrated in Figure 1, is the distinction proposed by Doshi-Velez & Kim (2017) between human-grounded and functionality-grounded evaluation methods. The former includes qualitative and quantitative evaluations by laypeople and experts, while the latter consists of (semi-)automatic metrics.",
      "Topic": 0,
      "Name": "0_al_explanation_et_xai",
      "Representation": [
        "al",
        "explanation",
        "et",
        "xai",
        "desiderata",
        "evaluation",
        "metrics",
        "2024",
        "explanations",
        "model",
        "2023",
        "explanans",
        "2021",
        "human",
        "grounded",
        "framework"
      ],
      "Representative_Docs": [
        "2 Related Work\nThere are five reviews, that we consider most similar to this work, as they present systematic functionality-grounded VXAI surveys. Le et al. (2023) and Nauta et al. (2023) both categorize the identified metrics based on a scheme of 12 properties, namely the Co-12 framework, which we discuss in more detail in Section 3. However, Le et al. (2023) restrict their analysis to metrics available through public XAI or VXAI toolkits (e.g., Quantus (Hedström et al., 2023)) and do not report on metrics introduced in the literature but not implemented in such libraries. Although there is some overlap with the study by Nauta et al. (2023), particularly in the inclusion of some identical metrics, their review also incorporates studies that merely apply VXAI metrics rather than introducing them. In contrast, our work provides detailed descriptions and categorizations of each identified metric. The review by Kadir et al. (2023) covers a broad range of domains and explanation types 2 and reports a wide variety of metrics. It also groups several metrics by method, a strategy shared by",
        "1 Introduction\nHowever, XAI is not a silver bullet. Van der Waa et al. (2021) point out, that humans tend to trust predictions more readily when an explanation is provided, often without carefully examining the explanation itself. This lack of critical scrutiny can lead to unwarranted trust, especially when decisions are taken based on incorrect or misleading explanations (Eiband et al., 2019; Jesus et al., 2021). To make matters worse, different XAI algorithms may result in conflicting explanations for the same model and sample (Krishna et al., 2022). Therefore, simply providing any explanation is not sufficient, but it is important to assess the quality of the explanation at hand (Sovrano et al., 2021). Unfortunately, while there is a plethora of XAI methods, evaluation of explanations is still an immature research area (Ribera & Lapedriza, 2019), with many studies relying on the notion of a good explanation as 'You'll know it when you see it', providing anecdotal evidence (i.e. small-scale qualitative validation) (Doshi-Velez & Kim, 2017; Nauta et al., 2023; Saarela &",
        "1 Introduction\n(Payrovnaziri et al., 2020), data and knowledge engineering (Li et al., 2020b), or timeseries classification (Theissler et al., 2022). Others focus on particular XAI approaches, such as visual explanations in CNNs (Mohamed et al., 2022) or instance-based explanations (Bayrak & Bach, 2024). Moreover, many surveys dedicate only limited attention to evaluation metrics, with their primary focus placed on the XAI methods themselves (Carvalho et al., 2019; Ding et al., 2022; Minh et al., 2022; Mohamed et al., 2022; Ali et al., 2023; Clement et al., 2023; Patrício et al., 2023; Chaddad et al., 2024; Gongane et al., 2024; Xua & Yang, 2024). By contrast, this review focuses exclusively on functionality-grounded evaluation across domains and is applicable to a wide range of XAI approaches."
      ],
      "Top_n_words": "al - explanation - et - xai - desiderata - evaluation - metrics - 2024 - explanations - model - 2023 - explanans - 2021 - human - grounded - framework",
      "Probability": 0.49392061986734065,
      "Representative_document": false
    },
    {
      "Document": "1 Introduction\nSince explanations are meant to aid humans, human-grounded evaluation remains the gold standard to assess their effectiveness in assisting humans (Doshi-Velez & Kim, 2017; Gunning & Aha, 2019; Miller et al., 2017). However, the faithfulness (i.e., technical correctness) of an explanation and the plausibility to humans do not necessarily correlate (Wiegreffe & Pinter, 2019; Jacovi & Goldberg, 2020; Atanasova, 2024a). Therefore, human-grounded evaluation of an explanation's comprehensibility should be distinguished from functionality-grounded evaluation of its faithfulness (Nauta et al., 2023). Especially, humans can't confidently attribute whether an unexpected explanans (i.e., the information provided to explain a decision ) is caused by a faulty explanation (process 1 1 ) or a flawed black-box model (Robnik-Šikonja & Bohanec, 2018; Zhang et al., 2019a); see Figure 2 for an illustration. In\n1 The exact definitions of explanandum, explanation, and explanans are provided at the end of Section 1.",
      "Topic": 0,
      "Name": "0_al_explanation_et_xai",
      "Representation": [
        "al",
        "explanation",
        "et",
        "xai",
        "desiderata",
        "evaluation",
        "metrics",
        "2024",
        "explanations",
        "model",
        "2023",
        "explanans",
        "2021",
        "human",
        "grounded",
        "framework"
      ],
      "Representative_Docs": [
        "2 Related Work\nThere are five reviews, that we consider most similar to this work, as they present systematic functionality-grounded VXAI surveys. Le et al. (2023) and Nauta et al. (2023) both categorize the identified metrics based on a scheme of 12 properties, namely the Co-12 framework, which we discuss in more detail in Section 3. However, Le et al. (2023) restrict their analysis to metrics available through public XAI or VXAI toolkits (e.g., Quantus (Hedström et al., 2023)) and do not report on metrics introduced in the literature but not implemented in such libraries. Although there is some overlap with the study by Nauta et al. (2023), particularly in the inclusion of some identical metrics, their review also incorporates studies that merely apply VXAI metrics rather than introducing them. In contrast, our work provides detailed descriptions and categorizations of each identified metric. The review by Kadir et al. (2023) covers a broad range of domains and explanation types 2 and reports a wide variety of metrics. It also groups several metrics by method, a strategy shared by",
        "1 Introduction\nHowever, XAI is not a silver bullet. Van der Waa et al. (2021) point out, that humans tend to trust predictions more readily when an explanation is provided, often without carefully examining the explanation itself. This lack of critical scrutiny can lead to unwarranted trust, especially when decisions are taken based on incorrect or misleading explanations (Eiband et al., 2019; Jesus et al., 2021). To make matters worse, different XAI algorithms may result in conflicting explanations for the same model and sample (Krishna et al., 2022). Therefore, simply providing any explanation is not sufficient, but it is important to assess the quality of the explanation at hand (Sovrano et al., 2021). Unfortunately, while there is a plethora of XAI methods, evaluation of explanations is still an immature research area (Ribera & Lapedriza, 2019), with many studies relying on the notion of a good explanation as 'You'll know it when you see it', providing anecdotal evidence (i.e. small-scale qualitative validation) (Doshi-Velez & Kim, 2017; Nauta et al., 2023; Saarela &",
        "1 Introduction\n(Payrovnaziri et al., 2020), data and knowledge engineering (Li et al., 2020b), or timeseries classification (Theissler et al., 2022). Others focus on particular XAI approaches, such as visual explanations in CNNs (Mohamed et al., 2022) or instance-based explanations (Bayrak & Bach, 2024). Moreover, many surveys dedicate only limited attention to evaluation metrics, with their primary focus placed on the XAI methods themselves (Carvalho et al., 2019; Ding et al., 2022; Minh et al., 2022; Mohamed et al., 2022; Ali et al., 2023; Clement et al., 2023; Patrício et al., 2023; Chaddad et al., 2024; Gongane et al., 2024; Xua & Yang, 2024). By contrast, this review focuses exclusively on functionality-grounded evaluation across domains and is applicable to a wide range of XAI approaches."
      ],
      "Top_n_words": "al - explanation - et - xai - desiderata - evaluation - metrics - 2024 - explanations - model - 2023 - explanans - 2021 - human - grounded - framework",
      "Probability": 1.0,
      "Representative_document": false
    },
    {
      "Document": "1 Introduction\nEffort and Cost\nFigure 1: The classification of XAI evaluation into human-grounded and functionality-grounded evaluation, adapted from the classification framework by Doshi-Velez & Kim (2017) and its visualization by Zhou et al. (2021).\nFigure 2: The heatmaps show two alternative example explanantia , indicating which input regions 1 were deemed decisive for the model's decision (the explanandum ). 1 A qualitative inspection allows for multiple interpretations, as it is unclear whether a) both the model and the explanation process (explanation ) are correct or flawed (top), or b) one is correct and the other failed (bottom). 1\nboth cases, the consequences can be severe, either reducing trust in a well-functioning model or, more critically, reinforcing trust in a flawed one. Further, human evaluation, especially through the system's developers, is prone to confirmation bias (Doshi-Velez & Kim, 2017; Lipton, 2018)",
      "Topic": 0,
      "Name": "0_al_explanation_et_xai",
      "Representation": [
        "al",
        "explanation",
        "et",
        "xai",
        "desiderata",
        "evaluation",
        "metrics",
        "2024",
        "explanations",
        "model",
        "2023",
        "explanans",
        "2021",
        "human",
        "grounded",
        "framework"
      ],
      "Representative_Docs": [
        "2 Related Work\nThere are five reviews, that we consider most similar to this work, as they present systematic functionality-grounded VXAI surveys. Le et al. (2023) and Nauta et al. (2023) both categorize the identified metrics based on a scheme of 12 properties, namely the Co-12 framework, which we discuss in more detail in Section 3. However, Le et al. (2023) restrict their analysis to metrics available through public XAI or VXAI toolkits (e.g., Quantus (Hedström et al., 2023)) and do not report on metrics introduced in the literature but not implemented in such libraries. Although there is some overlap with the study by Nauta et al. (2023), particularly in the inclusion of some identical metrics, their review also incorporates studies that merely apply VXAI metrics rather than introducing them. In contrast, our work provides detailed descriptions and categorizations of each identified metric. The review by Kadir et al. (2023) covers a broad range of domains and explanation types 2 and reports a wide variety of metrics. It also groups several metrics by method, a strategy shared by",
        "1 Introduction\nHowever, XAI is not a silver bullet. Van der Waa et al. (2021) point out, that humans tend to trust predictions more readily when an explanation is provided, often without carefully examining the explanation itself. This lack of critical scrutiny can lead to unwarranted trust, especially when decisions are taken based on incorrect or misleading explanations (Eiband et al., 2019; Jesus et al., 2021). To make matters worse, different XAI algorithms may result in conflicting explanations for the same model and sample (Krishna et al., 2022). Therefore, simply providing any explanation is not sufficient, but it is important to assess the quality of the explanation at hand (Sovrano et al., 2021). Unfortunately, while there is a plethora of XAI methods, evaluation of explanations is still an immature research area (Ribera & Lapedriza, 2019), with many studies relying on the notion of a good explanation as 'You'll know it when you see it', providing anecdotal evidence (i.e. small-scale qualitative validation) (Doshi-Velez & Kim, 2017; Nauta et al., 2023; Saarela &",
        "1 Introduction\n(Payrovnaziri et al., 2020), data and knowledge engineering (Li et al., 2020b), or timeseries classification (Theissler et al., 2022). Others focus on particular XAI approaches, such as visual explanations in CNNs (Mohamed et al., 2022) or instance-based explanations (Bayrak & Bach, 2024). Moreover, many surveys dedicate only limited attention to evaluation metrics, with their primary focus placed on the XAI methods themselves (Carvalho et al., 2019; Ding et al., 2022; Minh et al., 2022; Mohamed et al., 2022; Ali et al., 2023; Clement et al., 2023; Patrício et al., 2023; Chaddad et al., 2024; Gongane et al., 2024; Xua & Yang, 2024). By contrast, this review focuses exclusively on functionality-grounded evaluation across domains and is applicable to a wide range of XAI approaches."
      ],
      "Top_n_words": "al - explanation - et - xai - desiderata - evaluation - metrics - 2024 - explanations - model - 2023 - explanans - 2021 - human - grounded - framework",
      "Probability": 0.6239214331832077,
      "Representative_document": false
    },
    {
      "Document": "1 Introduction\nThere exist a number of surveys and guidelines that address human-centered evaluations (Hoffman et al., 2018; Miller, 2019; Chromik & Schuessler, 2020; Holzinger et al., 2020; Franklin & Lagnado, 2022; Hsiao et al., 2021; Jesus et al., 2021; Langer et al., 2021; Mohseni et al., 2021; van der Waa et al., 2021; Silva et al., 2023). Unfortunately, the range of reviews dedicated to functionality-grounded evaluation is still limited. This is despite the advantage of offering objective, quantitative metrics without requiring human experiments, which can save both time and cost (Doshi-Velez & Kim, 2017; Samek et al., 2019; Zhou et al., 2021). Most existing studies are narrow and restricted to a specific application domain (Giuste et al., 2022; Arreche et al., 2024), including cybersecurity (Pawlicki et al., 2024), medical image classification (Patrício et al., 2023; Chaddad et al., 2024), electronic health record data",
      "Topic": -1,
      "Name": "-1_et_al_yang_2021",
      "Representation": [
        "et",
        "al",
        "yang",
        "2021",
        "metrics",
        "vxai",
        "desiderata",
        "2019",
        "narrow",
        "text",
        "sources",
        "2024",
        "listed",
        "classification",
        "frameworks",
        "2020"
      ],
      "Representative_Docs": [
        "3.2 Proposed Framework of Desiderata\nBuilding on the desiderata frameworks established above and our findings on VXAI metrics, we propose a set of seven desiderata to serve as a categorization scheme for VXAI.\nOur goal is to offer a principled yet practical structure that enables consistent classification while avoiding the limitations of prior frameworks. These are either too narrow to accommodate relevant metrics or too broad and include properties beyond explainability. While properties such as fairness are often measured using XAI methods, we consider them beyond the scope of VXAI, because they assess the model's behavior rather than the explanation itself.",
        "1 Introduction\nThere exist a number of surveys and guidelines that address human-centered evaluations (Hoffman et al., 2018; Miller, 2019; Chromik & Schuessler, 2020; Holzinger et al., 2020; Franklin & Lagnado, 2022; Hsiao et al., 2021; Jesus et al., 2021; Langer et al., 2021; Mohseni et al., 2021; van der Waa et al., 2021; Silva et al., 2023). Unfortunately, the range of reviews dedicated to functionality-grounded evaluation is still limited. This is despite the advantage of offering objective, quantitative metrics without requiring human experiments, which can save both time and cost (Doshi-Velez & Kim, 2017; Samek et al., 2019; Zhou et al., 2021). Most existing studies are narrow and restricted to a specific application domain (Giuste et al., 2022; Arreche et al., 2024), including cybersecurity (Pawlicki et al., 2024), medical image classification (Patrício et al., 2023; Chaddad et al., 2024), electronic health record data",
        "3 Desiderata of XAI\n& Müller, Reported Metrics = 16 sources in text (metrics not listed) 40 sources in text. Yang et al., Primary VXAI Focus = ✓. Yang et al., Primarily Functionality- Grounded VXAI = ✓. Yang et al., (Semi-)Systematic Review = ✓. Yang et al., Date ↓ = Aug 2019. Yang et al., Desiderata = Generalizability, Fidelity, Persuasibility. Yang et al., Limited to = . Yang et al., Reported Metrics = (metrics not listed)\n† Co-12: Correctness, Output-Completeness, Consistency, Continuity, Contrastivity, Covariate Complexity, Compactness, Composition, Confidence, Context, Coherence, Controllability"
      ],
      "Top_n_words": "et - al - yang - 2021 - metrics - vxai - desiderata - 2019 - narrow - text - sources - 2024 - listed - classification - frameworks - 2020",
      "Probability": 0.2380935209535835,
      "Representative_document": true
    },
    {
      "Document": "1 Introduction\n(Payrovnaziri et al., 2020), data and knowledge engineering (Li et al., 2020b), or timeseries classification (Theissler et al., 2022). Others focus on particular XAI approaches, such as visual explanations in CNNs (Mohamed et al., 2022) or instance-based explanations (Bayrak & Bach, 2024). Moreover, many surveys dedicate only limited attention to evaluation metrics, with their primary focus placed on the XAI methods themselves (Carvalho et al., 2019; Ding et al., 2022; Minh et al., 2022; Mohamed et al., 2022; Ali et al., 2023; Clement et al., 2023; Patrício et al., 2023; Chaddad et al., 2024; Gongane et al., 2024; Xua & Yang, 2024). By contrast, this review focuses exclusively on functionality-grounded evaluation across domains and is applicable to a wide range of XAI approaches.",
      "Topic": 0,
      "Name": "0_al_explanation_et_xai",
      "Representation": [
        "al",
        "explanation",
        "et",
        "xai",
        "desiderata",
        "evaluation",
        "metrics",
        "2024",
        "explanations",
        "model",
        "2023",
        "explanans",
        "2021",
        "human",
        "grounded",
        "framework"
      ],
      "Representative_Docs": [
        "2 Related Work\nThere are five reviews, that we consider most similar to this work, as they present systematic functionality-grounded VXAI surveys. Le et al. (2023) and Nauta et al. (2023) both categorize the identified metrics based on a scheme of 12 properties, namely the Co-12 framework, which we discuss in more detail in Section 3. However, Le et al. (2023) restrict their analysis to metrics available through public XAI or VXAI toolkits (e.g., Quantus (Hedström et al., 2023)) and do not report on metrics introduced in the literature but not implemented in such libraries. Although there is some overlap with the study by Nauta et al. (2023), particularly in the inclusion of some identical metrics, their review also incorporates studies that merely apply VXAI metrics rather than introducing them. In contrast, our work provides detailed descriptions and categorizations of each identified metric. The review by Kadir et al. (2023) covers a broad range of domains and explanation types 2 and reports a wide variety of metrics. It also groups several metrics by method, a strategy shared by",
        "1 Introduction\nHowever, XAI is not a silver bullet. Van der Waa et al. (2021) point out, that humans tend to trust predictions more readily when an explanation is provided, often without carefully examining the explanation itself. This lack of critical scrutiny can lead to unwarranted trust, especially when decisions are taken based on incorrect or misleading explanations (Eiband et al., 2019; Jesus et al., 2021). To make matters worse, different XAI algorithms may result in conflicting explanations for the same model and sample (Krishna et al., 2022). Therefore, simply providing any explanation is not sufficient, but it is important to assess the quality of the explanation at hand (Sovrano et al., 2021). Unfortunately, while there is a plethora of XAI methods, evaluation of explanations is still an immature research area (Ribera & Lapedriza, 2019), with many studies relying on the notion of a good explanation as 'You'll know it when you see it', providing anecdotal evidence (i.e. small-scale qualitative validation) (Doshi-Velez & Kim, 2017; Nauta et al., 2023; Saarela &",
        "1 Introduction\n(Payrovnaziri et al., 2020), data and knowledge engineering (Li et al., 2020b), or timeseries classification (Theissler et al., 2022). Others focus on particular XAI approaches, such as visual explanations in CNNs (Mohamed et al., 2022) or instance-based explanations (Bayrak & Bach, 2024). Moreover, many surveys dedicate only limited attention to evaluation metrics, with their primary focus placed on the XAI methods themselves (Carvalho et al., 2019; Ding et al., 2022; Minh et al., 2022; Mohamed et al., 2022; Ali et al., 2023; Clement et al., 2023; Patrício et al., 2023; Chaddad et al., 2024; Gongane et al., 2024; Xua & Yang, 2024). By contrast, this review focuses exclusively on functionality-grounded evaluation across domains and is applicable to a wide range of XAI approaches."
      ],
      "Top_n_words": "al - explanation - et - xai - desiderata - evaluation - metrics - 2024 - explanations - model - 2023 - explanans - 2021 - human - grounded - framework",
      "Probability": 1.0,
      "Representative_document": true
    },
    {
      "Document": "Contributions\nDespite the growing number of proposed metrics, a comprehensive and unified framework for functionality-grounded evaluation is still missing. Further, the inconsistent use of terms such as interpretability, comprehensibility, understandability, transparency, and explainability (Koh & Liang, 2017; Guidotti et al., 2018; Arrieta et al., 2020; Markus et al., 2021) hampers conceptual clarity and comparability across approaches. To address this gap, we introduce a framework called eValuation of Explainable Artificial Intelligence (VXAI) , aimed at unifying functionality-grounded evaluation for XAI.\nOur contributions are as follows:",
      "Topic": 0,
      "Name": "0_al_explanation_et_xai",
      "Representation": [
        "al",
        "explanation",
        "et",
        "xai",
        "desiderata",
        "evaluation",
        "metrics",
        "2024",
        "explanations",
        "model",
        "2023",
        "explanans",
        "2021",
        "human",
        "grounded",
        "framework"
      ],
      "Representative_Docs": [
        "2 Related Work\nThere are five reviews, that we consider most similar to this work, as they present systematic functionality-grounded VXAI surveys. Le et al. (2023) and Nauta et al. (2023) both categorize the identified metrics based on a scheme of 12 properties, namely the Co-12 framework, which we discuss in more detail in Section 3. However, Le et al. (2023) restrict their analysis to metrics available through public XAI or VXAI toolkits (e.g., Quantus (Hedström et al., 2023)) and do not report on metrics introduced in the literature but not implemented in such libraries. Although there is some overlap with the study by Nauta et al. (2023), particularly in the inclusion of some identical metrics, their review also incorporates studies that merely apply VXAI metrics rather than introducing them. In contrast, our work provides detailed descriptions and categorizations of each identified metric. The review by Kadir et al. (2023) covers a broad range of domains and explanation types 2 and reports a wide variety of metrics. It also groups several metrics by method, a strategy shared by",
        "1 Introduction\nHowever, XAI is not a silver bullet. Van der Waa et al. (2021) point out, that humans tend to trust predictions more readily when an explanation is provided, often without carefully examining the explanation itself. This lack of critical scrutiny can lead to unwarranted trust, especially when decisions are taken based on incorrect or misleading explanations (Eiband et al., 2019; Jesus et al., 2021). To make matters worse, different XAI algorithms may result in conflicting explanations for the same model and sample (Krishna et al., 2022). Therefore, simply providing any explanation is not sufficient, but it is important to assess the quality of the explanation at hand (Sovrano et al., 2021). Unfortunately, while there is a plethora of XAI methods, evaluation of explanations is still an immature research area (Ribera & Lapedriza, 2019), with many studies relying on the notion of a good explanation as 'You'll know it when you see it', providing anecdotal evidence (i.e. small-scale qualitative validation) (Doshi-Velez & Kim, 2017; Nauta et al., 2023; Saarela &",
        "1 Introduction\n(Payrovnaziri et al., 2020), data and knowledge engineering (Li et al., 2020b), or timeseries classification (Theissler et al., 2022). Others focus on particular XAI approaches, such as visual explanations in CNNs (Mohamed et al., 2022) or instance-based explanations (Bayrak & Bach, 2024). Moreover, many surveys dedicate only limited attention to evaluation metrics, with their primary focus placed on the XAI methods themselves (Carvalho et al., 2019; Ding et al., 2022; Minh et al., 2022; Mohamed et al., 2022; Ali et al., 2023; Clement et al., 2023; Patrício et al., 2023; Chaddad et al., 2024; Gongane et al., 2024; Xua & Yang, 2024). By contrast, this review focuses exclusively on functionality-grounded evaluation across domains and is applicable to a wide range of XAI approaches."
      ],
      "Top_n_words": "al - explanation - et - xai - desiderata - evaluation - metrics - 2024 - explanations - model - 2023 - explanans - 2021 - human - grounded - framework",
      "Probability": 1.0,
      "Representative_document": false
    },
    {
      "Document": "Contributions\n- · We perform a systematic literature review based on the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines by Page et al. (2021), identifying 362 relevant publications that introduce or utilize evaluation metrics.\n- · We propose a three-dimensional categorization scheme consisting of desiderata, explanation type, and evaluation contextuality, and use it to organize the identified metrics.\n- · We aggregate these into 41 functionally similar metric groups, capturing common methodological patterns across the literature.\n- · To our knowledge, this results in the most comprehensive and unified VXAI framework to date and provides an extensible foundation for future research.",
      "Topic": 1,
      "Name": "1_et_al_vxai_metrics",
      "Representation": [
        "et",
        "al",
        "vxai",
        "metrics",
        "desiderata",
        "systematic",
        "primarily",
        "semi",
        "date",
        "primary",
        "limited",
        "review",
        "focus",
        "functionality",
        "reported",
        "grounded"
      ],
      "Representative_Docs": [
        "3 Desiderata of XAI\nal., Desiderata = Correctness, Comprehensibility, Stability. Alangari et al., Limited to = . Alangari et al., Reported Metrics = 59 metrics. Le et al., Primary VXAI Focus = ✓. Le et al., Primarily Functionality- Grounded VXAI = ✓. Le et al., (Semi-)Systematic Review = ✓. Le et al., Date ↓ = Aug 2023. Le et al., Desiderata = Co-12 †. Le et al., Limited to = . Le et al., Reported Metrics = 86 metrics from 17 toolkits. Salih et al., Primary VXAI Focus = ✓. Salih et al., Primarily Functionality- Grounded VXAI = . Salih et al., (Semi-)Systematic Review = ✓. Salih et al., Date ↓ = Aug 2023. Salih et al., Desiderata = . Salih et al., Limited to = Cardiology. Salih et al., Reported Metrics = 27 metrics. Kadir et al., Primary VXAI Focus = ✓.",
        "3 Desiderata of XAI\net al., Primarily Functionality- Grounded VXAI = ✓. Bodria et al., (Semi-)Systematic Review = ✓. Bodria et al., Date ↓ = Nov 2021. Bodria et al., Desiderata = . Bodria et al., Limited to = . Bodria et al., Reported Metrics = 6 metrics. Sovrano et al., Primary VXAI Focus = ✓. Sovrano et al., Primarily Functionality- Grounded VXAI = . Sovrano et al., (Semi-)Systematic Review = . Sovrano et al., Date ↓ = Oct 2021. Sovrano et al., Desiderata = Similarity, Exactness, Fruitfulness. Sovrano et al., Limited to = . Sovrano et al., Reported Metrics = 22 metrics. Ras et al., Primary VXAI Focus = . Ras et al., Primarily Functionality- Grounded VXAI = . Ras et al., (Semi-)Systematic Review = . Ras et al., Date ↓ = Sep 2021. Ras et al., Desiderata = . Ras et",
        "3 Desiderata of XAI\net al., Primarily Functionality- Grounded VXAI = ✓. Bommer et al., (Semi-)Systematic Review = . Bommer et al., Date ↓ = Mar 2024. Bommer et al., Desiderata = Robustness, Faithfulness, Complexity, Lo- calization, Randomization. Bommer et al., Limited to = Climate Science. Bommer et al., Reported Metrics = 10 metrics. Li et al., Primary VXAI Focus = ✓. Li et al., Primarily Functionality- Grounded VXAI = ✓. Li et al., (Semi-)Systematic Review = . Li et al., Date ↓ = Dec 2023. Li et al., Desiderata = Faithfulness. Li et al., Limited to = Feature Attributions. Li et al., Reported Metrics = 6 metrics. Alangari et al., Primary VXAI Focus = ✓. Alangari et al., Primarily Functionality- Grounded VXAI = . Alangari et al., (Semi-)Systematic Review = . Alangari et al., Date ↓ = Aug 2023. Alangari et"
      ],
      "Top_n_words": "et - al - vxai - metrics - desiderata - systematic - primarily - semi - date - primary - limited - review - focus - functionality - reported - grounded",
      "Probability": 1.0,
      "Representative_document": false
    },
    {
      "Document": "Contributions\nThe remainder of this review is structured as follows: In Section 2, we first present related studies on the topic of VXAI to motivate the need for this systematic review, before introducing the desiderata of XAI in Section 3. Further, Section 4 introduces our research method used to search for and identify relevant metrics. The categorization scheme and results are presented in Section 5, where we introduce our categorization framework (Subsection 5.1) and summarize the identified metrics (Subsection 5.2), complemented by a visual overview in Figure 6. A deeper discussion of these findings is provided in Section 6, while comprehensive descriptions of the metrics alongside references are listed in Appendix B. We conclude the review in Section 7, discussing the results and future paths for the area of VXAI.",
      "Topic": 0,
      "Name": "0_al_explanation_et_xai",
      "Representation": [
        "al",
        "explanation",
        "et",
        "xai",
        "desiderata",
        "evaluation",
        "metrics",
        "2024",
        "explanations",
        "model",
        "2023",
        "explanans",
        "2021",
        "human",
        "grounded",
        "framework"
      ],
      "Representative_Docs": [
        "2 Related Work\nThere are five reviews, that we consider most similar to this work, as they present systematic functionality-grounded VXAI surveys. Le et al. (2023) and Nauta et al. (2023) both categorize the identified metrics based on a scheme of 12 properties, namely the Co-12 framework, which we discuss in more detail in Section 3. However, Le et al. (2023) restrict their analysis to metrics available through public XAI or VXAI toolkits (e.g., Quantus (Hedström et al., 2023)) and do not report on metrics introduced in the literature but not implemented in such libraries. Although there is some overlap with the study by Nauta et al. (2023), particularly in the inclusion of some identical metrics, their review also incorporates studies that merely apply VXAI metrics rather than introducing them. In contrast, our work provides detailed descriptions and categorizations of each identified metric. The review by Kadir et al. (2023) covers a broad range of domains and explanation types 2 and reports a wide variety of metrics. It also groups several metrics by method, a strategy shared by",
        "1 Introduction\nHowever, XAI is not a silver bullet. Van der Waa et al. (2021) point out, that humans tend to trust predictions more readily when an explanation is provided, often without carefully examining the explanation itself. This lack of critical scrutiny can lead to unwarranted trust, especially when decisions are taken based on incorrect or misleading explanations (Eiband et al., 2019; Jesus et al., 2021). To make matters worse, different XAI algorithms may result in conflicting explanations for the same model and sample (Krishna et al., 2022). Therefore, simply providing any explanation is not sufficient, but it is important to assess the quality of the explanation at hand (Sovrano et al., 2021). Unfortunately, while there is a plethora of XAI methods, evaluation of explanations is still an immature research area (Ribera & Lapedriza, 2019), with many studies relying on the notion of a good explanation as 'You'll know it when you see it', providing anecdotal evidence (i.e. small-scale qualitative validation) (Doshi-Velez & Kim, 2017; Nauta et al., 2023; Saarela &",
        "1 Introduction\n(Payrovnaziri et al., 2020), data and knowledge engineering (Li et al., 2020b), or timeseries classification (Theissler et al., 2022). Others focus on particular XAI approaches, such as visual explanations in CNNs (Mohamed et al., 2022) or instance-based explanations (Bayrak & Bach, 2024). Moreover, many surveys dedicate only limited attention to evaluation metrics, with their primary focus placed on the XAI methods themselves (Carvalho et al., 2019; Ding et al., 2022; Minh et al., 2022; Mohamed et al., 2022; Ali et al., 2023; Clement et al., 2023; Patrício et al., 2023; Chaddad et al., 2024; Gongane et al., 2024; Xua & Yang, 2024). By contrast, this review focuses exclusively on functionality-grounded evaluation across domains and is applicable to a wide range of XAI approaches."
      ],
      "Top_n_words": "al - explanation - et - xai - desiderata - evaluation - metrics - 2024 - explanations - model - 2023 - explanans - 2021 - human - grounded - framework",
      "Probability": 0.4760080951832973,
      "Representative_document": false
    },
    {
      "Document": "Terminology\nTo avoid ambiguous language, throughout the paper we stick to the terminology of the XAI Handbook by Palacio et al. (2021): The goal of XAI is to facilitate understanding by providing insights into an explanandum ('What is to be explained'), usually a model or a model's decision. To accomplish this, we leverage an explanation , which is the process of getting insight into the explanandum. The resulting output of this process is the explanans , which provides the user with information about the model's inner workings. In a mathematical sense, the explanation can be viewed as a function that maps an explanandum to an explanans. For example, the explanandum could be a CNN's classification of a given input image. The explanation might be an algorithm such as GradCAM (Selvaraju et al., 2017), and the resulting heatmap is the explanans, which highlights important features. We use the Latin plural forms explananda (explanandum) and explanantia (explanans) throughout. When we refer to VXAI, we include both the evaluation of the",
      "Topic": 0,
      "Name": "0_al_explanation_et_xai",
      "Representation": [
        "al",
        "explanation",
        "et",
        "xai",
        "desiderata",
        "evaluation",
        "metrics",
        "2024",
        "explanations",
        "model",
        "2023",
        "explanans",
        "2021",
        "human",
        "grounded",
        "framework"
      ],
      "Representative_Docs": [
        "2 Related Work\nThere are five reviews, that we consider most similar to this work, as they present systematic functionality-grounded VXAI surveys. Le et al. (2023) and Nauta et al. (2023) both categorize the identified metrics based on a scheme of 12 properties, namely the Co-12 framework, which we discuss in more detail in Section 3. However, Le et al. (2023) restrict their analysis to metrics available through public XAI or VXAI toolkits (e.g., Quantus (Hedström et al., 2023)) and do not report on metrics introduced in the literature but not implemented in such libraries. Although there is some overlap with the study by Nauta et al. (2023), particularly in the inclusion of some identical metrics, their review also incorporates studies that merely apply VXAI metrics rather than introducing them. In contrast, our work provides detailed descriptions and categorizations of each identified metric. The review by Kadir et al. (2023) covers a broad range of domains and explanation types 2 and reports a wide variety of metrics. It also groups several metrics by method, a strategy shared by",
        "1 Introduction\nHowever, XAI is not a silver bullet. Van der Waa et al. (2021) point out, that humans tend to trust predictions more readily when an explanation is provided, often without carefully examining the explanation itself. This lack of critical scrutiny can lead to unwarranted trust, especially when decisions are taken based on incorrect or misleading explanations (Eiband et al., 2019; Jesus et al., 2021). To make matters worse, different XAI algorithms may result in conflicting explanations for the same model and sample (Krishna et al., 2022). Therefore, simply providing any explanation is not sufficient, but it is important to assess the quality of the explanation at hand (Sovrano et al., 2021). Unfortunately, while there is a plethora of XAI methods, evaluation of explanations is still an immature research area (Ribera & Lapedriza, 2019), with many studies relying on the notion of a good explanation as 'You'll know it when you see it', providing anecdotal evidence (i.e. small-scale qualitative validation) (Doshi-Velez & Kim, 2017; Nauta et al., 2023; Saarela &",
        "1 Introduction\n(Payrovnaziri et al., 2020), data and knowledge engineering (Li et al., 2020b), or timeseries classification (Theissler et al., 2022). Others focus on particular XAI approaches, such as visual explanations in CNNs (Mohamed et al., 2022) or instance-based explanations (Bayrak & Bach, 2024). Moreover, many surveys dedicate only limited attention to evaluation metrics, with their primary focus placed on the XAI methods themselves (Carvalho et al., 2019; Ding et al., 2022; Minh et al., 2022; Mohamed et al., 2022; Ali et al., 2023; Clement et al., 2023; Patrício et al., 2023; Chaddad et al., 2024; Gongane et al., 2024; Xua & Yang, 2024). By contrast, this review focuses exclusively on functionality-grounded evaluation across domains and is applicable to a wide range of XAI approaches."
      ],
      "Top_n_words": "al - explanation - et - xai - desiderata - evaluation - metrics - 2024 - explanations - model - 2023 - explanans - 2021 - human - grounded - framework",
      "Probability": 0.9042000022838359,
      "Representative_document": false
    },
    {
      "Document": "Terminology\nmethod (explanation) and its output (explanans), since most evaluation metrics necessarily assess the quality of explanations through the quality of their generated outputs.\nAs defined by the XAI Handbook, interpretation (or interpretability ) refers to the subsequent assignment of meaning to an explanation. It describes the process through which a human infers knowledge about the explanandum using the explanans. This step significantly influences the success of the explanation and also depends on the receiving human's (the explainee 's) mental model.\nexplanandum (pl. explananda ): What is to be explained, i.e. a model and its prediction. explanation (pl. explanations ): The process of explaining, i.e. the XAI algorithm. explanans (pl. explanantia ): The explaining information, i.e. the output of an explanation.",
      "Topic": 0,
      "Name": "0_al_explanation_et_xai",
      "Representation": [
        "al",
        "explanation",
        "et",
        "xai",
        "desiderata",
        "evaluation",
        "metrics",
        "2024",
        "explanations",
        "model",
        "2023",
        "explanans",
        "2021",
        "human",
        "grounded",
        "framework"
      ],
      "Representative_Docs": [
        "2 Related Work\nThere are five reviews, that we consider most similar to this work, as they present systematic functionality-grounded VXAI surveys. Le et al. (2023) and Nauta et al. (2023) both categorize the identified metrics based on a scheme of 12 properties, namely the Co-12 framework, which we discuss in more detail in Section 3. However, Le et al. (2023) restrict their analysis to metrics available through public XAI or VXAI toolkits (e.g., Quantus (Hedström et al., 2023)) and do not report on metrics introduced in the literature but not implemented in such libraries. Although there is some overlap with the study by Nauta et al. (2023), particularly in the inclusion of some identical metrics, their review also incorporates studies that merely apply VXAI metrics rather than introducing them. In contrast, our work provides detailed descriptions and categorizations of each identified metric. The review by Kadir et al. (2023) covers a broad range of domains and explanation types 2 and reports a wide variety of metrics. It also groups several metrics by method, a strategy shared by",
        "1 Introduction\nHowever, XAI is not a silver bullet. Van der Waa et al. (2021) point out, that humans tend to trust predictions more readily when an explanation is provided, often without carefully examining the explanation itself. This lack of critical scrutiny can lead to unwarranted trust, especially when decisions are taken based on incorrect or misleading explanations (Eiband et al., 2019; Jesus et al., 2021). To make matters worse, different XAI algorithms may result in conflicting explanations for the same model and sample (Krishna et al., 2022). Therefore, simply providing any explanation is not sufficient, but it is important to assess the quality of the explanation at hand (Sovrano et al., 2021). Unfortunately, while there is a plethora of XAI methods, evaluation of explanations is still an immature research area (Ribera & Lapedriza, 2019), with many studies relying on the notion of a good explanation as 'You'll know it when you see it', providing anecdotal evidence (i.e. small-scale qualitative validation) (Doshi-Velez & Kim, 2017; Nauta et al., 2023; Saarela &",
        "1 Introduction\n(Payrovnaziri et al., 2020), data and knowledge engineering (Li et al., 2020b), or timeseries classification (Theissler et al., 2022). Others focus on particular XAI approaches, such as visual explanations in CNNs (Mohamed et al., 2022) or instance-based explanations (Bayrak & Bach, 2024). Moreover, many surveys dedicate only limited attention to evaluation metrics, with their primary focus placed on the XAI methods themselves (Carvalho et al., 2019; Ding et al., 2022; Minh et al., 2022; Mohamed et al., 2022; Ali et al., 2023; Clement et al., 2023; Patrício et al., 2023; Chaddad et al., 2024; Gongane et al., 2024; Xua & Yang, 2024). By contrast, this review focuses exclusively on functionality-grounded evaluation across domains and is applicable to a wide range of XAI approaches."
      ],
      "Top_n_words": "al - explanation - et - xai - desiderata - evaluation - metrics - 2024 - explanations - model - 2023 - explanans - 2021 - human - grounded - framework",
      "Probability": 1.0,
      "Representative_document": false
    },
    {
      "Document": "2 Related Work\nAlthough the field of XAI has gained popularity over the past years, there is still no extensive and unified evaluation framework for XAI metrics. Various surveys have explored XAI and VXAI from different angles, ranging from human-grounded evaluation to technical metrics. Table 1 gives an overview over 30 such XAI reviews from the past years.",
      "Topic": 0,
      "Name": "0_al_explanation_et_xai",
      "Representation": [
        "al",
        "explanation",
        "et",
        "xai",
        "desiderata",
        "evaluation",
        "metrics",
        "2024",
        "explanations",
        "model",
        "2023",
        "explanans",
        "2021",
        "human",
        "grounded",
        "framework"
      ],
      "Representative_Docs": [
        "2 Related Work\nThere are five reviews, that we consider most similar to this work, as they present systematic functionality-grounded VXAI surveys. Le et al. (2023) and Nauta et al. (2023) both categorize the identified metrics based on a scheme of 12 properties, namely the Co-12 framework, which we discuss in more detail in Section 3. However, Le et al. (2023) restrict their analysis to metrics available through public XAI or VXAI toolkits (e.g., Quantus (Hedström et al., 2023)) and do not report on metrics introduced in the literature but not implemented in such libraries. Although there is some overlap with the study by Nauta et al. (2023), particularly in the inclusion of some identical metrics, their review also incorporates studies that merely apply VXAI metrics rather than introducing them. In contrast, our work provides detailed descriptions and categorizations of each identified metric. The review by Kadir et al. (2023) covers a broad range of domains and explanation types 2 and reports a wide variety of metrics. It also groups several metrics by method, a strategy shared by",
        "1 Introduction\nHowever, XAI is not a silver bullet. Van der Waa et al. (2021) point out, that humans tend to trust predictions more readily when an explanation is provided, often without carefully examining the explanation itself. This lack of critical scrutiny can lead to unwarranted trust, especially when decisions are taken based on incorrect or misleading explanations (Eiband et al., 2019; Jesus et al., 2021). To make matters worse, different XAI algorithms may result in conflicting explanations for the same model and sample (Krishna et al., 2022). Therefore, simply providing any explanation is not sufficient, but it is important to assess the quality of the explanation at hand (Sovrano et al., 2021). Unfortunately, while there is a plethora of XAI methods, evaluation of explanations is still an immature research area (Ribera & Lapedriza, 2019), with many studies relying on the notion of a good explanation as 'You'll know it when you see it', providing anecdotal evidence (i.e. small-scale qualitative validation) (Doshi-Velez & Kim, 2017; Nauta et al., 2023; Saarela &",
        "1 Introduction\n(Payrovnaziri et al., 2020), data and knowledge engineering (Li et al., 2020b), or timeseries classification (Theissler et al., 2022). Others focus on particular XAI approaches, such as visual explanations in CNNs (Mohamed et al., 2022) or instance-based explanations (Bayrak & Bach, 2024). Moreover, many surveys dedicate only limited attention to evaluation metrics, with their primary focus placed on the XAI methods themselves (Carvalho et al., 2019; Ding et al., 2022; Minh et al., 2022; Mohamed et al., 2022; Ali et al., 2023; Clement et al., 2023; Patrício et al., 2023; Chaddad et al., 2024; Gongane et al., 2024; Xua & Yang, 2024). By contrast, this review focuses exclusively on functionality-grounded evaluation across domains and is applicable to a wide range of XAI approaches."
      ],
      "Top_n_words": "al - explanation - et - xai - desiderata - evaluation - metrics - 2024 - explanations - model - 2023 - explanans - 2021 - human - grounded - framework",
      "Probability": 0.5118545731780892,
      "Representative_document": false
    },
    {
      "Document": "2 Related Work\nWhile evaluation of XAI is frequently given less attention in XAI surveys, 23 of these reviews directly focus on the topic of VXAI. Besides functionality-grounded evaluation, a second school of thought is concerned with human-grounded evaluation of explanations through qualitative expert evaluations or quantitative user studies, with representative surveys for this domain available as well (Sokol & Flach, 2020; Rawal et al., 2021; Naveed et al., 2024). Nevertheless, a considerable number of 19 reports focus specifically on the topic of functionality-grounded evaluation. Unfortunately, most of these surveys focus on a subset of well-known metrics, whereas only 14 surveys gathered VXAI metrics in a systematic or semi-systematic literature review. Further, numerous of the referenced reviews either lack an extensive list of desiderata and focus only on a subset of them, or limit their research to specific types of explanations 2 or application domains.",
      "Topic": 0,
      "Name": "0_al_explanation_et_xai",
      "Representation": [
        "al",
        "explanation",
        "et",
        "xai",
        "desiderata",
        "evaluation",
        "metrics",
        "2024",
        "explanations",
        "model",
        "2023",
        "explanans",
        "2021",
        "human",
        "grounded",
        "framework"
      ],
      "Representative_Docs": [
        "2 Related Work\nThere are five reviews, that we consider most similar to this work, as they present systematic functionality-grounded VXAI surveys. Le et al. (2023) and Nauta et al. (2023) both categorize the identified metrics based on a scheme of 12 properties, namely the Co-12 framework, which we discuss in more detail in Section 3. However, Le et al. (2023) restrict their analysis to metrics available through public XAI or VXAI toolkits (e.g., Quantus (Hedström et al., 2023)) and do not report on metrics introduced in the literature but not implemented in such libraries. Although there is some overlap with the study by Nauta et al. (2023), particularly in the inclusion of some identical metrics, their review also incorporates studies that merely apply VXAI metrics rather than introducing them. In contrast, our work provides detailed descriptions and categorizations of each identified metric. The review by Kadir et al. (2023) covers a broad range of domains and explanation types 2 and reports a wide variety of metrics. It also groups several metrics by method, a strategy shared by",
        "1 Introduction\nHowever, XAI is not a silver bullet. Van der Waa et al. (2021) point out, that humans tend to trust predictions more readily when an explanation is provided, often without carefully examining the explanation itself. This lack of critical scrutiny can lead to unwarranted trust, especially when decisions are taken based on incorrect or misleading explanations (Eiband et al., 2019; Jesus et al., 2021). To make matters worse, different XAI algorithms may result in conflicting explanations for the same model and sample (Krishna et al., 2022). Therefore, simply providing any explanation is not sufficient, but it is important to assess the quality of the explanation at hand (Sovrano et al., 2021). Unfortunately, while there is a plethora of XAI methods, evaluation of explanations is still an immature research area (Ribera & Lapedriza, 2019), with many studies relying on the notion of a good explanation as 'You'll know it when you see it', providing anecdotal evidence (i.e. small-scale qualitative validation) (Doshi-Velez & Kim, 2017; Nauta et al., 2023; Saarela &",
        "1 Introduction\n(Payrovnaziri et al., 2020), data and knowledge engineering (Li et al., 2020b), or timeseries classification (Theissler et al., 2022). Others focus on particular XAI approaches, such as visual explanations in CNNs (Mohamed et al., 2022) or instance-based explanations (Bayrak & Bach, 2024). Moreover, many surveys dedicate only limited attention to evaluation metrics, with their primary focus placed on the XAI methods themselves (Carvalho et al., 2019; Ding et al., 2022; Minh et al., 2022; Mohamed et al., 2022; Ali et al., 2023; Clement et al., 2023; Patrício et al., 2023; Chaddad et al., 2024; Gongane et al., 2024; Xua & Yang, 2024). By contrast, this review focuses exclusively on functionality-grounded evaluation across domains and is applicable to a wide range of XAI approaches."
      ],
      "Top_n_words": "al - explanation - et - xai - desiderata - evaluation - metrics - 2024 - explanations - model - 2023 - explanans - 2021 - human - grounded - framework",
      "Probability": 0.5747627887662411,
      "Representative_document": false
    },
    {
      "Document": "2 Related Work\nThere are five reviews, that we consider most similar to this work, as they present systematic functionality-grounded VXAI surveys. Le et al. (2023) and Nauta et al. (2023) both categorize the identified metrics based on a scheme of 12 properties, namely the Co-12 framework, which we discuss in more detail in Section 3. However, Le et al. (2023) restrict their analysis to metrics available through public XAI or VXAI toolkits (e.g., Quantus (Hedström et al., 2023)) and do not report on metrics introduced in the literature but not implemented in such libraries. Although there is some overlap with the study by Nauta et al. (2023), particularly in the inclusion of some identical metrics, their review also incorporates studies that merely apply VXAI metrics rather than introducing them. In contrast, our work provides detailed descriptions and categorizations of each identified metric. The review by Kadir et al. (2023) covers a broad range of domains and explanation types 2 and reports a wide variety of metrics. It also groups several metrics by method, a strategy shared by",
      "Topic": 0,
      "Name": "0_al_explanation_et_xai",
      "Representation": [
        "al",
        "explanation",
        "et",
        "xai",
        "desiderata",
        "evaluation",
        "metrics",
        "2024",
        "explanations",
        "model",
        "2023",
        "explanans",
        "2021",
        "human",
        "grounded",
        "framework"
      ],
      "Representative_Docs": [
        "2 Related Work\nThere are five reviews, that we consider most similar to this work, as they present systematic functionality-grounded VXAI surveys. Le et al. (2023) and Nauta et al. (2023) both categorize the identified metrics based on a scheme of 12 properties, namely the Co-12 framework, which we discuss in more detail in Section 3. However, Le et al. (2023) restrict their analysis to metrics available through public XAI or VXAI toolkits (e.g., Quantus (Hedström et al., 2023)) and do not report on metrics introduced in the literature but not implemented in such libraries. Although there is some overlap with the study by Nauta et al. (2023), particularly in the inclusion of some identical metrics, their review also incorporates studies that merely apply VXAI metrics rather than introducing them. In contrast, our work provides detailed descriptions and categorizations of each identified metric. The review by Kadir et al. (2023) covers a broad range of domains and explanation types 2 and reports a wide variety of metrics. It also groups several metrics by method, a strategy shared by",
        "1 Introduction\nHowever, XAI is not a silver bullet. Van der Waa et al. (2021) point out, that humans tend to trust predictions more readily when an explanation is provided, often without carefully examining the explanation itself. This lack of critical scrutiny can lead to unwarranted trust, especially when decisions are taken based on incorrect or misleading explanations (Eiband et al., 2019; Jesus et al., 2021). To make matters worse, different XAI algorithms may result in conflicting explanations for the same model and sample (Krishna et al., 2022). Therefore, simply providing any explanation is not sufficient, but it is important to assess the quality of the explanation at hand (Sovrano et al., 2021). Unfortunately, while there is a plethora of XAI methods, evaluation of explanations is still an immature research area (Ribera & Lapedriza, 2019), with many studies relying on the notion of a good explanation as 'You'll know it when you see it', providing anecdotal evidence (i.e. small-scale qualitative validation) (Doshi-Velez & Kim, 2017; Nauta et al., 2023; Saarela &",
        "1 Introduction\n(Payrovnaziri et al., 2020), data and knowledge engineering (Li et al., 2020b), or timeseries classification (Theissler et al., 2022). Others focus on particular XAI approaches, such as visual explanations in CNNs (Mohamed et al., 2022) or instance-based explanations (Bayrak & Bach, 2024). Moreover, many surveys dedicate only limited attention to evaluation metrics, with their primary focus placed on the XAI methods themselves (Carvalho et al., 2019; Ding et al., 2022; Minh et al., 2022; Mohamed et al., 2022; Ali et al., 2023; Clement et al., 2023; Patrício et al., 2023; Chaddad et al., 2024; Gongane et al., 2024; Xua & Yang, 2024). By contrast, this review focuses exclusively on functionality-grounded evaluation across domains and is applicable to a wide range of XAI approaches."
      ],
      "Top_n_words": "al - explanation - et - xai - desiderata - evaluation - metrics - 2024 - explanations - model - 2023 - explanans - 2021 - human - grounded - framework",
      "Probability": 0.49474984805772293,
      "Representative_document": true
    },
    {
      "Document": "2 Related Work\nour work. However, it does not adopt a categorization scheme based on the desiderata fulfilled by individual metrics. Notably, the recent reviews from Bayrak & Bach (2024) and Pawlicki et al. (2024) report a high number of individual metrics for VXAI. However, both limit the scope of their review considerably, either in terms of application domain (Pawlicki et al., 2024) or explanation type 2 (Bayrak & Bach, 2024). In contrast, our work includes all metrics reported to date and introduces a categorization scheme based on explicitly defined desiderata (see Section 3). Finally, many of the metrics we identified were introduced only recently, underlining the need for this more recent literature review.\nWhile previous reviews report between 10 and 90 individual metrics, our work introduces a unified structure by aggregating over 360 individual metrics into 41 conceptually related groups. This enables clearer comparison and interpretation across metrics. Unlike most surveys, we do not limit our analysis to specific explanation types or application domains, ensuring broader applicability across the XAI landscape.",
      "Topic": 0,
      "Name": "0_al_explanation_et_xai",
      "Representation": [
        "al",
        "explanation",
        "et",
        "xai",
        "desiderata",
        "evaluation",
        "metrics",
        "2024",
        "explanations",
        "model",
        "2023",
        "explanans",
        "2021",
        "human",
        "grounded",
        "framework"
      ],
      "Representative_Docs": [
        "2 Related Work\nThere are five reviews, that we consider most similar to this work, as they present systematic functionality-grounded VXAI surveys. Le et al. (2023) and Nauta et al. (2023) both categorize the identified metrics based on a scheme of 12 properties, namely the Co-12 framework, which we discuss in more detail in Section 3. However, Le et al. (2023) restrict their analysis to metrics available through public XAI or VXAI toolkits (e.g., Quantus (Hedström et al., 2023)) and do not report on metrics introduced in the literature but not implemented in such libraries. Although there is some overlap with the study by Nauta et al. (2023), particularly in the inclusion of some identical metrics, their review also incorporates studies that merely apply VXAI metrics rather than introducing them. In contrast, our work provides detailed descriptions and categorizations of each identified metric. The review by Kadir et al. (2023) covers a broad range of domains and explanation types 2 and reports a wide variety of metrics. It also groups several metrics by method, a strategy shared by",
        "1 Introduction\nHowever, XAI is not a silver bullet. Van der Waa et al. (2021) point out, that humans tend to trust predictions more readily when an explanation is provided, often without carefully examining the explanation itself. This lack of critical scrutiny can lead to unwarranted trust, especially when decisions are taken based on incorrect or misleading explanations (Eiband et al., 2019; Jesus et al., 2021). To make matters worse, different XAI algorithms may result in conflicting explanations for the same model and sample (Krishna et al., 2022). Therefore, simply providing any explanation is not sufficient, but it is important to assess the quality of the explanation at hand (Sovrano et al., 2021). Unfortunately, while there is a plethora of XAI methods, evaluation of explanations is still an immature research area (Ribera & Lapedriza, 2019), with many studies relying on the notion of a good explanation as 'You'll know it when you see it', providing anecdotal evidence (i.e. small-scale qualitative validation) (Doshi-Velez & Kim, 2017; Nauta et al., 2023; Saarela &",
        "1 Introduction\n(Payrovnaziri et al., 2020), data and knowledge engineering (Li et al., 2020b), or timeseries classification (Theissler et al., 2022). Others focus on particular XAI approaches, such as visual explanations in CNNs (Mohamed et al., 2022) or instance-based explanations (Bayrak & Bach, 2024). Moreover, many surveys dedicate only limited attention to evaluation metrics, with their primary focus placed on the XAI methods themselves (Carvalho et al., 2019; Ding et al., 2022; Minh et al., 2022; Mohamed et al., 2022; Ali et al., 2023; Clement et al., 2023; Patrício et al., 2023; Chaddad et al., 2024; Gongane et al., 2024; Xua & Yang, 2024). By contrast, this review focuses exclusively on functionality-grounded evaluation across domains and is applicable to a wide range of XAI approaches."
      ],
      "Top_n_words": "al - explanation - et - xai - desiderata - evaluation - metrics - 2024 - explanations - model - 2023 - explanans - 2021 - human - grounded - framework",
      "Probability": 0.5088213312835813,
      "Representative_document": false
    },
    {
      "Document": "3 Desiderata of XAI\nA well-founded evaluation of XAI methods requires clearly defined criteria for what constitutes a good explanation. To establish such criteria, we must first reflect on the role of explanations in the context of XAI. According to the definition in the XAI Handbook (Palacio et al., 2021), explaining a model and its behavior is a two-stage process: first, factual information about the model's decision process is generated (the explanans); this is then interpreted by the human user. The first stage can be evaluated using technical criteria that assess whether the model's reasoning has been captured truthfully and reliably. The second stage depends on the interpretability of the explanation, which can be assessed using general cognitive principles, even in the absence of a specific user model.\nTo capture the multifaceted nature of explanation quality, a number of desiderata have been proposed in the literature. We interpret these as functionality-grounded expectations that reflect the demands\n2 The explanation type refers to both the design of the explanation algorithm and, consequently, the nature of the resulting explanans, as introduced in Subsection 5.1.2.",
      "Topic": 0,
      "Name": "0_al_explanation_et_xai",
      "Representation": [
        "al",
        "explanation",
        "et",
        "xai",
        "desiderata",
        "evaluation",
        "metrics",
        "2024",
        "explanations",
        "model",
        "2023",
        "explanans",
        "2021",
        "human",
        "grounded",
        "framework"
      ],
      "Representative_Docs": [
        "2 Related Work\nThere are five reviews, that we consider most similar to this work, as they present systematic functionality-grounded VXAI surveys. Le et al. (2023) and Nauta et al. (2023) both categorize the identified metrics based on a scheme of 12 properties, namely the Co-12 framework, which we discuss in more detail in Section 3. However, Le et al. (2023) restrict their analysis to metrics available through public XAI or VXAI toolkits (e.g., Quantus (Hedström et al., 2023)) and do not report on metrics introduced in the literature but not implemented in such libraries. Although there is some overlap with the study by Nauta et al. (2023), particularly in the inclusion of some identical metrics, their review also incorporates studies that merely apply VXAI metrics rather than introducing them. In contrast, our work provides detailed descriptions and categorizations of each identified metric. The review by Kadir et al. (2023) covers a broad range of domains and explanation types 2 and reports a wide variety of metrics. It also groups several metrics by method, a strategy shared by",
        "1 Introduction\nHowever, XAI is not a silver bullet. Van der Waa et al. (2021) point out, that humans tend to trust predictions more readily when an explanation is provided, often without carefully examining the explanation itself. This lack of critical scrutiny can lead to unwarranted trust, especially when decisions are taken based on incorrect or misleading explanations (Eiband et al., 2019; Jesus et al., 2021). To make matters worse, different XAI algorithms may result in conflicting explanations for the same model and sample (Krishna et al., 2022). Therefore, simply providing any explanation is not sufficient, but it is important to assess the quality of the explanation at hand (Sovrano et al., 2021). Unfortunately, while there is a plethora of XAI methods, evaluation of explanations is still an immature research area (Ribera & Lapedriza, 2019), with many studies relying on the notion of a good explanation as 'You'll know it when you see it', providing anecdotal evidence (i.e. small-scale qualitative validation) (Doshi-Velez & Kim, 2017; Nauta et al., 2023; Saarela &",
        "1 Introduction\n(Payrovnaziri et al., 2020), data and knowledge engineering (Li et al., 2020b), or timeseries classification (Theissler et al., 2022). Others focus on particular XAI approaches, such as visual explanations in CNNs (Mohamed et al., 2022) or instance-based explanations (Bayrak & Bach, 2024). Moreover, many surveys dedicate only limited attention to evaluation metrics, with their primary focus placed on the XAI methods themselves (Carvalho et al., 2019; Ding et al., 2022; Minh et al., 2022; Mohamed et al., 2022; Ali et al., 2023; Clement et al., 2023; Patrício et al., 2023; Chaddad et al., 2024; Gongane et al., 2024; Xua & Yang, 2024). By contrast, this review focuses exclusively on functionality-grounded evaluation across domains and is applicable to a wide range of XAI approaches."
      ],
      "Top_n_words": "al - explanation - et - xai - desiderata - evaluation - metrics - 2024 - explanations - model - 2023 - explanans - 2021 - human - grounded - framework",
      "Probability": 1.0,
      "Representative_document": false
    },
    {
      "Document": "3 Desiderata of XAI\nThis work, Primary VXAI Focus = ✓. This work, Primarily Functionality- Grounded VXAI = ✓. This work, (Semi-)Systematic Review = ✓. This work, Date ↓ = Jan 2025. This work, Desiderata = Parsimony, Plausibility, Coverage, Fidelity, Continuity, Consistency, Effi- ciency. This work, Limited to = . This work, Reported Metrics = 41 metrics from 362 sources. Klein et al., Primary VXAI Focus = ✓. Klein et al., Primarily Functionality- Grounded VXAI = ✓. Klein et al., (Semi-)Systematic Review = . Klein et al., Date ↓ = Jan 2025. Klein et al., Desiderata = Faithfulness, Robustness, Complexity. Klein et al., Limited to = Feature Attributions; Computer Vision. Klein et al., Reported Metrics = 20 metrics. Pawlicki et al., Primary VXAI Focus = ✓. Pawlicki et al., Primarily Functionality- Grounded VXAI = ✓. Pawlicki et al., (Semi-)Systematic Review = ✓. Pawlicki et",
      "Topic": 1,
      "Name": "1_et_al_vxai_metrics",
      "Representation": [
        "et",
        "al",
        "vxai",
        "metrics",
        "desiderata",
        "systematic",
        "primarily",
        "semi",
        "date",
        "primary",
        "limited",
        "review",
        "focus",
        "functionality",
        "reported",
        "grounded"
      ],
      "Representative_Docs": [
        "3 Desiderata of XAI\nal., Desiderata = Correctness, Comprehensibility, Stability. Alangari et al., Limited to = . Alangari et al., Reported Metrics = 59 metrics. Le et al., Primary VXAI Focus = ✓. Le et al., Primarily Functionality- Grounded VXAI = ✓. Le et al., (Semi-)Systematic Review = ✓. Le et al., Date ↓ = Aug 2023. Le et al., Desiderata = Co-12 †. Le et al., Limited to = . Le et al., Reported Metrics = 86 metrics from 17 toolkits. Salih et al., Primary VXAI Focus = ✓. Salih et al., Primarily Functionality- Grounded VXAI = . Salih et al., (Semi-)Systematic Review = ✓. Salih et al., Date ↓ = Aug 2023. Salih et al., Desiderata = . Salih et al., Limited to = Cardiology. Salih et al., Reported Metrics = 27 metrics. Kadir et al., Primary VXAI Focus = ✓.",
        "3 Desiderata of XAI\net al., Primarily Functionality- Grounded VXAI = ✓. Bodria et al., (Semi-)Systematic Review = ✓. Bodria et al., Date ↓ = Nov 2021. Bodria et al., Desiderata = . Bodria et al., Limited to = . Bodria et al., Reported Metrics = 6 metrics. Sovrano et al., Primary VXAI Focus = ✓. Sovrano et al., Primarily Functionality- Grounded VXAI = . Sovrano et al., (Semi-)Systematic Review = . Sovrano et al., Date ↓ = Oct 2021. Sovrano et al., Desiderata = Similarity, Exactness, Fruitfulness. Sovrano et al., Limited to = . Sovrano et al., Reported Metrics = 22 metrics. Ras et al., Primary VXAI Focus = . Ras et al., Primarily Functionality- Grounded VXAI = . Ras et al., (Semi-)Systematic Review = . Ras et al., Date ↓ = Sep 2021. Ras et al., Desiderata = . Ras et",
        "3 Desiderata of XAI\net al., Primarily Functionality- Grounded VXAI = ✓. Bommer et al., (Semi-)Systematic Review = . Bommer et al., Date ↓ = Mar 2024. Bommer et al., Desiderata = Robustness, Faithfulness, Complexity, Lo- calization, Randomization. Bommer et al., Limited to = Climate Science. Bommer et al., Reported Metrics = 10 metrics. Li et al., Primary VXAI Focus = ✓. Li et al., Primarily Functionality- Grounded VXAI = ✓. Li et al., (Semi-)Systematic Review = . Li et al., Date ↓ = Dec 2023. Li et al., Desiderata = Faithfulness. Li et al., Limited to = Feature Attributions. Li et al., Reported Metrics = 6 metrics. Alangari et al., Primary VXAI Focus = ✓. Alangari et al., Primarily Functionality- Grounded VXAI = . Alangari et al., (Semi-)Systematic Review = . Alangari et al., Date ↓ = Aug 2023. Alangari et"
      ],
      "Top_n_words": "et - al - vxai - metrics - desiderata - systematic - primarily - semi - date - primary - limited - review - focus - functionality - reported - grounded",
      "Probability": 0.8172882957890741,
      "Representative_document": false
    },
    {
      "Document": "3 Desiderata of XAI\nal., Date ↓ = Oct 2024. Pawlicki et al., Desiderata = . Pawlicki et al., Limited to = Cybersecurity. Pawlicki et al., Reported Metrics = 86 metrics. Awal & Roy, Primary VXAI Focus = ✓. Awal & Roy, Primarily Functionality- Grounded VXAI = ✓. Awal & Roy, (Semi-)Systematic Review = . Awal & Roy, Date ↓ = Jun 2024. Awal & Roy, Desiderata = Reliability, Consistency. Awal & Roy, Limited to = Rule Explanations. Awal & Roy, Reported Metrics = 6 metrics. Bayrak & Bach, Primary VXAI Focus = ✓. Bayrak & Bach, Primarily Functionality- Grounded VXAI = ✓. Bayrak & Bach, (Semi-)Systematic Review = ✓. Bayrak & Bach, Date ↓ = Apr 2024. Bayrak & Bach, Desiderata = . Bayrak & Bach, Limited to = Counterfactuals. Bayrak & Bach, Reported Metrics = 66 metrics. Bommer et al., Primary VXAI Focus = ✓. Bommer",
      "Topic": 0,
      "Name": "0_al_explanation_et_xai",
      "Representation": [
        "al",
        "explanation",
        "et",
        "xai",
        "desiderata",
        "evaluation",
        "metrics",
        "2024",
        "explanations",
        "model",
        "2023",
        "explanans",
        "2021",
        "human",
        "grounded",
        "framework"
      ],
      "Representative_Docs": [
        "2 Related Work\nThere are five reviews, that we consider most similar to this work, as they present systematic functionality-grounded VXAI surveys. Le et al. (2023) and Nauta et al. (2023) both categorize the identified metrics based on a scheme of 12 properties, namely the Co-12 framework, which we discuss in more detail in Section 3. However, Le et al. (2023) restrict their analysis to metrics available through public XAI or VXAI toolkits (e.g., Quantus (Hedström et al., 2023)) and do not report on metrics introduced in the literature but not implemented in such libraries. Although there is some overlap with the study by Nauta et al. (2023), particularly in the inclusion of some identical metrics, their review also incorporates studies that merely apply VXAI metrics rather than introducing them. In contrast, our work provides detailed descriptions and categorizations of each identified metric. The review by Kadir et al. (2023) covers a broad range of domains and explanation types 2 and reports a wide variety of metrics. It also groups several metrics by method, a strategy shared by",
        "1 Introduction\nHowever, XAI is not a silver bullet. Van der Waa et al. (2021) point out, that humans tend to trust predictions more readily when an explanation is provided, often without carefully examining the explanation itself. This lack of critical scrutiny can lead to unwarranted trust, especially when decisions are taken based on incorrect or misleading explanations (Eiband et al., 2019; Jesus et al., 2021). To make matters worse, different XAI algorithms may result in conflicting explanations for the same model and sample (Krishna et al., 2022). Therefore, simply providing any explanation is not sufficient, but it is important to assess the quality of the explanation at hand (Sovrano et al., 2021). Unfortunately, while there is a plethora of XAI methods, evaluation of explanations is still an immature research area (Ribera & Lapedriza, 2019), with many studies relying on the notion of a good explanation as 'You'll know it when you see it', providing anecdotal evidence (i.e. small-scale qualitative validation) (Doshi-Velez & Kim, 2017; Nauta et al., 2023; Saarela &",
        "1 Introduction\n(Payrovnaziri et al., 2020), data and knowledge engineering (Li et al., 2020b), or timeseries classification (Theissler et al., 2022). Others focus on particular XAI approaches, such as visual explanations in CNNs (Mohamed et al., 2022) or instance-based explanations (Bayrak & Bach, 2024). Moreover, many surveys dedicate only limited attention to evaluation metrics, with their primary focus placed on the XAI methods themselves (Carvalho et al., 2019; Ding et al., 2022; Minh et al., 2022; Mohamed et al., 2022; Ali et al., 2023; Clement et al., 2023; Patrício et al., 2023; Chaddad et al., 2024; Gongane et al., 2024; Xua & Yang, 2024). By contrast, this review focuses exclusively on functionality-grounded evaluation across domains and is applicable to a wide range of XAI approaches."
      ],
      "Top_n_words": "al - explanation - et - xai - desiderata - evaluation - metrics - 2024 - explanations - model - 2023 - explanans - 2021 - human - grounded - framework",
      "Probability": 0.5460283769703939,
      "Representative_document": false
    },
    {
      "Document": "3 Desiderata of XAI\net al., Primarily Functionality- Grounded VXAI = ✓. Bommer et al., (Semi-)Systematic Review = . Bommer et al., Date ↓ = Mar 2024. Bommer et al., Desiderata = Robustness, Faithfulness, Complexity, Lo- calization, Randomization. Bommer et al., Limited to = Climate Science. Bommer et al., Reported Metrics = 10 metrics. Li et al., Primary VXAI Focus = ✓. Li et al., Primarily Functionality- Grounded VXAI = ✓. Li et al., (Semi-)Systematic Review = . Li et al., Date ↓ = Dec 2023. Li et al., Desiderata = Faithfulness. Li et al., Limited to = Feature Attributions. Li et al., Reported Metrics = 6 metrics. Alangari et al., Primary VXAI Focus = ✓. Alangari et al., Primarily Functionality- Grounded VXAI = . Alangari et al., (Semi-)Systematic Review = . Alangari et al., Date ↓ = Aug 2023. Alangari et",
      "Topic": 1,
      "Name": "1_et_al_vxai_metrics",
      "Representation": [
        "et",
        "al",
        "vxai",
        "metrics",
        "desiderata",
        "systematic",
        "primarily",
        "semi",
        "date",
        "primary",
        "limited",
        "review",
        "focus",
        "functionality",
        "reported",
        "grounded"
      ],
      "Representative_Docs": [
        "3 Desiderata of XAI\nal., Desiderata = Correctness, Comprehensibility, Stability. Alangari et al., Limited to = . Alangari et al., Reported Metrics = 59 metrics. Le et al., Primary VXAI Focus = ✓. Le et al., Primarily Functionality- Grounded VXAI = ✓. Le et al., (Semi-)Systematic Review = ✓. Le et al., Date ↓ = Aug 2023. Le et al., Desiderata = Co-12 †. Le et al., Limited to = . Le et al., Reported Metrics = 86 metrics from 17 toolkits. Salih et al., Primary VXAI Focus = ✓. Salih et al., Primarily Functionality- Grounded VXAI = . Salih et al., (Semi-)Systematic Review = ✓. Salih et al., Date ↓ = Aug 2023. Salih et al., Desiderata = . Salih et al., Limited to = Cardiology. Salih et al., Reported Metrics = 27 metrics. Kadir et al., Primary VXAI Focus = ✓.",
        "3 Desiderata of XAI\net al., Primarily Functionality- Grounded VXAI = ✓. Bodria et al., (Semi-)Systematic Review = ✓. Bodria et al., Date ↓ = Nov 2021. Bodria et al., Desiderata = . Bodria et al., Limited to = . Bodria et al., Reported Metrics = 6 metrics. Sovrano et al., Primary VXAI Focus = ✓. Sovrano et al., Primarily Functionality- Grounded VXAI = . Sovrano et al., (Semi-)Systematic Review = . Sovrano et al., Date ↓ = Oct 2021. Sovrano et al., Desiderata = Similarity, Exactness, Fruitfulness. Sovrano et al., Limited to = . Sovrano et al., Reported Metrics = 22 metrics. Ras et al., Primary VXAI Focus = . Ras et al., Primarily Functionality- Grounded VXAI = . Ras et al., (Semi-)Systematic Review = . Ras et al., Date ↓ = Sep 2021. Ras et al., Desiderata = . Ras et",
        "3 Desiderata of XAI\net al., Primarily Functionality- Grounded VXAI = ✓. Bommer et al., (Semi-)Systematic Review = . Bommer et al., Date ↓ = Mar 2024. Bommer et al., Desiderata = Robustness, Faithfulness, Complexity, Lo- calization, Randomization. Bommer et al., Limited to = Climate Science. Bommer et al., Reported Metrics = 10 metrics. Li et al., Primary VXAI Focus = ✓. Li et al., Primarily Functionality- Grounded VXAI = ✓. Li et al., (Semi-)Systematic Review = . Li et al., Date ↓ = Dec 2023. Li et al., Desiderata = Faithfulness. Li et al., Limited to = Feature Attributions. Li et al., Reported Metrics = 6 metrics. Alangari et al., Primary VXAI Focus = ✓. Alangari et al., Primarily Functionality- Grounded VXAI = . Alangari et al., (Semi-)Systematic Review = . Alangari et al., Date ↓ = Aug 2023. Alangari et"
      ],
      "Top_n_words": "et - al - vxai - metrics - desiderata - systematic - primarily - semi - date - primary - limited - review - focus - functionality - reported - grounded",
      "Probability": 1.0,
      "Representative_document": true
    },
    {
      "Document": "3 Desiderata of XAI\nal., Desiderata = Correctness, Comprehensibility, Stability. Alangari et al., Limited to = . Alangari et al., Reported Metrics = 59 metrics. Le et al., Primary VXAI Focus = ✓. Le et al., Primarily Functionality- Grounded VXAI = ✓. Le et al., (Semi-)Systematic Review = ✓. Le et al., Date ↓ = Aug 2023. Le et al., Desiderata = Co-12 †. Le et al., Limited to = . Le et al., Reported Metrics = 86 metrics from 17 toolkits. Salih et al., Primary VXAI Focus = ✓. Salih et al., Primarily Functionality- Grounded VXAI = . Salih et al., (Semi-)Systematic Review = ✓. Salih et al., Date ↓ = Aug 2023. Salih et al., Desiderata = . Salih et al., Limited to = Cardiology. Salih et al., Reported Metrics = 27 metrics. Kadir et al., Primary VXAI Focus = ✓.",
      "Topic": 1,
      "Name": "1_et_al_vxai_metrics",
      "Representation": [
        "et",
        "al",
        "vxai",
        "metrics",
        "desiderata",
        "systematic",
        "primarily",
        "semi",
        "date",
        "primary",
        "limited",
        "review",
        "focus",
        "functionality",
        "reported",
        "grounded"
      ],
      "Representative_Docs": [
        "3 Desiderata of XAI\nal., Desiderata = Correctness, Comprehensibility, Stability. Alangari et al., Limited to = . Alangari et al., Reported Metrics = 59 metrics. Le et al., Primary VXAI Focus = ✓. Le et al., Primarily Functionality- Grounded VXAI = ✓. Le et al., (Semi-)Systematic Review = ✓. Le et al., Date ↓ = Aug 2023. Le et al., Desiderata = Co-12 †. Le et al., Limited to = . Le et al., Reported Metrics = 86 metrics from 17 toolkits. Salih et al., Primary VXAI Focus = ✓. Salih et al., Primarily Functionality- Grounded VXAI = . Salih et al., (Semi-)Systematic Review = ✓. Salih et al., Date ↓ = Aug 2023. Salih et al., Desiderata = . Salih et al., Limited to = Cardiology. Salih et al., Reported Metrics = 27 metrics. Kadir et al., Primary VXAI Focus = ✓.",
        "3 Desiderata of XAI\net al., Primarily Functionality- Grounded VXAI = ✓. Bodria et al., (Semi-)Systematic Review = ✓. Bodria et al., Date ↓ = Nov 2021. Bodria et al., Desiderata = . Bodria et al., Limited to = . Bodria et al., Reported Metrics = 6 metrics. Sovrano et al., Primary VXAI Focus = ✓. Sovrano et al., Primarily Functionality- Grounded VXAI = . Sovrano et al., (Semi-)Systematic Review = . Sovrano et al., Date ↓ = Oct 2021. Sovrano et al., Desiderata = Similarity, Exactness, Fruitfulness. Sovrano et al., Limited to = . Sovrano et al., Reported Metrics = 22 metrics. Ras et al., Primary VXAI Focus = . Ras et al., Primarily Functionality- Grounded VXAI = . Ras et al., (Semi-)Systematic Review = . Ras et al., Date ↓ = Sep 2021. Ras et al., Desiderata = . Ras et",
        "3 Desiderata of XAI\net al., Primarily Functionality- Grounded VXAI = ✓. Bommer et al., (Semi-)Systematic Review = . Bommer et al., Date ↓ = Mar 2024. Bommer et al., Desiderata = Robustness, Faithfulness, Complexity, Lo- calization, Randomization. Bommer et al., Limited to = Climate Science. Bommer et al., Reported Metrics = 10 metrics. Li et al., Primary VXAI Focus = ✓. Li et al., Primarily Functionality- Grounded VXAI = ✓. Li et al., (Semi-)Systematic Review = . Li et al., Date ↓ = Dec 2023. Li et al., Desiderata = Faithfulness. Li et al., Limited to = Feature Attributions. Li et al., Reported Metrics = 6 metrics. Alangari et al., Primary VXAI Focus = ✓. Alangari et al., Primarily Functionality- Grounded VXAI = . Alangari et al., (Semi-)Systematic Review = . Alangari et al., Date ↓ = Aug 2023. Alangari et"
      ],
      "Top_n_words": "et - al - vxai - metrics - desiderata - systematic - primarily - semi - date - primary - limited - review - focus - functionality - reported - grounded",
      "Probability": 1.0,
      "Representative_document": true
    },
    {
      "Document": "3 Desiderata of XAI\nKadir et al., Primarily Functionality- Grounded VXAI = ✓. Kadir et al., (Semi-)Systematic Review = ✓. Kadir et al., Date ↓ = Jul 2023. Kadir et al., Desiderata = . Kadir et al., Limited to = . Kadir et al., Reported Metrics = 80 metrics. Hedström et al., Primary VXAI Focus = ✓. Hedström et al., Primarily Functionality- Grounded VXAI = ✓. Hedström et al., (Semi-)Systematic Review = . Hedström et al., Date ↓ = Apr 2023. Hedström et al., Desiderata = Faithfulness, Robustness, Localization, Complexity, Axiomatic, Randomization. Hedström et al., Limited to = Feature Attribution. Hedström et al., Reported Metrics = 27 metrics. Schwalbe & Finzel, Primary VXAI Focus = . Schwalbe & Finzel, Primarily Functionality- Grounded VXAI = . Schwalbe & Finzel,",
      "Topic": 1,
      "Name": "1_et_al_vxai_metrics",
      "Representation": [
        "et",
        "al",
        "vxai",
        "metrics",
        "desiderata",
        "systematic",
        "primarily",
        "semi",
        "date",
        "primary",
        "limited",
        "review",
        "focus",
        "functionality",
        "reported",
        "grounded"
      ],
      "Representative_Docs": [
        "3 Desiderata of XAI\nal., Desiderata = Correctness, Comprehensibility, Stability. Alangari et al., Limited to = . Alangari et al., Reported Metrics = 59 metrics. Le et al., Primary VXAI Focus = ✓. Le et al., Primarily Functionality- Grounded VXAI = ✓. Le et al., (Semi-)Systematic Review = ✓. Le et al., Date ↓ = Aug 2023. Le et al., Desiderata = Co-12 †. Le et al., Limited to = . Le et al., Reported Metrics = 86 metrics from 17 toolkits. Salih et al., Primary VXAI Focus = ✓. Salih et al., Primarily Functionality- Grounded VXAI = . Salih et al., (Semi-)Systematic Review = ✓. Salih et al., Date ↓ = Aug 2023. Salih et al., Desiderata = . Salih et al., Limited to = Cardiology. Salih et al., Reported Metrics = 27 metrics. Kadir et al., Primary VXAI Focus = ✓.",
        "3 Desiderata of XAI\net al., Primarily Functionality- Grounded VXAI = ✓. Bodria et al., (Semi-)Systematic Review = ✓. Bodria et al., Date ↓ = Nov 2021. Bodria et al., Desiderata = . Bodria et al., Limited to = . Bodria et al., Reported Metrics = 6 metrics. Sovrano et al., Primary VXAI Focus = ✓. Sovrano et al., Primarily Functionality- Grounded VXAI = . Sovrano et al., (Semi-)Systematic Review = . Sovrano et al., Date ↓ = Oct 2021. Sovrano et al., Desiderata = Similarity, Exactness, Fruitfulness. Sovrano et al., Limited to = . Sovrano et al., Reported Metrics = 22 metrics. Ras et al., Primary VXAI Focus = . Ras et al., Primarily Functionality- Grounded VXAI = . Ras et al., (Semi-)Systematic Review = . Ras et al., Date ↓ = Sep 2021. Ras et al., Desiderata = . Ras et",
        "3 Desiderata of XAI\net al., Primarily Functionality- Grounded VXAI = ✓. Bommer et al., (Semi-)Systematic Review = . Bommer et al., Date ↓ = Mar 2024. Bommer et al., Desiderata = Robustness, Faithfulness, Complexity, Lo- calization, Randomization. Bommer et al., Limited to = Climate Science. Bommer et al., Reported Metrics = 10 metrics. Li et al., Primary VXAI Focus = ✓. Li et al., Primarily Functionality- Grounded VXAI = ✓. Li et al., (Semi-)Systematic Review = . Li et al., Date ↓ = Dec 2023. Li et al., Desiderata = Faithfulness. Li et al., Limited to = Feature Attributions. Li et al., Reported Metrics = 6 metrics. Alangari et al., Primary VXAI Focus = ✓. Alangari et al., Primarily Functionality- Grounded VXAI = . Alangari et al., (Semi-)Systematic Review = . Alangari et al., Date ↓ = Aug 2023. Alangari et"
      ],
      "Top_n_words": "et - al - vxai - metrics - desiderata - systematic - primarily - semi - date - primary - limited - review - focus - functionality - reported - grounded",
      "Probability": 1.0,
      "Representative_document": false
    },
    {
      "Document": "3 Desiderata of XAI\n(Semi-)Systematic Review = ✓. Schwalbe & Finzel, Date ↓ = Jan 2023. Schwalbe & Finzel, Desiderata = . Schwalbe & Finzel, Limited to = . Schwalbe & Finzel, Reported Metrics = 11 metrics (already grouped). Agarwal et al., Primary VXAI Focus = ✓. Agarwal et al., Primarily Functionality- Grounded VXAI = ✓. Agarwal et al., (Semi-)Systematic Review = . Agarwal et al., Date ↓ = Nov 2022. Agarwal et al., Desiderata = Faithfulness, Stability. Agarwal et al., Limited to = Feature Attributions. Agarwal et al., Reported Metrics = 11 metrics. Coroama & Groza, Primary VXAI Focus = ✓. Coroama & Groza, Primarily Functionality- Grounded VXAI = . Coroama & Groza, (Semi-)Systematic Review = . Coroama & Groza, Date ↓ = Nov 2022. Coroama & Groza, Desiderata = .",
      "Topic": 1,
      "Name": "1_et_al_vxai_metrics",
      "Representation": [
        "et",
        "al",
        "vxai",
        "metrics",
        "desiderata",
        "systematic",
        "primarily",
        "semi",
        "date",
        "primary",
        "limited",
        "review",
        "focus",
        "functionality",
        "reported",
        "grounded"
      ],
      "Representative_Docs": [
        "3 Desiderata of XAI\nal., Desiderata = Correctness, Comprehensibility, Stability. Alangari et al., Limited to = . Alangari et al., Reported Metrics = 59 metrics. Le et al., Primary VXAI Focus = ✓. Le et al., Primarily Functionality- Grounded VXAI = ✓. Le et al., (Semi-)Systematic Review = ✓. Le et al., Date ↓ = Aug 2023. Le et al., Desiderata = Co-12 †. Le et al., Limited to = . Le et al., Reported Metrics = 86 metrics from 17 toolkits. Salih et al., Primary VXAI Focus = ✓. Salih et al., Primarily Functionality- Grounded VXAI = . Salih et al., (Semi-)Systematic Review = ✓. Salih et al., Date ↓ = Aug 2023. Salih et al., Desiderata = . Salih et al., Limited to = Cardiology. Salih et al., Reported Metrics = 27 metrics. Kadir et al., Primary VXAI Focus = ✓.",
        "3 Desiderata of XAI\net al., Primarily Functionality- Grounded VXAI = ✓. Bodria et al., (Semi-)Systematic Review = ✓. Bodria et al., Date ↓ = Nov 2021. Bodria et al., Desiderata = . Bodria et al., Limited to = . Bodria et al., Reported Metrics = 6 metrics. Sovrano et al., Primary VXAI Focus = ✓. Sovrano et al., Primarily Functionality- Grounded VXAI = . Sovrano et al., (Semi-)Systematic Review = . Sovrano et al., Date ↓ = Oct 2021. Sovrano et al., Desiderata = Similarity, Exactness, Fruitfulness. Sovrano et al., Limited to = . Sovrano et al., Reported Metrics = 22 metrics. Ras et al., Primary VXAI Focus = . Ras et al., Primarily Functionality- Grounded VXAI = . Ras et al., (Semi-)Systematic Review = . Ras et al., Date ↓ = Sep 2021. Ras et al., Desiderata = . Ras et",
        "3 Desiderata of XAI\net al., Primarily Functionality- Grounded VXAI = ✓. Bommer et al., (Semi-)Systematic Review = . Bommer et al., Date ↓ = Mar 2024. Bommer et al., Desiderata = Robustness, Faithfulness, Complexity, Lo- calization, Randomization. Bommer et al., Limited to = Climate Science. Bommer et al., Reported Metrics = 10 metrics. Li et al., Primary VXAI Focus = ✓. Li et al., Primarily Functionality- Grounded VXAI = ✓. Li et al., (Semi-)Systematic Review = . Li et al., Date ↓ = Dec 2023. Li et al., Desiderata = Faithfulness. Li et al., Limited to = Feature Attributions. Li et al., Reported Metrics = 6 metrics. Alangari et al., Primary VXAI Focus = ✓. Alangari et al., Primarily Functionality- Grounded VXAI = . Alangari et al., (Semi-)Systematic Review = . Alangari et al., Date ↓ = Aug 2023. Alangari et"
      ],
      "Top_n_words": "et - al - vxai - metrics - desiderata - systematic - primarily - semi - date - primary - limited - review - focus - functionality - reported - grounded",
      "Probability": 1.0,
      "Representative_document": false
    },
    {
      "Document": "3 Desiderata of XAI\nCoroama & Groza, Limited to = . Coroama & Groza, Reported Metrics = 26 metrics. Verma et al., Primary VXAI Focus = . Verma et al., Primarily Functionality- Grounded VXAI = ✓. Verma et al., (Semi-)Systematic Review = . Verma et al., Date ↓ = Nov 2022. Verma et al., Desiderata = . Verma et al., Limited to = Counterfactuals. Verma et al., Reported Metrics = 9 metrics (already grouped). Belaid et al., Primary VXAI Focus = ✓. Belaid et al., Primarily Functionality- Grounded VXAI = ✓. Belaid et al., (Semi-)Systematic Review = . Belaid et al., Date ↓ = Oct 2022. Belaid et al., Desiderata = Fidelity, Fragility, Stability, Simplicity, Stress, Other. Belaid et al., Limited to = Feature Attributions. Belaid et al., Reported Metrics = 22 metrics. Cugny et al., Primary VXAI Focus =",
      "Topic": 1,
      "Name": "1_et_al_vxai_metrics",
      "Representation": [
        "et",
        "al",
        "vxai",
        "metrics",
        "desiderata",
        "systematic",
        "primarily",
        "semi",
        "date",
        "primary",
        "limited",
        "review",
        "focus",
        "functionality",
        "reported",
        "grounded"
      ],
      "Representative_Docs": [
        "3 Desiderata of XAI\nal., Desiderata = Correctness, Comprehensibility, Stability. Alangari et al., Limited to = . Alangari et al., Reported Metrics = 59 metrics. Le et al., Primary VXAI Focus = ✓. Le et al., Primarily Functionality- Grounded VXAI = ✓. Le et al., (Semi-)Systematic Review = ✓. Le et al., Date ↓ = Aug 2023. Le et al., Desiderata = Co-12 †. Le et al., Limited to = . Le et al., Reported Metrics = 86 metrics from 17 toolkits. Salih et al., Primary VXAI Focus = ✓. Salih et al., Primarily Functionality- Grounded VXAI = . Salih et al., (Semi-)Systematic Review = ✓. Salih et al., Date ↓ = Aug 2023. Salih et al., Desiderata = . Salih et al., Limited to = Cardiology. Salih et al., Reported Metrics = 27 metrics. Kadir et al., Primary VXAI Focus = ✓.",
        "3 Desiderata of XAI\net al., Primarily Functionality- Grounded VXAI = ✓. Bodria et al., (Semi-)Systematic Review = ✓. Bodria et al., Date ↓ = Nov 2021. Bodria et al., Desiderata = . Bodria et al., Limited to = . Bodria et al., Reported Metrics = 6 metrics. Sovrano et al., Primary VXAI Focus = ✓. Sovrano et al., Primarily Functionality- Grounded VXAI = . Sovrano et al., (Semi-)Systematic Review = . Sovrano et al., Date ↓ = Oct 2021. Sovrano et al., Desiderata = Similarity, Exactness, Fruitfulness. Sovrano et al., Limited to = . Sovrano et al., Reported Metrics = 22 metrics. Ras et al., Primary VXAI Focus = . Ras et al., Primarily Functionality- Grounded VXAI = . Ras et al., (Semi-)Systematic Review = . Ras et al., Date ↓ = Sep 2021. Ras et al., Desiderata = . Ras et",
        "3 Desiderata of XAI\net al., Primarily Functionality- Grounded VXAI = ✓. Bommer et al., (Semi-)Systematic Review = . Bommer et al., Date ↓ = Mar 2024. Bommer et al., Desiderata = Robustness, Faithfulness, Complexity, Lo- calization, Randomization. Bommer et al., Limited to = Climate Science. Bommer et al., Reported Metrics = 10 metrics. Li et al., Primary VXAI Focus = ✓. Li et al., Primarily Functionality- Grounded VXAI = ✓. Li et al., (Semi-)Systematic Review = . Li et al., Date ↓ = Dec 2023. Li et al., Desiderata = Faithfulness. Li et al., Limited to = Feature Attributions. Li et al., Reported Metrics = 6 metrics. Alangari et al., Primary VXAI Focus = ✓. Alangari et al., Primarily Functionality- Grounded VXAI = . Alangari et al., (Semi-)Systematic Review = . Alangari et al., Date ↓ = Aug 2023. Alangari et"
      ],
      "Top_n_words": "et - al - vxai - metrics - desiderata - systematic - primarily - semi - date - primary - limited - review - focus - functionality - reported - grounded",
      "Probability": 1.0,
      "Representative_document": false
    },
    {
      "Document": "3 Desiderata of XAI\n✓. Cugny et al., Primarily Functionality- Grounded VXAI = ✓. Cugny et al., (Semi-)Systematic Review = . Cugny et al., Date ↓ = Oct 2022. Cugny et al., Desiderata = . Cugny et al., Limited to = . Cugny et al., Reported Metrics = 6 metrics. Lopes et al., Primary VXAI Focus = ✓. Lopes et al., Primarily Functionality- Grounded VXAI = . Lopes et al., (Semi-)Systematic Review = ✓. Lopes et al., Date ↓ = Aug 2022. Lopes et al., Desiderata = Fidelity (Completeness, Soundness), Inter- pretability, Broadness, Simplicity, Clarity). Lopes et al., Limited to = . Lopes et al., Reported Metrics = 43 metrics. Yuan et al., Primary VXAI Focus = . Yuan et al., Primarily Functionality- Grounded VXAI = ✓. Yuan et al., (Semi-)Systematic Review = ✓. Yuan et al., Date ↓ = Jul 2022. Yuan et",
      "Topic": 1,
      "Name": "1_et_al_vxai_metrics",
      "Representation": [
        "et",
        "al",
        "vxai",
        "metrics",
        "desiderata",
        "systematic",
        "primarily",
        "semi",
        "date",
        "primary",
        "limited",
        "review",
        "focus",
        "functionality",
        "reported",
        "grounded"
      ],
      "Representative_Docs": [
        "3 Desiderata of XAI\nal., Desiderata = Correctness, Comprehensibility, Stability. Alangari et al., Limited to = . Alangari et al., Reported Metrics = 59 metrics. Le et al., Primary VXAI Focus = ✓. Le et al., Primarily Functionality- Grounded VXAI = ✓. Le et al., (Semi-)Systematic Review = ✓. Le et al., Date ↓ = Aug 2023. Le et al., Desiderata = Co-12 †. Le et al., Limited to = . Le et al., Reported Metrics = 86 metrics from 17 toolkits. Salih et al., Primary VXAI Focus = ✓. Salih et al., Primarily Functionality- Grounded VXAI = . Salih et al., (Semi-)Systematic Review = ✓. Salih et al., Date ↓ = Aug 2023. Salih et al., Desiderata = . Salih et al., Limited to = Cardiology. Salih et al., Reported Metrics = 27 metrics. Kadir et al., Primary VXAI Focus = ✓.",
        "3 Desiderata of XAI\net al., Primarily Functionality- Grounded VXAI = ✓. Bodria et al., (Semi-)Systematic Review = ✓. Bodria et al., Date ↓ = Nov 2021. Bodria et al., Desiderata = . Bodria et al., Limited to = . Bodria et al., Reported Metrics = 6 metrics. Sovrano et al., Primary VXAI Focus = ✓. Sovrano et al., Primarily Functionality- Grounded VXAI = . Sovrano et al., (Semi-)Systematic Review = . Sovrano et al., Date ↓ = Oct 2021. Sovrano et al., Desiderata = Similarity, Exactness, Fruitfulness. Sovrano et al., Limited to = . Sovrano et al., Reported Metrics = 22 metrics. Ras et al., Primary VXAI Focus = . Ras et al., Primarily Functionality- Grounded VXAI = . Ras et al., (Semi-)Systematic Review = . Ras et al., Date ↓ = Sep 2021. Ras et al., Desiderata = . Ras et",
        "3 Desiderata of XAI\net al., Primarily Functionality- Grounded VXAI = ✓. Bommer et al., (Semi-)Systematic Review = . Bommer et al., Date ↓ = Mar 2024. Bommer et al., Desiderata = Robustness, Faithfulness, Complexity, Lo- calization, Randomization. Bommer et al., Limited to = Climate Science. Bommer et al., Reported Metrics = 10 metrics. Li et al., Primary VXAI Focus = ✓. Li et al., Primarily Functionality- Grounded VXAI = ✓. Li et al., (Semi-)Systematic Review = . Li et al., Date ↓ = Dec 2023. Li et al., Desiderata = Faithfulness. Li et al., Limited to = Feature Attributions. Li et al., Reported Metrics = 6 metrics. Alangari et al., Primary VXAI Focus = ✓. Alangari et al., Primarily Functionality- Grounded VXAI = . Alangari et al., (Semi-)Systematic Review = . Alangari et al., Date ↓ = Aug 2023. Alangari et"
      ],
      "Top_n_words": "et - al - vxai - metrics - desiderata - systematic - primarily - semi - date - primary - limited - review - focus - functionality - reported - grounded",
      "Probability": 1.0,
      "Representative_document": false
    },
    {
      "Document": "3 Desiderata of XAI\nal., Desiderata = Fidelity, Sparsity, Stability, Accuracy. Yuan et al., Limited to = Graph Neural Networks. Yuan et al., Reported Metrics = 7 metrics. Löfström et al., Primary VXAI Focus = ✓. Löfström et al., Primarily Functionality- Grounded VXAI = . Löfström et al., (Semi-)Systematic Review = ✓. Löfström et al., Date ↓ = Mar 2022. Löfström et al., Desiderata = . Löfström et al., Limited to = . Löfström et al., Reported Metrics = 10 metrics. Vilone & Longo, Primary VXAI Focus = ✓. Vilone & Longo, Primarily Functionality- Grounded VXAI = . Vilone & Longo, (Semi-)Systematic Review = ✓. Vilone & Longo, Date ↓ = Dec 2021. Vilone & Longo, Desiderata = . Vilone & Longo, Limited to = . Vilone & Longo, Reported Metrics = 36 metrics. Bodria et al., Primary VXAI Focus = . Bodria",
      "Topic": 1,
      "Name": "1_et_al_vxai_metrics",
      "Representation": [
        "et",
        "al",
        "vxai",
        "metrics",
        "desiderata",
        "systematic",
        "primarily",
        "semi",
        "date",
        "primary",
        "limited",
        "review",
        "focus",
        "functionality",
        "reported",
        "grounded"
      ],
      "Representative_Docs": [
        "3 Desiderata of XAI\nal., Desiderata = Correctness, Comprehensibility, Stability. Alangari et al., Limited to = . Alangari et al., Reported Metrics = 59 metrics. Le et al., Primary VXAI Focus = ✓. Le et al., Primarily Functionality- Grounded VXAI = ✓. Le et al., (Semi-)Systematic Review = ✓. Le et al., Date ↓ = Aug 2023. Le et al., Desiderata = Co-12 †. Le et al., Limited to = . Le et al., Reported Metrics = 86 metrics from 17 toolkits. Salih et al., Primary VXAI Focus = ✓. Salih et al., Primarily Functionality- Grounded VXAI = . Salih et al., (Semi-)Systematic Review = ✓. Salih et al., Date ↓ = Aug 2023. Salih et al., Desiderata = . Salih et al., Limited to = Cardiology. Salih et al., Reported Metrics = 27 metrics. Kadir et al., Primary VXAI Focus = ✓.",
        "3 Desiderata of XAI\net al., Primarily Functionality- Grounded VXAI = ✓. Bodria et al., (Semi-)Systematic Review = ✓. Bodria et al., Date ↓ = Nov 2021. Bodria et al., Desiderata = . Bodria et al., Limited to = . Bodria et al., Reported Metrics = 6 metrics. Sovrano et al., Primary VXAI Focus = ✓. Sovrano et al., Primarily Functionality- Grounded VXAI = . Sovrano et al., (Semi-)Systematic Review = . Sovrano et al., Date ↓ = Oct 2021. Sovrano et al., Desiderata = Similarity, Exactness, Fruitfulness. Sovrano et al., Limited to = . Sovrano et al., Reported Metrics = 22 metrics. Ras et al., Primary VXAI Focus = . Ras et al., Primarily Functionality- Grounded VXAI = . Ras et al., (Semi-)Systematic Review = . Ras et al., Date ↓ = Sep 2021. Ras et al., Desiderata = . Ras et",
        "3 Desiderata of XAI\net al., Primarily Functionality- Grounded VXAI = ✓. Bommer et al., (Semi-)Systematic Review = . Bommer et al., Date ↓ = Mar 2024. Bommer et al., Desiderata = Robustness, Faithfulness, Complexity, Lo- calization, Randomization. Bommer et al., Limited to = Climate Science. Bommer et al., Reported Metrics = 10 metrics. Li et al., Primary VXAI Focus = ✓. Li et al., Primarily Functionality- Grounded VXAI = ✓. Li et al., (Semi-)Systematic Review = . Li et al., Date ↓ = Dec 2023. Li et al., Desiderata = Faithfulness. Li et al., Limited to = Feature Attributions. Li et al., Reported Metrics = 6 metrics. Alangari et al., Primary VXAI Focus = ✓. Alangari et al., Primarily Functionality- Grounded VXAI = . Alangari et al., (Semi-)Systematic Review = . Alangari et al., Date ↓ = Aug 2023. Alangari et"
      ],
      "Top_n_words": "et - al - vxai - metrics - desiderata - systematic - primarily - semi - date - primary - limited - review - focus - functionality - reported - grounded",
      "Probability": 0.8524641821285643,
      "Representative_document": false
    },
    {
      "Document": "3 Desiderata of XAI\net al., Primarily Functionality- Grounded VXAI = ✓. Bodria et al., (Semi-)Systematic Review = ✓. Bodria et al., Date ↓ = Nov 2021. Bodria et al., Desiderata = . Bodria et al., Limited to = . Bodria et al., Reported Metrics = 6 metrics. Sovrano et al., Primary VXAI Focus = ✓. Sovrano et al., Primarily Functionality- Grounded VXAI = . Sovrano et al., (Semi-)Systematic Review = . Sovrano et al., Date ↓ = Oct 2021. Sovrano et al., Desiderata = Similarity, Exactness, Fruitfulness. Sovrano et al., Limited to = . Sovrano et al., Reported Metrics = 22 metrics. Ras et al., Primary VXAI Focus = . Ras et al., Primarily Functionality- Grounded VXAI = . Ras et al., (Semi-)Systematic Review = . Ras et al., Date ↓ = Sep 2021. Ras et al., Desiderata = . Ras et",
      "Topic": 1,
      "Name": "1_et_al_vxai_metrics",
      "Representation": [
        "et",
        "al",
        "vxai",
        "metrics",
        "desiderata",
        "systematic",
        "primarily",
        "semi",
        "date",
        "primary",
        "limited",
        "review",
        "focus",
        "functionality",
        "reported",
        "grounded"
      ],
      "Representative_Docs": [
        "3 Desiderata of XAI\nal., Desiderata = Correctness, Comprehensibility, Stability. Alangari et al., Limited to = . Alangari et al., Reported Metrics = 59 metrics. Le et al., Primary VXAI Focus = ✓. Le et al., Primarily Functionality- Grounded VXAI = ✓. Le et al., (Semi-)Systematic Review = ✓. Le et al., Date ↓ = Aug 2023. Le et al., Desiderata = Co-12 †. Le et al., Limited to = . Le et al., Reported Metrics = 86 metrics from 17 toolkits. Salih et al., Primary VXAI Focus = ✓. Salih et al., Primarily Functionality- Grounded VXAI = . Salih et al., (Semi-)Systematic Review = ✓. Salih et al., Date ↓ = Aug 2023. Salih et al., Desiderata = . Salih et al., Limited to = Cardiology. Salih et al., Reported Metrics = 27 metrics. Kadir et al., Primary VXAI Focus = ✓.",
        "3 Desiderata of XAI\net al., Primarily Functionality- Grounded VXAI = ✓. Bodria et al., (Semi-)Systematic Review = ✓. Bodria et al., Date ↓ = Nov 2021. Bodria et al., Desiderata = . Bodria et al., Limited to = . Bodria et al., Reported Metrics = 6 metrics. Sovrano et al., Primary VXAI Focus = ✓. Sovrano et al., Primarily Functionality- Grounded VXAI = . Sovrano et al., (Semi-)Systematic Review = . Sovrano et al., Date ↓ = Oct 2021. Sovrano et al., Desiderata = Similarity, Exactness, Fruitfulness. Sovrano et al., Limited to = . Sovrano et al., Reported Metrics = 22 metrics. Ras et al., Primary VXAI Focus = . Ras et al., Primarily Functionality- Grounded VXAI = . Ras et al., (Semi-)Systematic Review = . Ras et al., Date ↓ = Sep 2021. Ras et al., Desiderata = . Ras et",
        "3 Desiderata of XAI\net al., Primarily Functionality- Grounded VXAI = ✓. Bommer et al., (Semi-)Systematic Review = . Bommer et al., Date ↓ = Mar 2024. Bommer et al., Desiderata = Robustness, Faithfulness, Complexity, Lo- calization, Randomization. Bommer et al., Limited to = Climate Science. Bommer et al., Reported Metrics = 10 metrics. Li et al., Primary VXAI Focus = ✓. Li et al., Primarily Functionality- Grounded VXAI = ✓. Li et al., (Semi-)Systematic Review = . Li et al., Date ↓ = Dec 2023. Li et al., Desiderata = Faithfulness. Li et al., Limited to = Feature Attributions. Li et al., Reported Metrics = 6 metrics. Alangari et al., Primary VXAI Focus = ✓. Alangari et al., Primarily Functionality- Grounded VXAI = . Alangari et al., (Semi-)Systematic Review = . Alangari et al., Date ↓ = Aug 2023. Alangari et"
      ],
      "Top_n_words": "et - al - vxai - metrics - desiderata - systematic - primarily - semi - date - primary - limited - review - focus - functionality - reported - grounded",
      "Probability": 0.8198642635137049,
      "Representative_document": true
    },
    {
      "Document": "3 Desiderata of XAI\nal., Limited to = . Ras et al., Reported Metrics = 13 sources in text (metrics not listed). Mohseni et al., Primary VXAI Focus = . Mohseni et al., Primarily Functionality- Grounded VXAI = . Mohseni et al., (Semi-)Systematic Review = ✓. Mohseni et al., Date ↓ = Aug 2021. Mohseni et al., Desiderata = Fidelity, Trustworthiness. Mohseni et al., Limited to = . Mohseni et al., Reported Metrics = 15 metrics. Yeh & Ravikumar, Primary VXAI Focus = ✓. Yeh & Ravikumar, Primarily Functionality- Grounded VXAI = ✓. Yeh & Ravikumar, (Semi-)Systematic Review = . Yeh & Ravikumar, Date ↓ = Jun 2021. Yeh & Ravikumar, Desiderata = . Yeh & Ravikumar, Limited to = . Yeh & Ravikumar, Reported Metrics = 7 metrics. Nauta et al., Primary VXAI Focus = ✓. Nauta et al., Primarily Functionality- Grounded VXAI =",
      "Topic": 1,
      "Name": "1_et_al_vxai_metrics",
      "Representation": [
        "et",
        "al",
        "vxai",
        "metrics",
        "desiderata",
        "systematic",
        "primarily",
        "semi",
        "date",
        "primary",
        "limited",
        "review",
        "focus",
        "functionality",
        "reported",
        "grounded"
      ],
      "Representative_Docs": [
        "3 Desiderata of XAI\nal., Desiderata = Correctness, Comprehensibility, Stability. Alangari et al., Limited to = . Alangari et al., Reported Metrics = 59 metrics. Le et al., Primary VXAI Focus = ✓. Le et al., Primarily Functionality- Grounded VXAI = ✓. Le et al., (Semi-)Systematic Review = ✓. Le et al., Date ↓ = Aug 2023. Le et al., Desiderata = Co-12 †. Le et al., Limited to = . Le et al., Reported Metrics = 86 metrics from 17 toolkits. Salih et al., Primary VXAI Focus = ✓. Salih et al., Primarily Functionality- Grounded VXAI = . Salih et al., (Semi-)Systematic Review = ✓. Salih et al., Date ↓ = Aug 2023. Salih et al., Desiderata = . Salih et al., Limited to = Cardiology. Salih et al., Reported Metrics = 27 metrics. Kadir et al., Primary VXAI Focus = ✓.",
        "3 Desiderata of XAI\net al., Primarily Functionality- Grounded VXAI = ✓. Bodria et al., (Semi-)Systematic Review = ✓. Bodria et al., Date ↓ = Nov 2021. Bodria et al., Desiderata = . Bodria et al., Limited to = . Bodria et al., Reported Metrics = 6 metrics. Sovrano et al., Primary VXAI Focus = ✓. Sovrano et al., Primarily Functionality- Grounded VXAI = . Sovrano et al., (Semi-)Systematic Review = . Sovrano et al., Date ↓ = Oct 2021. Sovrano et al., Desiderata = Similarity, Exactness, Fruitfulness. Sovrano et al., Limited to = . Sovrano et al., Reported Metrics = 22 metrics. Ras et al., Primary VXAI Focus = . Ras et al., Primarily Functionality- Grounded VXAI = . Ras et al., (Semi-)Systematic Review = . Ras et al., Date ↓ = Sep 2021. Ras et al., Desiderata = . Ras et",
        "3 Desiderata of XAI\net al., Primarily Functionality- Grounded VXAI = ✓. Bommer et al., (Semi-)Systematic Review = . Bommer et al., Date ↓ = Mar 2024. Bommer et al., Desiderata = Robustness, Faithfulness, Complexity, Lo- calization, Randomization. Bommer et al., Limited to = Climate Science. Bommer et al., Reported Metrics = 10 metrics. Li et al., Primary VXAI Focus = ✓. Li et al., Primarily Functionality- Grounded VXAI = ✓. Li et al., (Semi-)Systematic Review = . Li et al., Date ↓ = Dec 2023. Li et al., Desiderata = Faithfulness. Li et al., Limited to = Feature Attributions. Li et al., Reported Metrics = 6 metrics. Alangari et al., Primary VXAI Focus = ✓. Alangari et al., Primarily Functionality- Grounded VXAI = . Alangari et al., (Semi-)Systematic Review = . Alangari et al., Date ↓ = Aug 2023. Alangari et"
      ],
      "Top_n_words": "et - al - vxai - metrics - desiderata - systematic - primarily - semi - date - primary - limited - review - focus - functionality - reported - grounded",
      "Probability": 1.0,
      "Representative_document": false
    },
    {
      "Document": "3 Desiderata of XAI\n✓. Nauta et al., (Semi-)Systematic Review = ✓. Nauta et al., Date ↓ = May 2021. Nauta et al., Desiderata = Co-12 † Fidelity (Completeness, Soundness), Inter-. Nauta et al., Limited to = . Nauta et al., Reported Metrics = 28 metrics (already grouped). Zhou et al., Primary VXAI Focus = ✓. Zhou et al., Primarily Functionality- Grounded VXAI = ✓. Zhou et al., (Semi-)Systematic Review = . Zhou et al., Date ↓ = Jan 2021. Zhou et al., Desiderata = pretability, Broadness, Simplicity, Clarity). Zhou et al., Limited to = . Zhou et al., Reported Metrics = 17 metrics. Samek & Müller, Primary VXAI Focus = . Samek & Müller, Primarily Functionality- Grounded VXAI = . Samek & Müller, (Semi-)Systematic Review = . Samek & Müller, Date ↓ = Sep 2019. Samek & Müller, Desiderata = . Samek & Müller, Limited to = . Samek",
      "Topic": 1,
      "Name": "1_et_al_vxai_metrics",
      "Representation": [
        "et",
        "al",
        "vxai",
        "metrics",
        "desiderata",
        "systematic",
        "primarily",
        "semi",
        "date",
        "primary",
        "limited",
        "review",
        "focus",
        "functionality",
        "reported",
        "grounded"
      ],
      "Representative_Docs": [
        "3 Desiderata of XAI\nal., Desiderata = Correctness, Comprehensibility, Stability. Alangari et al., Limited to = . Alangari et al., Reported Metrics = 59 metrics. Le et al., Primary VXAI Focus = ✓. Le et al., Primarily Functionality- Grounded VXAI = ✓. Le et al., (Semi-)Systematic Review = ✓. Le et al., Date ↓ = Aug 2023. Le et al., Desiderata = Co-12 †. Le et al., Limited to = . Le et al., Reported Metrics = 86 metrics from 17 toolkits. Salih et al., Primary VXAI Focus = ✓. Salih et al., Primarily Functionality- Grounded VXAI = . Salih et al., (Semi-)Systematic Review = ✓. Salih et al., Date ↓ = Aug 2023. Salih et al., Desiderata = . Salih et al., Limited to = Cardiology. Salih et al., Reported Metrics = 27 metrics. Kadir et al., Primary VXAI Focus = ✓.",
        "3 Desiderata of XAI\net al., Primarily Functionality- Grounded VXAI = ✓. Bodria et al., (Semi-)Systematic Review = ✓. Bodria et al., Date ↓ = Nov 2021. Bodria et al., Desiderata = . Bodria et al., Limited to = . Bodria et al., Reported Metrics = 6 metrics. Sovrano et al., Primary VXAI Focus = ✓. Sovrano et al., Primarily Functionality- Grounded VXAI = . Sovrano et al., (Semi-)Systematic Review = . Sovrano et al., Date ↓ = Oct 2021. Sovrano et al., Desiderata = Similarity, Exactness, Fruitfulness. Sovrano et al., Limited to = . Sovrano et al., Reported Metrics = 22 metrics. Ras et al., Primary VXAI Focus = . Ras et al., Primarily Functionality- Grounded VXAI = . Ras et al., (Semi-)Systematic Review = . Ras et al., Date ↓ = Sep 2021. Ras et al., Desiderata = . Ras et",
        "3 Desiderata of XAI\net al., Primarily Functionality- Grounded VXAI = ✓. Bommer et al., (Semi-)Systematic Review = . Bommer et al., Date ↓ = Mar 2024. Bommer et al., Desiderata = Robustness, Faithfulness, Complexity, Lo- calization, Randomization. Bommer et al., Limited to = Climate Science. Bommer et al., Reported Metrics = 10 metrics. Li et al., Primary VXAI Focus = ✓. Li et al., Primarily Functionality- Grounded VXAI = ✓. Li et al., (Semi-)Systematic Review = . Li et al., Date ↓ = Dec 2023. Li et al., Desiderata = Faithfulness. Li et al., Limited to = Feature Attributions. Li et al., Reported Metrics = 6 metrics. Alangari et al., Primary VXAI Focus = ✓. Alangari et al., Primarily Functionality- Grounded VXAI = . Alangari et al., (Semi-)Systematic Review = . Alangari et al., Date ↓ = Aug 2023. Alangari et"
      ],
      "Top_n_words": "et - al - vxai - metrics - desiderata - systematic - primarily - semi - date - primary - limited - review - focus - functionality - reported - grounded",
      "Probability": 1.0,
      "Representative_document": false
    },
    {
      "Document": "3 Desiderata of XAI\n& Müller, Reported Metrics = 16 sources in text (metrics not listed) 40 sources in text. Yang et al., Primary VXAI Focus = ✓. Yang et al., Primarily Functionality- Grounded VXAI = ✓. Yang et al., (Semi-)Systematic Review = ✓. Yang et al., Date ↓ = Aug 2019. Yang et al., Desiderata = Generalizability, Fidelity, Persuasibility. Yang et al., Limited to = . Yang et al., Reported Metrics = (metrics not listed)\n† Co-12: Correctness, Output-Completeness, Consistency, Continuity, Contrastivity, Covariate Complexity, Compactness, Composition, Confidence, Context, Coherence, Controllability",
      "Topic": -1,
      "Name": "-1_et_al_yang_2021",
      "Representation": [
        "et",
        "al",
        "yang",
        "2021",
        "metrics",
        "vxai",
        "desiderata",
        "2019",
        "narrow",
        "text",
        "sources",
        "2024",
        "listed",
        "classification",
        "frameworks",
        "2020"
      ],
      "Representative_Docs": [
        "3.2 Proposed Framework of Desiderata\nBuilding on the desiderata frameworks established above and our findings on VXAI metrics, we propose a set of seven desiderata to serve as a categorization scheme for VXAI.\nOur goal is to offer a principled yet practical structure that enables consistent classification while avoiding the limitations of prior frameworks. These are either too narrow to accommodate relevant metrics or too broad and include properties beyond explainability. While properties such as fairness are often measured using XAI methods, we consider them beyond the scope of VXAI, because they assess the model's behavior rather than the explanation itself.",
        "1 Introduction\nThere exist a number of surveys and guidelines that address human-centered evaluations (Hoffman et al., 2018; Miller, 2019; Chromik & Schuessler, 2020; Holzinger et al., 2020; Franklin & Lagnado, 2022; Hsiao et al., 2021; Jesus et al., 2021; Langer et al., 2021; Mohseni et al., 2021; van der Waa et al., 2021; Silva et al., 2023). Unfortunately, the range of reviews dedicated to functionality-grounded evaluation is still limited. This is despite the advantage of offering objective, quantitative metrics without requiring human experiments, which can save both time and cost (Doshi-Velez & Kim, 2017; Samek et al., 2019; Zhou et al., 2021). Most existing studies are narrow and restricted to a specific application domain (Giuste et al., 2022; Arreche et al., 2024), including cybersecurity (Pawlicki et al., 2024), medical image classification (Patrício et al., 2023; Chaddad et al., 2024), electronic health record data",
        "3 Desiderata of XAI\n& Müller, Reported Metrics = 16 sources in text (metrics not listed) 40 sources in text. Yang et al., Primary VXAI Focus = ✓. Yang et al., Primarily Functionality- Grounded VXAI = ✓. Yang et al., (Semi-)Systematic Review = ✓. Yang et al., Date ↓ = Aug 2019. Yang et al., Desiderata = Generalizability, Fidelity, Persuasibility. Yang et al., Limited to = . Yang et al., Reported Metrics = (metrics not listed)\n† Co-12: Correctness, Output-Completeness, Consistency, Continuity, Contrastivity, Covariate Complexity, Compactness, Composition, Confidence, Context, Coherence, Controllability"
      ],
      "Top_n_words": "et - al - yang - 2021 - metrics - vxai - desiderata - 2019 - narrow - text - sources - 2024 - listed - classification - frameworks - 2020",
      "Probability": 0.273157828495043,
      "Representative_document": true
    },
    {
      "Document": "3 Desiderata of XAI\nTable 1: Overview of recent XAI reviews, sorted by date. The table indicates whether each survey primarily focused on evaluation metrics, whether it reported mainly functionality-grounded metrics, and whether a (semi-)structured review was conducted. The date refers to the earliest available point in the article's timeline; either the database query, submission, or publication, depending on what was reported. For each survey, we also report the desiderata used to classify the metrics and any limitations regarding explanation type or application domain. Note that not all surveys systematically listed their assessed metrics, so the reported metric count may vary depending on the method of extraction.\nof both stages of the explanation process. In this section, we first provide an overview of existing desiderata proposed in prior work. We then introduce a unified framework that systematically describes the requirements for ensuring technical soundness and for bridging the interpretation gap.",
      "Topic": 0,
      "Name": "0_al_explanation_et_xai",
      "Representation": [
        "al",
        "explanation",
        "et",
        "xai",
        "desiderata",
        "evaluation",
        "metrics",
        "2024",
        "explanations",
        "model",
        "2023",
        "explanans",
        "2021",
        "human",
        "grounded",
        "framework"
      ],
      "Representative_Docs": [
        "2 Related Work\nThere are five reviews, that we consider most similar to this work, as they present systematic functionality-grounded VXAI surveys. Le et al. (2023) and Nauta et al. (2023) both categorize the identified metrics based on a scheme of 12 properties, namely the Co-12 framework, which we discuss in more detail in Section 3. However, Le et al. (2023) restrict their analysis to metrics available through public XAI or VXAI toolkits (e.g., Quantus (Hedström et al., 2023)) and do not report on metrics introduced in the literature but not implemented in such libraries. Although there is some overlap with the study by Nauta et al. (2023), particularly in the inclusion of some identical metrics, their review also incorporates studies that merely apply VXAI metrics rather than introducing them. In contrast, our work provides detailed descriptions and categorizations of each identified metric. The review by Kadir et al. (2023) covers a broad range of domains and explanation types 2 and reports a wide variety of metrics. It also groups several metrics by method, a strategy shared by",
        "1 Introduction\nHowever, XAI is not a silver bullet. Van der Waa et al. (2021) point out, that humans tend to trust predictions more readily when an explanation is provided, often without carefully examining the explanation itself. This lack of critical scrutiny can lead to unwarranted trust, especially when decisions are taken based on incorrect or misleading explanations (Eiband et al., 2019; Jesus et al., 2021). To make matters worse, different XAI algorithms may result in conflicting explanations for the same model and sample (Krishna et al., 2022). Therefore, simply providing any explanation is not sufficient, but it is important to assess the quality of the explanation at hand (Sovrano et al., 2021). Unfortunately, while there is a plethora of XAI methods, evaluation of explanations is still an immature research area (Ribera & Lapedriza, 2019), with many studies relying on the notion of a good explanation as 'You'll know it when you see it', providing anecdotal evidence (i.e. small-scale qualitative validation) (Doshi-Velez & Kim, 2017; Nauta et al., 2023; Saarela &",
        "1 Introduction\n(Payrovnaziri et al., 2020), data and knowledge engineering (Li et al., 2020b), or timeseries classification (Theissler et al., 2022). Others focus on particular XAI approaches, such as visual explanations in CNNs (Mohamed et al., 2022) or instance-based explanations (Bayrak & Bach, 2024). Moreover, many surveys dedicate only limited attention to evaluation metrics, with their primary focus placed on the XAI methods themselves (Carvalho et al., 2019; Ding et al., 2022; Minh et al., 2022; Mohamed et al., 2022; Ali et al., 2023; Clement et al., 2023; Patrício et al., 2023; Chaddad et al., 2024; Gongane et al., 2024; Xua & Yang, 2024). By contrast, this review focuses exclusively on functionality-grounded evaluation across domains and is applicable to a wide range of XAI approaches."
      ],
      "Top_n_words": "al - explanation - et - xai - desiderata - evaluation - metrics - 2024 - explanations - model - 2023 - explanans - 2021 - human - grounded - framework",
      "Probability": 0.5126079216881354,
      "Representative_document": false
    },
    {
      "Document": "3.1 Common Formulation of Desiderata\nSeveral XAI surveys report that there is no ubiquitous consensus on appropriate desiderata, with some of the categories related to goals pursued through XAI, rather than standalone desiderata of XAI, e.g., Trustworthiness, Acceptance, or Fairness (Doshi-Velez & Kim, 2017; Langer et al., 2021; Vilone & Longo, 2021; Elkhawaga et al., 2023). Hence, we conduct a scoping review, reporting the main desiderata used by different authors and analyzing the similarities as well as differences in their formulations. For the sake of brevity, we exclude some of the papers listed in Table 1, as the missing ones either overlap considerably (e.g. Awal & Roy (2024) and Klein et al. (2024)), rely on a different notion of desiderata (e.g., Sovrano et al. (2021)), or use no desiderata at all. We will first present these frameworks using the authors' original terminology before introducing our own categorization scheme.",
      "Topic": 0,
      "Name": "0_al_explanation_et_xai",
      "Representation": [
        "al",
        "explanation",
        "et",
        "xai",
        "desiderata",
        "evaluation",
        "metrics",
        "2024",
        "explanations",
        "model",
        "2023",
        "explanans",
        "2021",
        "human",
        "grounded",
        "framework"
      ],
      "Representative_Docs": [
        "2 Related Work\nThere are five reviews, that we consider most similar to this work, as they present systematic functionality-grounded VXAI surveys. Le et al. (2023) and Nauta et al. (2023) both categorize the identified metrics based on a scheme of 12 properties, namely the Co-12 framework, which we discuss in more detail in Section 3. However, Le et al. (2023) restrict their analysis to metrics available through public XAI or VXAI toolkits (e.g., Quantus (Hedström et al., 2023)) and do not report on metrics introduced in the literature but not implemented in such libraries. Although there is some overlap with the study by Nauta et al. (2023), particularly in the inclusion of some identical metrics, their review also incorporates studies that merely apply VXAI metrics rather than introducing them. In contrast, our work provides detailed descriptions and categorizations of each identified metric. The review by Kadir et al. (2023) covers a broad range of domains and explanation types 2 and reports a wide variety of metrics. It also groups several metrics by method, a strategy shared by",
        "1 Introduction\nHowever, XAI is not a silver bullet. Van der Waa et al. (2021) point out, that humans tend to trust predictions more readily when an explanation is provided, often without carefully examining the explanation itself. This lack of critical scrutiny can lead to unwarranted trust, especially when decisions are taken based on incorrect or misleading explanations (Eiband et al., 2019; Jesus et al., 2021). To make matters worse, different XAI algorithms may result in conflicting explanations for the same model and sample (Krishna et al., 2022). Therefore, simply providing any explanation is not sufficient, but it is important to assess the quality of the explanation at hand (Sovrano et al., 2021). Unfortunately, while there is a plethora of XAI methods, evaluation of explanations is still an immature research area (Ribera & Lapedriza, 2019), with many studies relying on the notion of a good explanation as 'You'll know it when you see it', providing anecdotal evidence (i.e. small-scale qualitative validation) (Doshi-Velez & Kim, 2017; Nauta et al., 2023; Saarela &",
        "1 Introduction\n(Payrovnaziri et al., 2020), data and knowledge engineering (Li et al., 2020b), or timeseries classification (Theissler et al., 2022). Others focus on particular XAI approaches, such as visual explanations in CNNs (Mohamed et al., 2022) or instance-based explanations (Bayrak & Bach, 2024). Moreover, many surveys dedicate only limited attention to evaluation metrics, with their primary focus placed on the XAI methods themselves (Carvalho et al., 2019; Ding et al., 2022; Minh et al., 2022; Mohamed et al., 2022; Ali et al., 2023; Clement et al., 2023; Patrício et al., 2023; Chaddad et al., 2024; Gongane et al., 2024; Xua & Yang, 2024). By contrast, this review focuses exclusively on functionality-grounded evaluation across domains and is applicable to a wide range of XAI approaches."
      ],
      "Top_n_words": "al - explanation - et - xai - desiderata - evaluation - metrics - 2024 - explanations - model - 2023 - explanans - 2021 - human - grounded - framework",
      "Probability": 0.5919710066751858,
      "Representative_document": false
    },
    {
      "Document": "3.1 Common Formulation of Desiderata\nThe famous Co-12 properties, introduced by Nauta et al. (2023) and reused by Le et al. (2023), constitute one of the most extensive existing frameworks for categorizing XAI metrics. They group the properties along three different dimensions: Content ( Correctness, Completeness, Consistency, Continuity, Contrastivity, Covariate Complexity ), Presentation ( Compactness, Composition, Confidence ), and User ( Context, Coherence, Controllability ). While the first dimension focuses on the information contained in the explanans, the second and third dimensions address the way the information is conveyed. Although some of these human-centered properties can be measured through proxies, others may mainly be evaluated through human-grounded evaluation.",
      "Topic": 0,
      "Name": "0_al_explanation_et_xai",
      "Representation": [
        "al",
        "explanation",
        "et",
        "xai",
        "desiderata",
        "evaluation",
        "metrics",
        "2024",
        "explanations",
        "model",
        "2023",
        "explanans",
        "2021",
        "human",
        "grounded",
        "framework"
      ],
      "Representative_Docs": [
        "2 Related Work\nThere are five reviews, that we consider most similar to this work, as they present systematic functionality-grounded VXAI surveys. Le et al. (2023) and Nauta et al. (2023) both categorize the identified metrics based on a scheme of 12 properties, namely the Co-12 framework, which we discuss in more detail in Section 3. However, Le et al. (2023) restrict their analysis to metrics available through public XAI or VXAI toolkits (e.g., Quantus (Hedström et al., 2023)) and do not report on metrics introduced in the literature but not implemented in such libraries. Although there is some overlap with the study by Nauta et al. (2023), particularly in the inclusion of some identical metrics, their review also incorporates studies that merely apply VXAI metrics rather than introducing them. In contrast, our work provides detailed descriptions and categorizations of each identified metric. The review by Kadir et al. (2023) covers a broad range of domains and explanation types 2 and reports a wide variety of metrics. It also groups several metrics by method, a strategy shared by",
        "1 Introduction\nHowever, XAI is not a silver bullet. Van der Waa et al. (2021) point out, that humans tend to trust predictions more readily when an explanation is provided, often without carefully examining the explanation itself. This lack of critical scrutiny can lead to unwarranted trust, especially when decisions are taken based on incorrect or misleading explanations (Eiband et al., 2019; Jesus et al., 2021). To make matters worse, different XAI algorithms may result in conflicting explanations for the same model and sample (Krishna et al., 2022). Therefore, simply providing any explanation is not sufficient, but it is important to assess the quality of the explanation at hand (Sovrano et al., 2021). Unfortunately, while there is a plethora of XAI methods, evaluation of explanations is still an immature research area (Ribera & Lapedriza, 2019), with many studies relying on the notion of a good explanation as 'You'll know it when you see it', providing anecdotal evidence (i.e. small-scale qualitative validation) (Doshi-Velez & Kim, 2017; Nauta et al., 2023; Saarela &",
        "1 Introduction\n(Payrovnaziri et al., 2020), data and knowledge engineering (Li et al., 2020b), or timeseries classification (Theissler et al., 2022). Others focus on particular XAI approaches, such as visual explanations in CNNs (Mohamed et al., 2022) or instance-based explanations (Bayrak & Bach, 2024). Moreover, many surveys dedicate only limited attention to evaluation metrics, with their primary focus placed on the XAI methods themselves (Carvalho et al., 2019; Ding et al., 2022; Minh et al., 2022; Mohamed et al., 2022; Ali et al., 2023; Clement et al., 2023; Patrício et al., 2023; Chaddad et al., 2024; Gongane et al., 2024; Xua & Yang, 2024). By contrast, this review focuses exclusively on functionality-grounded evaluation across domains and is applicable to a wide range of XAI approaches."
      ],
      "Top_n_words": "al - explanation - et - xai - desiderata - evaluation - metrics - 2024 - explanations - model - 2023 - explanans - 2021 - human - grounded - framework",
      "Probability": 0.6193604178383162,
      "Representative_document": false
    },
    {
      "Document": "3.1 Common Formulation of Desiderata\nZhou et al. (2021), based on the taxonomy of Markus et al. (2021), define Interpretability and Fidelity as the two major components of explainability. The former is concerned with providing understandable explanantia and includes the properties of Clarity , Broadness , and Parsimony . Fidelity, on the other hand, refers to how accurately an explanans reflects the model's behavior, and consists of the properties of Completeness and Soundness .",
      "Topic": 0,
      "Name": "0_al_explanation_et_xai",
      "Representation": [
        "al",
        "explanation",
        "et",
        "xai",
        "desiderata",
        "evaluation",
        "metrics",
        "2024",
        "explanations",
        "model",
        "2023",
        "explanans",
        "2021",
        "human",
        "grounded",
        "framework"
      ],
      "Representative_Docs": [
        "2 Related Work\nThere are five reviews, that we consider most similar to this work, as they present systematic functionality-grounded VXAI surveys. Le et al. (2023) and Nauta et al. (2023) both categorize the identified metrics based on a scheme of 12 properties, namely the Co-12 framework, which we discuss in more detail in Section 3. However, Le et al. (2023) restrict their analysis to metrics available through public XAI or VXAI toolkits (e.g., Quantus (Hedström et al., 2023)) and do not report on metrics introduced in the literature but not implemented in such libraries. Although there is some overlap with the study by Nauta et al. (2023), particularly in the inclusion of some identical metrics, their review also incorporates studies that merely apply VXAI metrics rather than introducing them. In contrast, our work provides detailed descriptions and categorizations of each identified metric. The review by Kadir et al. (2023) covers a broad range of domains and explanation types 2 and reports a wide variety of metrics. It also groups several metrics by method, a strategy shared by",
        "1 Introduction\nHowever, XAI is not a silver bullet. Van der Waa et al. (2021) point out, that humans tend to trust predictions more readily when an explanation is provided, often without carefully examining the explanation itself. This lack of critical scrutiny can lead to unwarranted trust, especially when decisions are taken based on incorrect or misleading explanations (Eiband et al., 2019; Jesus et al., 2021). To make matters worse, different XAI algorithms may result in conflicting explanations for the same model and sample (Krishna et al., 2022). Therefore, simply providing any explanation is not sufficient, but it is important to assess the quality of the explanation at hand (Sovrano et al., 2021). Unfortunately, while there is a plethora of XAI methods, evaluation of explanations is still an immature research area (Ribera & Lapedriza, 2019), with many studies relying on the notion of a good explanation as 'You'll know it when you see it', providing anecdotal evidence (i.e. small-scale qualitative validation) (Doshi-Velez & Kim, 2017; Nauta et al., 2023; Saarela &",
        "1 Introduction\n(Payrovnaziri et al., 2020), data and knowledge engineering (Li et al., 2020b), or timeseries classification (Theissler et al., 2022). Others focus on particular XAI approaches, such as visual explanations in CNNs (Mohamed et al., 2022) or instance-based explanations (Bayrak & Bach, 2024). Moreover, many surveys dedicate only limited attention to evaluation metrics, with their primary focus placed on the XAI methods themselves (Carvalho et al., 2019; Ding et al., 2022; Minh et al., 2022; Mohamed et al., 2022; Ali et al., 2023; Clement et al., 2023; Patrício et al., 2023; Chaddad et al., 2024; Gongane et al., 2024; Xua & Yang, 2024). By contrast, this review focuses exclusively on functionality-grounded evaluation across domains and is applicable to a wide range of XAI approaches."
      ],
      "Top_n_words": "al - explanation - et - xai - desiderata - evaluation - metrics - 2024 - explanations - model - 2023 - explanans - 2021 - human - grounded - framework",
      "Probability": 0.8099437798098321,
      "Representative_document": false
    },
    {
      "Document": "3.1 Common Formulation of Desiderata\nThe framework proposed by Robnik-Šikonja & Bohanec (2018), which was adopted by Carvalho et al. (2019) and Molnar (2020), differentiates between properties of explanations and individual explanantia. Investigating the properties of methods (i.e. explanations), they consider Translucency , Portability , and Algorithmic Complexity , which can all be interpreted as desiderata, while Expressive Power is a descriptive property. The properties of individual explanantia include Comprehensibility , Importance , Representativeness , Fidelity , and Stability . However, their categorization encompasses further properties, which we do not consider as proper desiderata of XAI: Accuracy , Novelty , Certainty , and Consistency . Accuracy is a measurement of the underlying black-box model, while Novelty and Certainty are rather explanantia themselves, than properties of general explanations. Further, Consistency between different black-box models is not necessarily a useful measure, as different models may derive similar predictions based on different reasoning (see Rashomon Effect (Breiman, 2001; Leventi-Peetz & Weber, 2022)).",
      "Topic": 0,
      "Name": "0_al_explanation_et_xai",
      "Representation": [
        "al",
        "explanation",
        "et",
        "xai",
        "desiderata",
        "evaluation",
        "metrics",
        "2024",
        "explanations",
        "model",
        "2023",
        "explanans",
        "2021",
        "human",
        "grounded",
        "framework"
      ],
      "Representative_Docs": [
        "2 Related Work\nThere are five reviews, that we consider most similar to this work, as they present systematic functionality-grounded VXAI surveys. Le et al. (2023) and Nauta et al. (2023) both categorize the identified metrics based on a scheme of 12 properties, namely the Co-12 framework, which we discuss in more detail in Section 3. However, Le et al. (2023) restrict their analysis to metrics available through public XAI or VXAI toolkits (e.g., Quantus (Hedström et al., 2023)) and do not report on metrics introduced in the literature but not implemented in such libraries. Although there is some overlap with the study by Nauta et al. (2023), particularly in the inclusion of some identical metrics, their review also incorporates studies that merely apply VXAI metrics rather than introducing them. In contrast, our work provides detailed descriptions and categorizations of each identified metric. The review by Kadir et al. (2023) covers a broad range of domains and explanation types 2 and reports a wide variety of metrics. It also groups several metrics by method, a strategy shared by",
        "1 Introduction\nHowever, XAI is not a silver bullet. Van der Waa et al. (2021) point out, that humans tend to trust predictions more readily when an explanation is provided, often without carefully examining the explanation itself. This lack of critical scrutiny can lead to unwarranted trust, especially when decisions are taken based on incorrect or misleading explanations (Eiband et al., 2019; Jesus et al., 2021). To make matters worse, different XAI algorithms may result in conflicting explanations for the same model and sample (Krishna et al., 2022). Therefore, simply providing any explanation is not sufficient, but it is important to assess the quality of the explanation at hand (Sovrano et al., 2021). Unfortunately, while there is a plethora of XAI methods, evaluation of explanations is still an immature research area (Ribera & Lapedriza, 2019), with many studies relying on the notion of a good explanation as 'You'll know it when you see it', providing anecdotal evidence (i.e. small-scale qualitative validation) (Doshi-Velez & Kim, 2017; Nauta et al., 2023; Saarela &",
        "1 Introduction\n(Payrovnaziri et al., 2020), data and knowledge engineering (Li et al., 2020b), or timeseries classification (Theissler et al., 2022). Others focus on particular XAI approaches, such as visual explanations in CNNs (Mohamed et al., 2022) or instance-based explanations (Bayrak & Bach, 2024). Moreover, many surveys dedicate only limited attention to evaluation metrics, with their primary focus placed on the XAI methods themselves (Carvalho et al., 2019; Ding et al., 2022; Minh et al., 2022; Mohamed et al., 2022; Ali et al., 2023; Clement et al., 2023; Patrício et al., 2023; Chaddad et al., 2024; Gongane et al., 2024; Xua & Yang, 2024). By contrast, this review focuses exclusively on functionality-grounded evaluation across domains and is applicable to a wide range of XAI approaches."
      ],
      "Top_n_words": "al - explanation - et - xai - desiderata - evaluation - metrics - 2024 - explanations - model - 2023 - explanans - 2021 - human - grounded - framework",
      "Probability": 0.8641293440883969,
      "Representative_document": false
    },
    {
      "Document": "3.1 Common Formulation of Desiderata\nThe famous XAI review by Guidotti et al. (2018), inspired by earlier works such as Andrews et al. (1995) and Johansson et al. (2004), reports three less fine-grained desiderata: Interpretability , Fidelity , and Accuracy . Similar to previously discussed reviews, Interpretability describes human understandability, while Fidelity measures how well the explanans imitates the black box, and Accuracy focuses on predictive performance, which is outside the scope of XAI in our context. Additionally, Consistency is introduced by Andrews et al. (1995), expecting reproducible explanations, while Johansson et al. (2004) emphasize the explanation's algorithmic Scalability and Generality .",
      "Topic": 0,
      "Name": "0_al_explanation_et_xai",
      "Representation": [
        "al",
        "explanation",
        "et",
        "xai",
        "desiderata",
        "evaluation",
        "metrics",
        "2024",
        "explanations",
        "model",
        "2023",
        "explanans",
        "2021",
        "human",
        "grounded",
        "framework"
      ],
      "Representative_Docs": [
        "2 Related Work\nThere are five reviews, that we consider most similar to this work, as they present systematic functionality-grounded VXAI surveys. Le et al. (2023) and Nauta et al. (2023) both categorize the identified metrics based on a scheme of 12 properties, namely the Co-12 framework, which we discuss in more detail in Section 3. However, Le et al. (2023) restrict their analysis to metrics available through public XAI or VXAI toolkits (e.g., Quantus (Hedström et al., 2023)) and do not report on metrics introduced in the literature but not implemented in such libraries. Although there is some overlap with the study by Nauta et al. (2023), particularly in the inclusion of some identical metrics, their review also incorporates studies that merely apply VXAI metrics rather than introducing them. In contrast, our work provides detailed descriptions and categorizations of each identified metric. The review by Kadir et al. (2023) covers a broad range of domains and explanation types 2 and reports a wide variety of metrics. It also groups several metrics by method, a strategy shared by",
        "1 Introduction\nHowever, XAI is not a silver bullet. Van der Waa et al. (2021) point out, that humans tend to trust predictions more readily when an explanation is provided, often without carefully examining the explanation itself. This lack of critical scrutiny can lead to unwarranted trust, especially when decisions are taken based on incorrect or misleading explanations (Eiband et al., 2019; Jesus et al., 2021). To make matters worse, different XAI algorithms may result in conflicting explanations for the same model and sample (Krishna et al., 2022). Therefore, simply providing any explanation is not sufficient, but it is important to assess the quality of the explanation at hand (Sovrano et al., 2021). Unfortunately, while there is a plethora of XAI methods, evaluation of explanations is still an immature research area (Ribera & Lapedriza, 2019), with many studies relying on the notion of a good explanation as 'You'll know it when you see it', providing anecdotal evidence (i.e. small-scale qualitative validation) (Doshi-Velez & Kim, 2017; Nauta et al., 2023; Saarela &",
        "1 Introduction\n(Payrovnaziri et al., 2020), data and knowledge engineering (Li et al., 2020b), or timeseries classification (Theissler et al., 2022). Others focus on particular XAI approaches, such as visual explanations in CNNs (Mohamed et al., 2022) or instance-based explanations (Bayrak & Bach, 2024). Moreover, many surveys dedicate only limited attention to evaluation metrics, with their primary focus placed on the XAI methods themselves (Carvalho et al., 2019; Ding et al., 2022; Minh et al., 2022; Mohamed et al., 2022; Ali et al., 2023; Clement et al., 2023; Patrício et al., 2023; Chaddad et al., 2024; Gongane et al., 2024; Xua & Yang, 2024). By contrast, this review focuses exclusively on functionality-grounded evaluation across domains and is applicable to a wide range of XAI approaches."
      ],
      "Top_n_words": "al - explanation - et - xai - desiderata - evaluation - metrics - 2024 - explanations - model - 2023 - explanans - 2021 - human - grounded - framework",
      "Probability": 1.0,
      "Representative_document": false
    },
    {
      "Document": "3.1 Common Formulation of Desiderata\nAlvarez-Melis & Jaakkola (2018b), Jesus et al. (2021), and Alangari et al. (2023a) all report a similar set of desiderata. The understandability of explanations is measured in terms such as Interpretability , while Faithfulness and the corresponding desiderata give insight into how truthful the explanation is to the underlying black-box model. All three works further report the Stability of explanations as a desired property, assessing whether explanantia on similar inputs are similar.\nIn their Quantus toolkit, Hedström et al. (2023) (and the follow-up study by Bommer et al. (2024) as well), categorize their metrics partly through desiderata, namely Faithfulness , Robustness , and\nComplexity . Simultaneously, part of their metrics are grouped by their conceptual similarity, including Localization , Randomization , and Axiomatic .",
      "Topic": 0,
      "Name": "0_al_explanation_et_xai",
      "Representation": [
        "al",
        "explanation",
        "et",
        "xai",
        "desiderata",
        "evaluation",
        "metrics",
        "2024",
        "explanations",
        "model",
        "2023",
        "explanans",
        "2021",
        "human",
        "grounded",
        "framework"
      ],
      "Representative_Docs": [
        "2 Related Work\nThere are five reviews, that we consider most similar to this work, as they present systematic functionality-grounded VXAI surveys. Le et al. (2023) and Nauta et al. (2023) both categorize the identified metrics based on a scheme of 12 properties, namely the Co-12 framework, which we discuss in more detail in Section 3. However, Le et al. (2023) restrict their analysis to metrics available through public XAI or VXAI toolkits (e.g., Quantus (Hedström et al., 2023)) and do not report on metrics introduced in the literature but not implemented in such libraries. Although there is some overlap with the study by Nauta et al. (2023), particularly in the inclusion of some identical metrics, their review also incorporates studies that merely apply VXAI metrics rather than introducing them. In contrast, our work provides detailed descriptions and categorizations of each identified metric. The review by Kadir et al. (2023) covers a broad range of domains and explanation types 2 and reports a wide variety of metrics. It also groups several metrics by method, a strategy shared by",
        "1 Introduction\nHowever, XAI is not a silver bullet. Van der Waa et al. (2021) point out, that humans tend to trust predictions more readily when an explanation is provided, often without carefully examining the explanation itself. This lack of critical scrutiny can lead to unwarranted trust, especially when decisions are taken based on incorrect or misleading explanations (Eiband et al., 2019; Jesus et al., 2021). To make matters worse, different XAI algorithms may result in conflicting explanations for the same model and sample (Krishna et al., 2022). Therefore, simply providing any explanation is not sufficient, but it is important to assess the quality of the explanation at hand (Sovrano et al., 2021). Unfortunately, while there is a plethora of XAI methods, evaluation of explanations is still an immature research area (Ribera & Lapedriza, 2019), with many studies relying on the notion of a good explanation as 'You'll know it when you see it', providing anecdotal evidence (i.e. small-scale qualitative validation) (Doshi-Velez & Kim, 2017; Nauta et al., 2023; Saarela &",
        "1 Introduction\n(Payrovnaziri et al., 2020), data and knowledge engineering (Li et al., 2020b), or timeseries classification (Theissler et al., 2022). Others focus on particular XAI approaches, such as visual explanations in CNNs (Mohamed et al., 2022) or instance-based explanations (Bayrak & Bach, 2024). Moreover, many surveys dedicate only limited attention to evaluation metrics, with their primary focus placed on the XAI methods themselves (Carvalho et al., 2019; Ding et al., 2022; Minh et al., 2022; Mohamed et al., 2022; Ali et al., 2023; Clement et al., 2023; Patrício et al., 2023; Chaddad et al., 2024; Gongane et al., 2024; Xua & Yang, 2024). By contrast, this review focuses exclusively on functionality-grounded evaluation across domains and is applicable to a wide range of XAI approaches."
      ],
      "Top_n_words": "al - explanation - et - xai - desiderata - evaluation - metrics - 2024 - explanations - model - 2023 - explanans - 2021 - human - grounded - framework",
      "Probability": 0.829074647340532,
      "Representative_document": false
    },
    {
      "Document": "3.1 Common Formulation of Desiderata\nFinally, the Compare-xAI benchmark by Belaid et al. (2022) organizes functional tests into six categories, namely Fidelity , the robustness-related Stability and Fragility , the interpretability desideratum Simplicity , and the explanation-methods-focused Stress and Portability (which they integrate under 'Other').",
      "Topic": 0,
      "Name": "0_al_explanation_et_xai",
      "Representation": [
        "al",
        "explanation",
        "et",
        "xai",
        "desiderata",
        "evaluation",
        "metrics",
        "2024",
        "explanations",
        "model",
        "2023",
        "explanans",
        "2021",
        "human",
        "grounded",
        "framework"
      ],
      "Representative_Docs": [
        "2 Related Work\nThere are five reviews, that we consider most similar to this work, as they present systematic functionality-grounded VXAI surveys. Le et al. (2023) and Nauta et al. (2023) both categorize the identified metrics based on a scheme of 12 properties, namely the Co-12 framework, which we discuss in more detail in Section 3. However, Le et al. (2023) restrict their analysis to metrics available through public XAI or VXAI toolkits (e.g., Quantus (Hedström et al., 2023)) and do not report on metrics introduced in the literature but not implemented in such libraries. Although there is some overlap with the study by Nauta et al. (2023), particularly in the inclusion of some identical metrics, their review also incorporates studies that merely apply VXAI metrics rather than introducing them. In contrast, our work provides detailed descriptions and categorizations of each identified metric. The review by Kadir et al. (2023) covers a broad range of domains and explanation types 2 and reports a wide variety of metrics. It also groups several metrics by method, a strategy shared by",
        "1 Introduction\nHowever, XAI is not a silver bullet. Van der Waa et al. (2021) point out, that humans tend to trust predictions more readily when an explanation is provided, often without carefully examining the explanation itself. This lack of critical scrutiny can lead to unwarranted trust, especially when decisions are taken based on incorrect or misleading explanations (Eiband et al., 2019; Jesus et al., 2021). To make matters worse, different XAI algorithms may result in conflicting explanations for the same model and sample (Krishna et al., 2022). Therefore, simply providing any explanation is not sufficient, but it is important to assess the quality of the explanation at hand (Sovrano et al., 2021). Unfortunately, while there is a plethora of XAI methods, evaluation of explanations is still an immature research area (Ribera & Lapedriza, 2019), with many studies relying on the notion of a good explanation as 'You'll know it when you see it', providing anecdotal evidence (i.e. small-scale qualitative validation) (Doshi-Velez & Kim, 2017; Nauta et al., 2023; Saarela &",
        "1 Introduction\n(Payrovnaziri et al., 2020), data and knowledge engineering (Li et al., 2020b), or timeseries classification (Theissler et al., 2022). Others focus on particular XAI approaches, such as visual explanations in CNNs (Mohamed et al., 2022) or instance-based explanations (Bayrak & Bach, 2024). Moreover, many surveys dedicate only limited attention to evaluation metrics, with their primary focus placed on the XAI methods themselves (Carvalho et al., 2019; Ding et al., 2022; Minh et al., 2022; Mohamed et al., 2022; Ali et al., 2023; Clement et al., 2023; Patrício et al., 2023; Chaddad et al., 2024; Gongane et al., 2024; Xua & Yang, 2024). By contrast, this review focuses exclusively on functionality-grounded evaluation across domains and is applicable to a wide range of XAI approaches."
      ],
      "Top_n_words": "al - explanation - et - xai - desiderata - evaluation - metrics - 2024 - explanations - model - 2023 - explanans - 2021 - human - grounded - framework",
      "Probability": 0.5490631931323581,
      "Representative_document": false
    },
    {
      "Document": "3.1 Common Formulation of Desiderata\nWhile many existing frameworks overlap conceptually, a unified and practically usable categorization scheme for VXAI metrics is still lacking. This requires a structured set of desiderata that defines what makes a good explanation and supports consistent classification of metrics. Prior work often enforces a rigid one-to-one mapping between metrics and desiderata; in contrast, we decouple these dimensions, defining a set of mostly independent desiderata to which each metric may contribute individually or jointly. Lightweight frameworks tend to omit critical aspects of explanation quality, while broader ones sometimes include goals that are not intrinsic to the explanation itself (e.g., accuracy). We restrict our scope to properties that reflect the explanation rather than the underlying model and clarify excluded cases after presenting our set. Although all desiderata rely on proxies, we limit ourselves to properties that are quantifiable in principle. Highly abstract or vague notions lacking empirical grounding are omitted. Lastly, our framework is designed to be extensible, allowing the integration of future desiderata as the field evolves.",
      "Topic": 0,
      "Name": "0_al_explanation_et_xai",
      "Representation": [
        "al",
        "explanation",
        "et",
        "xai",
        "desiderata",
        "evaluation",
        "metrics",
        "2024",
        "explanations",
        "model",
        "2023",
        "explanans",
        "2021",
        "human",
        "grounded",
        "framework"
      ],
      "Representative_Docs": [
        "2 Related Work\nThere are five reviews, that we consider most similar to this work, as they present systematic functionality-grounded VXAI surveys. Le et al. (2023) and Nauta et al. (2023) both categorize the identified metrics based on a scheme of 12 properties, namely the Co-12 framework, which we discuss in more detail in Section 3. However, Le et al. (2023) restrict their analysis to metrics available through public XAI or VXAI toolkits (e.g., Quantus (Hedström et al., 2023)) and do not report on metrics introduced in the literature but not implemented in such libraries. Although there is some overlap with the study by Nauta et al. (2023), particularly in the inclusion of some identical metrics, their review also incorporates studies that merely apply VXAI metrics rather than introducing them. In contrast, our work provides detailed descriptions and categorizations of each identified metric. The review by Kadir et al. (2023) covers a broad range of domains and explanation types 2 and reports a wide variety of metrics. It also groups several metrics by method, a strategy shared by",
        "1 Introduction\nHowever, XAI is not a silver bullet. Van der Waa et al. (2021) point out, that humans tend to trust predictions more readily when an explanation is provided, often without carefully examining the explanation itself. This lack of critical scrutiny can lead to unwarranted trust, especially when decisions are taken based on incorrect or misleading explanations (Eiband et al., 2019; Jesus et al., 2021). To make matters worse, different XAI algorithms may result in conflicting explanations for the same model and sample (Krishna et al., 2022). Therefore, simply providing any explanation is not sufficient, but it is important to assess the quality of the explanation at hand (Sovrano et al., 2021). Unfortunately, while there is a plethora of XAI methods, evaluation of explanations is still an immature research area (Ribera & Lapedriza, 2019), with many studies relying on the notion of a good explanation as 'You'll know it when you see it', providing anecdotal evidence (i.e. small-scale qualitative validation) (Doshi-Velez & Kim, 2017; Nauta et al., 2023; Saarela &",
        "1 Introduction\n(Payrovnaziri et al., 2020), data and knowledge engineering (Li et al., 2020b), or timeseries classification (Theissler et al., 2022). Others focus on particular XAI approaches, such as visual explanations in CNNs (Mohamed et al., 2022) or instance-based explanations (Bayrak & Bach, 2024). Moreover, many surveys dedicate only limited attention to evaluation metrics, with their primary focus placed on the XAI methods themselves (Carvalho et al., 2019; Ding et al., 2022; Minh et al., 2022; Mohamed et al., 2022; Ali et al., 2023; Clement et al., 2023; Patrício et al., 2023; Chaddad et al., 2024; Gongane et al., 2024; Xua & Yang, 2024). By contrast, this review focuses exclusively on functionality-grounded evaluation across domains and is applicable to a wide range of XAI approaches."
      ],
      "Top_n_words": "al - explanation - et - xai - desiderata - evaluation - metrics - 2024 - explanations - model - 2023 - explanans - 2021 - human - grounded - framework",
      "Probability": 0.6062099528361736,
      "Representative_document": false
    },
    {
      "Document": "3.2 Proposed Framework of Desiderata\nBuilding on the desiderata frameworks established above and our findings on VXAI metrics, we propose a set of seven desiderata to serve as a categorization scheme for VXAI.\nOur goal is to offer a principled yet practical structure that enables consistent classification while avoiding the limitations of prior frameworks. These are either too narrow to accommodate relevant metrics or too broad and include properties beyond explainability. While properties such as fairness are often measured using XAI methods, we consider them beyond the scope of VXAI, because they assess the model's behavior rather than the explanation itself.",
      "Topic": -1,
      "Name": "-1_et_al_yang_2021",
      "Representation": [
        "et",
        "al",
        "yang",
        "2021",
        "metrics",
        "vxai",
        "desiderata",
        "2019",
        "narrow",
        "text",
        "sources",
        "2024",
        "listed",
        "classification",
        "frameworks",
        "2020"
      ],
      "Representative_Docs": [
        "3.2 Proposed Framework of Desiderata\nBuilding on the desiderata frameworks established above and our findings on VXAI metrics, we propose a set of seven desiderata to serve as a categorization scheme for VXAI.\nOur goal is to offer a principled yet practical structure that enables consistent classification while avoiding the limitations of prior frameworks. These are either too narrow to accommodate relevant metrics or too broad and include properties beyond explainability. While properties such as fairness are often measured using XAI methods, we consider them beyond the scope of VXAI, because they assess the model's behavior rather than the explanation itself.",
        "1 Introduction\nThere exist a number of surveys and guidelines that address human-centered evaluations (Hoffman et al., 2018; Miller, 2019; Chromik & Schuessler, 2020; Holzinger et al., 2020; Franklin & Lagnado, 2022; Hsiao et al., 2021; Jesus et al., 2021; Langer et al., 2021; Mohseni et al., 2021; van der Waa et al., 2021; Silva et al., 2023). Unfortunately, the range of reviews dedicated to functionality-grounded evaluation is still limited. This is despite the advantage of offering objective, quantitative metrics without requiring human experiments, which can save both time and cost (Doshi-Velez & Kim, 2017; Samek et al., 2019; Zhou et al., 2021). Most existing studies are narrow and restricted to a specific application domain (Giuste et al., 2022; Arreche et al., 2024), including cybersecurity (Pawlicki et al., 2024), medical image classification (Patrício et al., 2023; Chaddad et al., 2024), electronic health record data",
        "3 Desiderata of XAI\n& Müller, Reported Metrics = 16 sources in text (metrics not listed) 40 sources in text. Yang et al., Primary VXAI Focus = ✓. Yang et al., Primarily Functionality- Grounded VXAI = ✓. Yang et al., (Semi-)Systematic Review = ✓. Yang et al., Date ↓ = Aug 2019. Yang et al., Desiderata = Generalizability, Fidelity, Persuasibility. Yang et al., Limited to = . Yang et al., Reported Metrics = (metrics not listed)\n† Co-12: Correctness, Output-Completeness, Consistency, Continuity, Contrastivity, Covariate Complexity, Compactness, Composition, Confidence, Context, Coherence, Controllability"
      ],
      "Top_n_words": "et - al - yang - 2021 - metrics - vxai - desiderata - 2019 - narrow - text - sources - 2024 - listed - classification - frameworks - 2020",
      "Probability": 0.2625941117486108,
      "Representative_document": true
    },
    {
      "Document": "3.2 Proposed Framework of Desiderata\nBuilding on the two-stage view of explaining described by Palacio et al. (2021) (i.e., presenting factual information followed by human interpretation), we define two complementary dimensions of explanation quality. The Technical (T) dimension comprises desiderata that assess the factual correctness, robustness, and reliability of the explanation, ensuring that it faithfully reflects the model's reasoning. In contrast, the Interpretability (I) dimension captures how the explanation is conveyed and how accessible, intuitive, and useful it is to a general-purpose user. This separation is aligned with existing frameworks such as the Co-12 properties (Nauta et al., 2023) and the taxonomy by Zhou et al. (2021). The desiderata are designed to be as independent from each other as possible, allowing for reliable quantification of different aspects relevant to trustworthy XAI.\nWe present our categorization scheme and its relation to other frameworks in Figure 3 and introduce them in more detail in the following paragraphs. In total, we define seven desiderata, two associated with Interpretability, and five belonging to the Technical dimension:",
      "Topic": 0,
      "Name": "0_al_explanation_et_xai",
      "Representation": [
        "al",
        "explanation",
        "et",
        "xai",
        "desiderata",
        "evaluation",
        "metrics",
        "2024",
        "explanations",
        "model",
        "2023",
        "explanans",
        "2021",
        "human",
        "grounded",
        "framework"
      ],
      "Representative_Docs": [
        "2 Related Work\nThere are five reviews, that we consider most similar to this work, as they present systematic functionality-grounded VXAI surveys. Le et al. (2023) and Nauta et al. (2023) both categorize the identified metrics based on a scheme of 12 properties, namely the Co-12 framework, which we discuss in more detail in Section 3. However, Le et al. (2023) restrict their analysis to metrics available through public XAI or VXAI toolkits (e.g., Quantus (Hedström et al., 2023)) and do not report on metrics introduced in the literature but not implemented in such libraries. Although there is some overlap with the study by Nauta et al. (2023), particularly in the inclusion of some identical metrics, their review also incorporates studies that merely apply VXAI metrics rather than introducing them. In contrast, our work provides detailed descriptions and categorizations of each identified metric. The review by Kadir et al. (2023) covers a broad range of domains and explanation types 2 and reports a wide variety of metrics. It also groups several metrics by method, a strategy shared by",
        "1 Introduction\nHowever, XAI is not a silver bullet. Van der Waa et al. (2021) point out, that humans tend to trust predictions more readily when an explanation is provided, often without carefully examining the explanation itself. This lack of critical scrutiny can lead to unwarranted trust, especially when decisions are taken based on incorrect or misleading explanations (Eiband et al., 2019; Jesus et al., 2021). To make matters worse, different XAI algorithms may result in conflicting explanations for the same model and sample (Krishna et al., 2022). Therefore, simply providing any explanation is not sufficient, but it is important to assess the quality of the explanation at hand (Sovrano et al., 2021). Unfortunately, while there is a plethora of XAI methods, evaluation of explanations is still an immature research area (Ribera & Lapedriza, 2019), with many studies relying on the notion of a good explanation as 'You'll know it when you see it', providing anecdotal evidence (i.e. small-scale qualitative validation) (Doshi-Velez & Kim, 2017; Nauta et al., 2023; Saarela &",
        "1 Introduction\n(Payrovnaziri et al., 2020), data and knowledge engineering (Li et al., 2020b), or timeseries classification (Theissler et al., 2022). Others focus on particular XAI approaches, such as visual explanations in CNNs (Mohamed et al., 2022) or instance-based explanations (Bayrak & Bach, 2024). Moreover, many surveys dedicate only limited attention to evaluation metrics, with their primary focus placed on the XAI methods themselves (Carvalho et al., 2019; Ding et al., 2022; Minh et al., 2022; Mohamed et al., 2022; Ali et al., 2023; Clement et al., 2023; Patrício et al., 2023; Chaddad et al., 2024; Gongane et al., 2024; Xua & Yang, 2024). By contrast, this review focuses exclusively on functionality-grounded evaluation across domains and is applicable to a wide range of XAI approaches."
      ],
      "Top_n_words": "al - explanation - et - xai - desiderata - evaluation - metrics - 2024 - explanations - model - 2023 - explanans - 2021 - human - grounded - framework",
      "Probability": 1.0,
      "Representative_document": false
    },
    {
      "Document": "3.2 Proposed Framework of Desiderata\n- (I) Parsimony : The explanation should keep the explanans concise to support interpretability.\n- (T) Coverage : The explanation should provide an explanans for every explanandum.\n- (I) Plausibility : The explanation should shape the explanans to align with human expectations.\n- (T) Fidelity : The explanation should make the explanans reflect the model's true reasoning.\n- (T) Consistency : The explanation should produce stable explanantia across repeated evaluations.\n- (T) Continuity : The explanation should ensure that similar explananda yield similar explanantia.\n- (T) Efficiency : The explanation should compute the explanans efficiently and broadly.",
      "Topic": 0,
      "Name": "0_al_explanation_et_xai",
      "Representation": [
        "al",
        "explanation",
        "et",
        "xai",
        "desiderata",
        "evaluation",
        "metrics",
        "2024",
        "explanations",
        "model",
        "2023",
        "explanans",
        "2021",
        "human",
        "grounded",
        "framework"
      ],
      "Representative_Docs": [
        "2 Related Work\nThere are five reviews, that we consider most similar to this work, as they present systematic functionality-grounded VXAI surveys. Le et al. (2023) and Nauta et al. (2023) both categorize the identified metrics based on a scheme of 12 properties, namely the Co-12 framework, which we discuss in more detail in Section 3. However, Le et al. (2023) restrict their analysis to metrics available through public XAI or VXAI toolkits (e.g., Quantus (Hedström et al., 2023)) and do not report on metrics introduced in the literature but not implemented in such libraries. Although there is some overlap with the study by Nauta et al. (2023), particularly in the inclusion of some identical metrics, their review also incorporates studies that merely apply VXAI metrics rather than introducing them. In contrast, our work provides detailed descriptions and categorizations of each identified metric. The review by Kadir et al. (2023) covers a broad range of domains and explanation types 2 and reports a wide variety of metrics. It also groups several metrics by method, a strategy shared by",
        "1 Introduction\nHowever, XAI is not a silver bullet. Van der Waa et al. (2021) point out, that humans tend to trust predictions more readily when an explanation is provided, often without carefully examining the explanation itself. This lack of critical scrutiny can lead to unwarranted trust, especially when decisions are taken based on incorrect or misleading explanations (Eiband et al., 2019; Jesus et al., 2021). To make matters worse, different XAI algorithms may result in conflicting explanations for the same model and sample (Krishna et al., 2022). Therefore, simply providing any explanation is not sufficient, but it is important to assess the quality of the explanation at hand (Sovrano et al., 2021). Unfortunately, while there is a plethora of XAI methods, evaluation of explanations is still an immature research area (Ribera & Lapedriza, 2019), with many studies relying on the notion of a good explanation as 'You'll know it when you see it', providing anecdotal evidence (i.e. small-scale qualitative validation) (Doshi-Velez & Kim, 2017; Nauta et al., 2023; Saarela &",
        "1 Introduction\n(Payrovnaziri et al., 2020), data and knowledge engineering (Li et al., 2020b), or timeseries classification (Theissler et al., 2022). Others focus on particular XAI approaches, such as visual explanations in CNNs (Mohamed et al., 2022) or instance-based explanations (Bayrak & Bach, 2024). Moreover, many surveys dedicate only limited attention to evaluation metrics, with their primary focus placed on the XAI methods themselves (Carvalho et al., 2019; Ding et al., 2022; Minh et al., 2022; Mohamed et al., 2022; Ali et al., 2023; Clement et al., 2023; Patrício et al., 2023; Chaddad et al., 2024; Gongane et al., 2024; Xua & Yang, 2024). By contrast, this review focuses exclusively on functionality-grounded evaluation across domains and is applicable to a wide range of XAI approaches."
      ],
      "Top_n_words": "al - explanation - et - xai - desiderata - evaluation - metrics - 2024 - explanations - model - 2023 - explanans - 2021 - human - grounded - framework",
      "Probability": 0.8645417863056478,
      "Representative_document": false
    },
    {
      "Document": "3.2.1 Parsimony\nThe explanation should keep the explanans concise to support interpretability.",
      "Topic": 0,
      "Name": "0_al_explanation_et_xai",
      "Representation": [
        "al",
        "explanation",
        "et",
        "xai",
        "desiderata",
        "evaluation",
        "metrics",
        "2024",
        "explanations",
        "model",
        "2023",
        "explanans",
        "2021",
        "human",
        "grounded",
        "framework"
      ],
      "Representative_Docs": [
        "2 Related Work\nThere are five reviews, that we consider most similar to this work, as they present systematic functionality-grounded VXAI surveys. Le et al. (2023) and Nauta et al. (2023) both categorize the identified metrics based on a scheme of 12 properties, namely the Co-12 framework, which we discuss in more detail in Section 3. However, Le et al. (2023) restrict their analysis to metrics available through public XAI or VXAI toolkits (e.g., Quantus (Hedström et al., 2023)) and do not report on metrics introduced in the literature but not implemented in such libraries. Although there is some overlap with the study by Nauta et al. (2023), particularly in the inclusion of some identical metrics, their review also incorporates studies that merely apply VXAI metrics rather than introducing them. In contrast, our work provides detailed descriptions and categorizations of each identified metric. The review by Kadir et al. (2023) covers a broad range of domains and explanation types 2 and reports a wide variety of metrics. It also groups several metrics by method, a strategy shared by",
        "1 Introduction\nHowever, XAI is not a silver bullet. Van der Waa et al. (2021) point out, that humans tend to trust predictions more readily when an explanation is provided, often without carefully examining the explanation itself. This lack of critical scrutiny can lead to unwarranted trust, especially when decisions are taken based on incorrect or misleading explanations (Eiband et al., 2019; Jesus et al., 2021). To make matters worse, different XAI algorithms may result in conflicting explanations for the same model and sample (Krishna et al., 2022). Therefore, simply providing any explanation is not sufficient, but it is important to assess the quality of the explanation at hand (Sovrano et al., 2021). Unfortunately, while there is a plethora of XAI methods, evaluation of explanations is still an immature research area (Ribera & Lapedriza, 2019), with many studies relying on the notion of a good explanation as 'You'll know it when you see it', providing anecdotal evidence (i.e. small-scale qualitative validation) (Doshi-Velez & Kim, 2017; Nauta et al., 2023; Saarela &",
        "1 Introduction\n(Payrovnaziri et al., 2020), data and knowledge engineering (Li et al., 2020b), or timeseries classification (Theissler et al., 2022). Others focus on particular XAI approaches, such as visual explanations in CNNs (Mohamed et al., 2022) or instance-based explanations (Bayrak & Bach, 2024). Moreover, many surveys dedicate only limited attention to evaluation metrics, with their primary focus placed on the XAI methods themselves (Carvalho et al., 2019; Ding et al., 2022; Minh et al., 2022; Mohamed et al., 2022; Ali et al., 2023; Clement et al., 2023; Patrício et al., 2023; Chaddad et al., 2024; Gongane et al., 2024; Xua & Yang, 2024). By contrast, this review focuses exclusively on functionality-grounded evaluation across domains and is applicable to a wide range of XAI approaches."
      ],
      "Top_n_words": "al - explanation - et - xai - desiderata - evaluation - metrics - 2024 - explanations - model - 2023 - explanans - 2021 - human - grounded - framework",
      "Probability": 0.6834101042591529,
      "Representative_document": false
    }
  ],
  "document_chunks": null
}