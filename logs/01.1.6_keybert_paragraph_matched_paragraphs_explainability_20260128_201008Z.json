{
  "created_at": "20260128_201008Z",
  "keyword": "explainability",
  "generated_synonyms": [
    "interpretability",
    "transparency",
    "understandability",
    "accountability",
    "justifiability",
    "non-opaqueness",
    "explain",
    "interpret",
    "understand",
    "justify"
  ],
  "num_matched": 87,
  "num_unmatched": 210,
  "matched": [
    {
      "index": 0,
      "paragraph": "Unifying VXAI: A Systematic Review and Framework for the Evaluation of Explainable AI\nDavid Dembinsky\ndavid.dembinsky@dfki.de\nGerman Research Center for Artificial Intelligence\nAdriano Lucieri\nadriano.lucieri@dfki.de\nGerman Research Center for Artificial Intelligence\nStanislav Frolov\nstanislav.frolov@dfki.de\nGerman Research Center for Artificial Intelligence\nHiba Najjar\nhiba.najjar@dfki.de\nGerman Research Center for Artificial Intelligence\nKo Watanabe\nko.watanabe@dfki.de\nGerman Research Center for Artificial Intelligence\nAndreas Dengel\nandreas.dengel@dfki.de\nGerman Research Center for Artificial Intelligence",
      "keywords": [
        "ai",
        "explainable",
        "vxai",
        "intelligence",
        "unifying",
        "systematic",
        "artificial",
        "evaluation"
      ],
      "match_type": "near_paragraph_global",
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": 0.5907081365585327
    },
    {
      "index": 1,
      "paragraph": "Abstract\nModern AI systems frequently rely on opaque black-box models, most notably Deep Neural Networks, whose performance stems from complex architectures with millions of learned parameters. While powerful, their complexity poses a major challenge to trustworthiness, particularly due to a lack of transparency. Explainable AI (XAI) addresses this issue by providing human-understandable explanations of model behavior. However, to ensure their usefulness and trustworthiness, such explanations must be rigorously evaluated. Despite the growing number of XAI methods, the field lacks standardized evaluation protocols and consensus on appropriate metrics. To address this gap, we conduct a systematic literature review following the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines and introduce a unified framework for the eValuation of XAI (VXAI) . We identify 362 relevant publications and aggregate their contributions into 41 functionally similar metric groups. In addition, we propose a three-dimensional categorization scheme spanning explanation type, evaluation contextuality, and explanation quality desiderata. Our framework provides the most comprehensive and structured overview of VXAI to date. It supports systematic metric selection, promotes comparability across methods, and offers a flexible foundation for future extensions.",
      "keywords": [
        "ai",
        "explainable",
        "explanations",
        "xai",
        "neural",
        "explanation",
        "contextuality",
        "systematic"
      ],
      "match_type": "near_paragraph_global",
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": 0.5679348707199097
    },
    {
      "index": 2,
      "paragraph": "1 Introduction\nExplainable AI (XAI) is a research area of growing interest to both AI researchers and practitioners. It aims to alleviate the black-box issue of current deep-learning models, which can reach stunning performances at the expense of their interpretability (Vilone & Longo, 2021). Government-affiliated initiatives, such as the European Union High-Level Expert Group on AI (2019), the U.S. National",
      "keywords": [
        "interpretability",
        "explainable",
        "ai",
        "xai",
        "deep",
        "learning",
        "models",
        "expert"
      ],
      "match_type": "synonym",
      "matched_terms": [
        "interpretability"
      ],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 3,
      "paragraph": "1 Introduction\nInstitute of Standards and Technology (2023), and the DARPA initiative (Gunning & Aha, 2019), identified XAI as a crucial part of Trustworthy AI. Especially as it helps AI systems in serving the 'right to explain' its decisions (Goodman & Flaxman, 2017) and fosters user trust through understanding of the system (Morandini et al., 2023). XAI already plays a fundamental role in making high-stakes AI systems more trustworthy (Saarela & Podgorelec, 2024; Xua & Yang, 2024), with broad applications in areas such as healthcare, finance, autonomous driving, natural disaster detection, energy management, military and remote sensing (Adadi & Berrada, 2018; Markus et al., 2021; Saraswat et al., 2022; Kadir et al., 2023; Hosain et al., 2024; Höhl et al., 2024). Furthermore, explainability is used to help with other dimensions of trustworthiness like privacy, robustness, or fairness (Doshi-Velez & Kim, 2017; Yang et al., 2019; Arrieta et al., 2020;",
      "keywords": [
        "explainability",
        "ai",
        "xai",
        "trustworthiness",
        "autonomous",
        "trust",
        "trustworthy",
        "privacy"
      ],
      "match_type": "exact",
      "matched_terms": [
        "explainability"
      ],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 7,
      "paragraph": "1 Introduction\nBecause evaluation is still not performed consistently and seldom systematically (Adadi & Berrada, 2018; Lipton, 2018; Payrovnaziri et al., 2020; Messina et al., 2022; Lopes et al., 2022; De Camargo et al., 2023; Kadir et al., 2023; Nauta et al., 2023; Mohamed et al., 2024; Naveed et al., 2024; Saarela & Podgorelec, 2024; Salih et al., 2024a), the community frequently calls to develop comprehensive and unified evaluation standards (Pinto & Paquette, 2024; Saarela & Podgorelec, 2024; Xua & Yang, 2024). A central motivation behind such efforts is to enable the comparison of explanations and asses whether explainability is achieved (Markus et al., 2021; Zhou et al., 2021)",
      "keywords": [
        "explainability",
        "explanations",
        "evaluation",
        "comparison",
        "comprehensive",
        "consistently",
        "systematically",
        "efforts"
      ],
      "match_type": "exact",
      "matched_terms": [
        "explainability"
      ],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 9,
      "paragraph": "1 Introduction\nSince explanations are meant to aid humans, human-grounded evaluation remains the gold standard to assess their effectiveness in assisting humans (Doshi-Velez & Kim, 2017; Gunning & Aha, 2019; Miller et al., 2017). However, the faithfulness (i.e., technical correctness) of an explanation and the plausibility to humans do not necessarily correlate (Wiegreffe & Pinter, 2019; Jacovi & Goldberg, 2020; Atanasova, 2024a). Therefore, human-grounded evaluation of an explanation's comprehensibility should be distinguished from functionality-grounded evaluation of its faithfulness (Nauta et al., 2023). Especially, humans can't confidently attribute whether an unexpected explanans (i.e., the information provided to explain a decision ) is caused by a faulty explanation (process 1 1 ) or a flawed black-box model (Robnik-Šikonja & Bohanec, 2018; Zhang et al., 2019a); see Figure 2 for an illustration. In\n1 The exact definitions of explanandum, explanation, and explanans are provided at the end of Section 1.",
      "keywords": [
        "explanations",
        "comprehensibility",
        "explanans",
        "explanation",
        "explanandum",
        "functionality",
        "plausibility",
        "evaluation"
      ],
      "match_type": "near_paragraph_global",
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": 0.6397176384925842
    },
    {
      "index": 13,
      "paragraph": "Contributions\nDespite the growing number of proposed metrics, a comprehensive and unified framework for functionality-grounded evaluation is still missing. Further, the inconsistent use of terms such as interpretability, comprehensibility, understandability, transparency, and explainability (Koh & Liang, 2017; Guidotti et al., 2018; Arrieta et al., 2020; Markus et al., 2021) hampers conceptual clarity and comparability across approaches. To address this gap, we introduce a framework called eValuation of Explainable Artificial Intelligence (VXAI) , aimed at unifying functionality-grounded evaluation for XAI.\nOur contributions are as follows:",
      "keywords": [
        "explainability",
        "interpretability",
        "understandability",
        "comprehensibility",
        "explainable",
        "intelligence",
        "xai",
        "evaluation"
      ],
      "match_type": "exact",
      "matched_terms": [
        "explainability"
      ],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 17,
      "paragraph": "Terminology\nmethod (explanation) and its output (explanans), since most evaluation metrics necessarily assess the quality of explanations through the quality of their generated outputs.\nAs defined by the XAI Handbook, interpretation (or interpretability ) refers to the subsequent assignment of meaning to an explanation. It describes the process through which a human infers knowledge about the explanandum using the explanans. This step significantly influences the success of the explanation and also depends on the receiving human's (the explainee 's) mental model.\nexplanandum (pl. explananda ): What is to be explained, i.e. a model and its prediction. explanation (pl. explanations ): The process of explaining, i.e. the XAI algorithm. explanans (pl. explanantia ): The explaining information, i.e. the output of an explanation.",
      "keywords": [
        "interpretability",
        "explanans",
        "explanandum",
        "explanations",
        "explananda",
        "explanantia",
        "interpretation",
        "explanation"
      ],
      "match_type": "synonym",
      "matched_terms": [
        "interpretability"
      ],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 22,
      "paragraph": "3 Desiderata of XAI\nA well-founded evaluation of XAI methods requires clearly defined criteria for what constitutes a good explanation. To establish such criteria, we must first reflect on the role of explanations in the context of XAI. According to the definition in the XAI Handbook (Palacio et al., 2021), explaining a model and its behavior is a two-stage process: first, factual information about the model's decision process is generated (the explanans); this is then interpreted by the human user. The first stage can be evaluated using technical criteria that assess whether the model's reasoning has been captured truthfully and reliably. The second stage depends on the interpretability of the explanation, which can be assessed using general cognitive principles, even in the absence of a specific user model.\nTo capture the multifaceted nature of explanation quality, a number of desiderata have been proposed in the literature. We interpret these as functionality-grounded expectations that reflect the demands\n2 The explanation type refers to both the design of the explanation algorithm and, consequently, the nature of the resulting explanans, as introduced in Subsection 5.1.2.",
      "keywords": [
        "interpretability",
        "explanations",
        "xai",
        "explanans",
        "explanation",
        "explaining",
        "reasoning",
        "interpret"
      ],
      "match_type": "synonym",
      "matched_terms": [
        "interpretability",
        "interpret"
      ],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 39,
      "paragraph": "3.1 Common Formulation of Desiderata\nZhou et al. (2021), based on the taxonomy of Markus et al. (2021), define Interpretability and Fidelity as the two major components of explainability. The former is concerned with providing understandable explanantia and includes the properties of Clarity , Broadness , and Parsimony . Fidelity, on the other hand, refers to how accurately an explanans reflects the model's behavior, and consists of the properties of Completeness and Soundness .",
      "keywords": [
        "explainability",
        "interpretability",
        "clarity",
        "explanans",
        "explanantia",
        "soundness",
        "broadness",
        "completeness"
      ],
      "match_type": "exact",
      "matched_terms": [
        "explainability"
      ],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 40,
      "paragraph": "3.1 Common Formulation of Desiderata\nThe framework proposed by Robnik-Šikonja & Bohanec (2018), which was adopted by Carvalho et al. (2019) and Molnar (2020), differentiates between properties of explanations and individual explanantia. Investigating the properties of methods (i.e. explanations), they consider Translucency , Portability , and Algorithmic Complexity , which can all be interpreted as desiderata, while Expressive Power is a descriptive property. The properties of individual explanantia include Comprehensibility , Importance , Representativeness , Fidelity , and Stability . However, their categorization encompasses further properties, which we do not consider as proper desiderata of XAI: Accuracy , Novelty , Certainty , and Consistency . Accuracy is a measurement of the underlying black-box model, while Novelty and Certainty are rather explanantia themselves, than properties of general explanations. Further, Consistency between different black-box models is not necessarily a useful measure, as different models may derive similar predictions based on different reasoning (see Rashomon Effect (Breiman, 2001; Leventi-Peetz & Weber, 2022)).",
      "keywords": [
        "explanations",
        "comprehensibility",
        "explanantia",
        "categorization",
        "xai",
        "accuracy",
        "predictions",
        "complexity"
      ],
      "match_type": "near_paragraph_global",
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": 0.5237685441970825
    },
    {
      "index": 41,
      "paragraph": "3.1 Common Formulation of Desiderata\nThe famous XAI review by Guidotti et al. (2018), inspired by earlier works such as Andrews et al. (1995) and Johansson et al. (2004), reports three less fine-grained desiderata: Interpretability , Fidelity , and Accuracy . Similar to previously discussed reviews, Interpretability describes human understandability, while Fidelity measures how well the explanans imitates the black box, and Accuracy focuses on predictive performance, which is outside the scope of XAI in our context. Additionally, Consistency is introduced by Andrews et al. (1995), expecting reproducible explanations, while Johansson et al. (2004) emphasize the explanation's algorithmic Scalability and Generality .",
      "keywords": [
        "interpretability",
        "predictive",
        "understandability",
        "explanations",
        "xai",
        "explanans",
        "accuracy",
        "explanation"
      ],
      "match_type": "synonym",
      "matched_terms": [
        "understandability",
        "interpretability"
      ],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 42,
      "paragraph": "3.1 Common Formulation of Desiderata\nAlvarez-Melis & Jaakkola (2018b), Jesus et al. (2021), and Alangari et al. (2023a) all report a similar set of desiderata. The understandability of explanations is measured in terms such as Interpretability , while Faithfulness and the corresponding desiderata give insight into how truthful the explanation is to the underlying black-box model. All three works further report the Stability of explanations as a desired property, assessing whether explanantia on similar inputs are similar.\nIn their Quantus toolkit, Hedström et al. (2023) (and the follow-up study by Bommer et al. (2024) as well), categorize their metrics partly through desiderata, namely Faithfulness , Robustness , and\nComplexity . Simultaneously, part of their metrics are grouped by their conceptual similarity, including Localization , Randomization , and Axiomatic .",
      "keywords": [
        "interpretability",
        "understandability",
        "explanations",
        "complexity",
        "explanation",
        "explanantia",
        "insight",
        "robustness"
      ],
      "match_type": "synonym",
      "matched_terms": [
        "understandability",
        "interpretability"
      ],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 43,
      "paragraph": "3.1 Common Formulation of Desiderata\nFinally, the Compare-xAI benchmark by Belaid et al. (2022) organizes functional tests into six categories, namely Fidelity , the robustness-related Stability and Fragility , the interpretability desideratum Simplicity , and the explanation-methods-focused Stress and Portability (which they integrate under 'Other').",
      "keywords": [
        "robustness",
        "interpretability",
        "benchmark",
        "tests",
        "functional",
        "xai",
        "compare",
        "fragility"
      ],
      "match_type": "synonym",
      "matched_terms": [
        "interpretability"
      ],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 45,
      "paragraph": "3.2 Proposed Framework of Desiderata\nBuilding on the desiderata frameworks established above and our findings on VXAI metrics, we propose a set of seven desiderata to serve as a categorization scheme for VXAI.\nOur goal is to offer a principled yet practical structure that enables consistent classification while avoiding the limitations of prior frameworks. These are either too narrow to accommodate relevant metrics or too broad and include properties beyond explainability. While properties such as fairness are often measured using XAI methods, we consider them beyond the scope of VXAI, because they assess the model's behavior rather than the explanation itself.",
      "keywords": [
        "fairness",
        "explainability",
        "categorization",
        "classification",
        "vxai",
        "xai",
        "behavior",
        "desiderata"
      ],
      "match_type": "exact",
      "matched_terms": [
        "explainability"
      ],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 46,
      "paragraph": "3.2 Proposed Framework of Desiderata\nBuilding on the two-stage view of explaining described by Palacio et al. (2021) (i.e., presenting factual information followed by human interpretation), we define two complementary dimensions of explanation quality. The Technical (T) dimension comprises desiderata that assess the factual correctness, robustness, and reliability of the explanation, ensuring that it faithfully reflects the model's reasoning. In contrast, the Interpretability (I) dimension captures how the explanation is conveyed and how accessible, intuitive, and useful it is to a general-purpose user. This separation is aligned with existing frameworks such as the Co-12 properties (Nauta et al., 2023) and the taxonomy by Zhou et al. (2021). The desiderata are designed to be as independent from each other as possible, allowing for reliable quantification of different aspects relevant to trustworthy XAI.\nWe present our categorization scheme and its relation to other frameworks in Figure 3 and introduce them in more detail in the following paragraphs. In total, we define seven desiderata, two associated with Interpretability, and five belonging to the Technical dimension:",
      "keywords": [
        "interpretability",
        "explanation",
        "intuitive",
        "explaining",
        "reasoning",
        "xai",
        "interpretation",
        "conveyed"
      ],
      "match_type": "synonym",
      "matched_terms": [
        "interpretability"
      ],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 47,
      "paragraph": "3.2 Proposed Framework of Desiderata\n- (I) Parsimony : The explanation should keep the explanans concise to support interpretability.\n- (T) Coverage : The explanation should provide an explanans for every explanandum.\n- (I) Plausibility : The explanation should shape the explanans to align with human expectations.\n- (T) Fidelity : The explanation should make the explanans reflect the model's true reasoning.\n- (T) Consistency : The explanation should produce stable explanantia across repeated evaluations.\n- (T) Continuity : The explanation should ensure that similar explananda yield similar explanantia.\n- (T) Efficiency : The explanation should compute the explanans efficiently and broadly.",
      "keywords": [
        "interpretability",
        "reasoning",
        "explanans",
        "explananda",
        "evaluations",
        "explanandum",
        "explanantia",
        "explanation"
      ],
      "match_type": "synonym",
      "matched_terms": [
        "interpretability"
      ],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 48,
      "paragraph": "3.2.1 Parsimony\nThe explanation should keep the explanans concise to support interpretability.",
      "keywords": [
        "parsimony",
        "interpretability",
        "explanans",
        "explanation",
        "concise",
        "support"
      ],
      "match_type": "synonym",
      "matched_terms": [
        "interpretability"
      ],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 49,
      "paragraph": "XAI Desiderata\nDesiderata reported by only individual sources (per column) are marked with an asterisk (*)\nThe primary purpose of an explanation is to convey information about the black-box model or its decision process to humans. Therefore, the resulting explanans must be expressed in a way that the human mind can grasp easily to increase the explanations success. While actual interpretability, can only be evaluated through human-grounded evaluation, Parsimony is one of the most prevalent proxies defined to serve as a functionality-grounded approximation. Since our mental capacity is limited and we tend to struggle with an overload of information (Miller, 1956; 2019; Alangari et al., 2023a), providing short and simple explanantia helps humans understand more effectively.",
      "keywords": [
        "interpretability",
        "explanations",
        "explanans",
        "explanation",
        "explanantia",
        "functionality",
        "understand",
        "parsimony"
      ],
      "match_type": "synonym",
      "matched_terms": [
        "understand",
        "interpretability"
      ],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 50,
      "paragraph": "XAI Desiderata\nNauta et al. (2023) introduce the property of Compactness, arguing that a briefer explanans is easier to understand. Similarly, they use Covariate Complexity to assess how complex the features are that constitute the explanans, where higher interpretability is supported by providing a few high-level concepts in favor of a very granular explanans. Both of these aspects are summarized under Parsimony by Markus et al. (2021) and Zhou et al. (2021), preferring simpler explanantia over longer or more complex ones. The scheme used in the Quantus library (Hedström et al., 2023; Bommer et al., 2024) defines a group called Complexity. It specifically tests for concise explanantia and aims to have as few features as possible to be easier to understand. The associated interpretability desiderata from other authors are defined less explicitly, but similarly favoring simpler explanantia (Andrews et al., 1995; Johansson et al., 2004; Guidotti et al., 2018), proposing that simple explanantia should be short (Alvarez-Melis & Jaakkola,",
      "keywords": [
        "interpretability",
        "explanans",
        "complexity",
        "explanantia",
        "xai",
        "concise",
        "simpler",
        "features"
      ],
      "match_type": "synonym",
      "matched_terms": [
        "interpretability"
      ],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 51,
      "paragraph": "XAI Desiderata\n2018b; Jesus et al., 2021; Alangari et al., 2023a), promoting small explanantia and only focussing on relevant parts (Robnik-Šikonja & Bohanec, 2018; Carvalho et al., 2019; Molnar, 2020), and expecting an explanans with concentrated information to facilitate human understanding (Belaid et al., 2022).\nFollowing the proposed definitions, we include Parsimony as one of our desiderata. It expects explanantia to be as brief and concise as possible, to ensure that rationales can be understood easily and fast. We focus Parsimony exclusively on this aspect, as other associated properties are either covered by separate desiderata (such as truthfulness of the explanation) or excluded entirely as they are not functionality-grounded (general understandability of the explanation).",
      "keywords": [
        "understandability",
        "explanantia",
        "explanans",
        "parsimony",
        "rationales",
        "understanding",
        "explanation",
        "concise"
      ],
      "match_type": "synonym",
      "matched_terms": [
        "understandability"
      ],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 52,
      "paragraph": "3.2.2 Plausibility\nThe explanation should shape the explanans to align with human expectations.",
      "keywords": [
        "plausibility",
        "explanans",
        "explanation",
        "expectations",
        "human",
        "shape",
        "align"
      ],
      "match_type": "near_paragraph_global",
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": 0.6324886679649353
    },
    {
      "index": 53,
      "paragraph": "3.2.2 Plausibility\nTo improve the acceptance of explanations and facilitate their interpretation, the Plausibility desideratum is contained in most authors' interpretability desiderata. Coherence as defined by Nauta et al. (2023) is the accordance of an explanans with the user's previous knowledge and expectations. The metrics classified under Localization by Hedström et al. (2023) evaluate whether an explanation shows a rationale similar to what humans would expect. This is concordant with the definition of Comprehensibility (Alvarez-Melis & Jaakkola, 2018b; Jesus et al., 2021; Alangari et al., 2023a), which states that an explanans should be similar to what a human expert would choose as the correct rationale. Furthermore, Nauta et al. (2023) introduce Contrastivity, which supports Plausibility, as an explanans should be specific to the given explanandum. Similarly, Clarity is introduced by Markus et al. (2021) and Zhou et al. (2021) expecting explanantia to be unambiguous.",
      "keywords": [
        "interpretability",
        "comprehensibility",
        "coherence",
        "plausibility",
        "explanandum",
        "explanations",
        "explanans",
        "explanantia"
      ],
      "match_type": "synonym",
      "matched_terms": [
        "interpretability"
      ],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 54,
      "paragraph": "3.2.2 Plausibility\nWe include the Plausibility desideratum, which encompasses the idea that explanations should align with human knowledge and intuition. On one hand, this includes human expectations towards the result (explanans), e.g., 'The model focuses on what a human would focus on'. On the other hand, the XAI methods' behavior (explanation) should also be aligned with human intuition, e.g., 'The outputs for individual inputs should differ'.",
      "keywords": [
        "plausibility",
        "explanans",
        "explanations",
        "xai",
        "intuition",
        "explanation",
        "model",
        "desideratum"
      ],
      "match_type": "near_paragraph_global",
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": 0.538018524646759
    },
    {
      "index": 55,
      "paragraph": "3.2.3 Coverage\nThe explanation should provide an explanans for every explanandum.\nThe extent to which an explanation or explanans can be applied is considered by two frameworks. Unfortunately, definitions from both surveys are vague. Markus et al. (2021) and Zhou et al. (2021) define the Broadness of an explanation as 'how generally applicable' it is, without further elaboration on the implications of this definition. More concretely, Representativeness is presented by Robnik-\nŠikonja & Bohanec (2018) Carvalho et al. (2019), and Molnar (2020). It reflects the number of explananda that are covered by an individual explanans, although this definition focuses mainly on the distinction between global and local explanation methods.\nTo add more clarity to these definitions, we include Coverage with an alternative definition. It defines the amount of explananda that are covered by the explanation, i.e. reflecting whether there exists an explanans for every data input or output.",
      "keywords": [
        "explanans",
        "explanandum",
        "explananda",
        "broadness",
        "explanation",
        "elaboration",
        "clarity",
        "extent"
      ],
      "match_type": "near_paragraph_global",
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": 0.5021144151687622
    },
    {
      "index": 57,
      "paragraph": "3.2.4 Fidelity\nCorrectness refers to whether the explanation truthfully represents the internal logic and decision process of the black-box model. It is one of the most frequently emphasized desiderata across the reviewed frameworks. Without correctness, even the most interpretable or simple explanation may provide no meaningful insight. Terms like Faithfulness, Truthfulness, and Fidelity are often used interchangeably in literature to describe this idea. Correctness encompasses both local fidelity for individual explanantia and global alignment across the dataset (Robnik-Šikonja & Bohanec, 2018; Carvalho et al., 2019; Molnar, 2020). The general consensus is that an explanation should reveal what truly drives the model's outputs (Alvarez-Melis & Jaakkola, 2018b; Markus et al., 2021; Zhou et al., 2021; Belaid et al., 2022; Alangari et al., 2023a; Nauta et al., 2023). It is commonly assessed by how well the explanation reflects or mimics the model's behavior (Andrews et al., 1995; Johansson et al., 2004; Guidotti et al., 2018).",
      "keywords": [
        "interpretable",
        "faithfulness",
        "correctness",
        "truthfulness",
        "explanation",
        "fidelity",
        "alignment",
        "insight"
      ],
      "match_type": "near_paragraph_global",
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": 0.5067234039306641
    },
    {
      "index": 58,
      "paragraph": "3.2.4 Fidelity\nCompleteness, in contrast, describes how much of the model's reasoning is captured by the explanation. According to the Co-12 properties by Nauta et al. (2023), an explanation should ideally include the full scope of the model's rationale. Some authors treat Completeness as a sub-aspect of Fidelity (Markus et al., 2021; Zhou et al., 2021), while others define Fidelity itself as the capacity to capture all of the information embodied in the model (Andrews et al., 1995; Johansson et al., 2004; Guidotti et al., 2018).",
      "keywords": [
        "completeness",
        "fidelity",
        "information",
        "aspect",
        "properties",
        "explanation",
        "describes",
        "reasoning"
      ],
      "match_type": "near_paragraph_global",
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": 0.5425524115562439
    },
    {
      "index": 60,
      "paragraph": "3.2.5 Continuity\nThe explanation should ensure that similar explananda yield similar explanantia.\nJust as the robustness or stability of a standard AI model is of great interest, similar expectations apply to explainability. Most frameworks highlight this desideratum, and various metrics have been proposed to assess how stable or reliable explanations are. However, the terminology used in literature is inconsistent, at times overlapping and at other times diverging in meaning.\nNauta et al. (2023) introduce the term Continuity as the smoothness of the explanation, i.e. similar explananda should yield similar explanantia. Others refer to this idea as Stability (Robnik-Šikonja & Bohanec, 2018; Carvalho et al., 2019; Molnar, 2020), describing it as the resilience against slight variations in input features that do not alter the model's prediction. The term Robustness is used by Alvarez-Melis & Jaakkola (2018b), Jesus et al. (2021), and Alangari et al. (2023a), to describe the same behavior, referring to it as a key requirement for trustworthy XAI.",
      "keywords": [
        "explainability",
        "robustness",
        "explanations",
        "explanantia",
        "explananda",
        "ai",
        "explanation",
        "stability"
      ],
      "match_type": "exact",
      "matched_terms": [
        "explainability"
      ],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 61,
      "paragraph": "3.2.5 Continuity\nThe Quantus toolkit reflects the prevalence of this concept, providing a 'Robustness' metric category (Hedström et al., 2023; Bommer et al., 2024), which assesses the similarity of explanantia under minor changes in input. Finally, Belaid et al. (2022) cover the same idea under the term Stability. In addition, they assess Fragility, which they define as the resilience of explanations against malicious manipulation, such as adversarial attacks.\nOur Continuity desideratum covers both of the mentioned properties. It includes the smoothness of explanations with respect to 'naïve' changes in the explanandum that ideally do not affect the model's behavior, as well as the resilience of explanations against malicious manipulation attempts. Note, that this includes changes over the input data as well as the model. We decided to adopt the term Continuity instead of Stability or Robustness, to reduce the possible confusion with model robustness.",
      "keywords": [
        "robustness",
        "adversarial",
        "fragility",
        "continuity",
        "explanations",
        "explanantia",
        "stability",
        "explanandum"
      ],
      "match_type": "near_paragraph_global",
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": 0.47026535868644714
    },
    {
      "index": 62,
      "paragraph": "3.2.6 Consistency\nThe explanation should produce stable explanantia across repeated evaluations.\nWhile Continuity investigates the smoothness and similarity between similar but different inputs, the Consistency of explanations for identical inputs also needs to be considered. However, few authors explicitly consider this desideratum.\nConsistency is introduced by Nauta et al. (2023) as a direct measure of the determinism of an XAI algorithm. Similarly, one part of the definition of Stability by Robnik-Šikonja & Bohanec (2018), Carvalho et al. (2019), and Molnar (2020) considers variations in explanations based on nondeterminism. The oldest formulation of Consistency is given by Andrews et al. (1995) and considers explanation methods to be consistent when they produce equivalent results under repetition.",
      "keywords": [
        "consistency",
        "determinism",
        "explanations",
        "xai",
        "explanation",
        "consistent",
        "stability",
        "explanantia"
      ],
      "match_type": "near_paragraph_global",
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": 0.4588810205459595
    },
    {
      "index": 63,
      "paragraph": "3.2.6 Consistency\nHowever, several frameworks additionally consider the similarity of explanantia generated from different models trained on the same data (Robnik-Šikonja & Bohanec, 2018; Carvalho et al., 2019; Molnar, 2020; Nauta et al., 2023). Yet, different models can produce the same prediction while relying on entirely different internal reasoning. This is especially true as there are often multiple valid reasons for the same event, also known as the Rashomon Effect (Breiman, 2001; Leventi-Peetz & Weber, 2022).\nWe include Consistency using the initial formulations, i.e., explanations should be deterministic or selfconsistent, always presenting the same explanans for identical explananda. While the latter definition is present in one of the identified metrics, we do not explicitly add it to the definition of our Consistency desideratum, as we do not believe that different explananda (inputs), i.e. different models, necessarily result in identical explanantia (outputs).",
      "keywords": [
        "consistency",
        "explanans",
        "prediction",
        "explananda",
        "explanations",
        "explanantia",
        "similarity",
        "reasoning"
      ],
      "match_type": "near_paragraph_global",
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": 0.503150999546051
    },
    {
      "index": 64,
      "paragraph": "3.2.7 Efficiency\nThe explanation should compute the explanans efficiently and broadly.\nFinally, out of practical considerations, we want explanations to be conveniently applicable. This includes considering the range of models or situations in which the algorithm can be effectively applied. Simultaneously, it also includes the time it takes to compute an individual explanans.\nThe first property is introduced as Portability and Translucency throughout literature (Robnik-Šikonja & Bohanec, 2018; Carvalho et al., 2019; Molnar, 2020). Portability is the variety of models for which an explanation can be used, while Translucency is the necessity of the explanation algorithm to have access to the internals of the model. Similarly, Johansson et al. (2004) measure Generality, given by the restrictions or overhead necessary to apply an explanation to specific models. Belaid et al. (2022) refer to Portability as the diverse set of models to which the explanation can be applied.",
      "keywords": [
        "explanations",
        "explanans",
        "explanation",
        "compute",
        "efficiency",
        "generality",
        "portability",
        "translucency"
      ],
      "match_type": "near_paragraph_global",
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": 0.5807712078094482
    },
    {
      "index": 68,
      "paragraph": "3.2.8 Excluded Desiderata\nFurther, both Confidence (Nauta et al., 2023) and the closely related Certainty (Robnik-Šikonja & Bohanec, 2018; Carvalho et al., 2019; Molnar, 2020) are excluded. They both describe whether confidence scores of the model's decisions or the explanans are displayed. However, this is not a general desideratum for explanations, but can rather be seen as providing assisting information, which is also an explanans in itself, and is then subject to all the desiderata of our framework. Related to this, Novelty (Robnik-Šikonja & Bohanec, 2018; Carvalho et al., 2019; Molnar, 2020) refers to whether data instances lie within or outside the training distribution. While relevant to assessing model behavior or uncertainty, this property does not characterize the quality of the explanation itself. Instead, it serves as contextual information that may inform or accompany an explanans, but is not an intrinsic requirement for explainability.",
      "keywords": [
        "explainability",
        "explanations",
        "contextual",
        "explanans",
        "uncertainty",
        "explanation",
        "novelty",
        "confidence"
      ],
      "match_type": "exact",
      "matched_terms": [
        "explainability"
      ],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 70,
      "paragraph": "3.2.8 Excluded Desiderata\nWhile it is a standard metric during model training, Accuracy of explanations is also reported by several frameworks (Andrews et al., 1995; Johansson et al., 2004; Guidotti et al., 2018; Robnik-Šikonja & Bohanec, 2018; Carvalho et al., 2019; Molnar, 2020). Importantly, this is only meaningful for specific types of explanations, such as white-box surrogate models (e.g., decision trees, see Subsection 5.1.2 for a definition). It is different from 'surrogate fidelity', since this measures the performance against the black-box model's predictions and not the ground-truth. We restrict our framework to explanantia derived from black-box models, rather than evaluating interpretable models that are directly trained on the data as predictive models themselves.",
      "keywords": [
        "predictive",
        "interpretable",
        "explanations",
        "accuracy",
        "predictions",
        "explanantia",
        "desiderata",
        "model"
      ],
      "match_type": "near_paragraph_global",
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": 0.5638258457183838
    },
    {
      "index": 73,
      "paragraph": "3.3 Considerations\nSecond, a truly useful explanation can only be achieved when considering the interplay between the individual Desiderata. Otherwise, trivial explanantia can be constructed to optimize individual desiderata, as we illustrate through the following two examples: a) A Saliency Map, which highlights the entire input image, is certainly complete, as it encompasses all relevant features; however, it is hardly parsimonious and unlikely to be correct, as not every region is going to be truly decisive for the prediction. b) A Concept explanation which identifies the same identical concept regardless of the input may be maximally consistent, continuous, and parsimonious; nevertheless, it will unlikely be either correct or complete, as it disregards input-specific variation.",
      "keywords": [
        "saliency",
        "explanation",
        "features",
        "concept",
        "encompasses",
        "explanantia",
        "prediction",
        "highlights"
      ],
      "match_type": "near_paragraph_global",
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": 0.4687766134738922
    },
    {
      "index": 74,
      "paragraph": "3.3 Considerations\nFurther, to ensure the usefulness of XAI to humans, we believe that some hierarchy exists between our defined desiderata. While an exact ordering is implausible, as it depends on the exact setting and needs, at least some general guidelines can be given: Fidelity is the foundation of the entire evaluation process. If there is no necessity for an explanation to be correct and complete, it can be completely arbitrary and will not necessarily give any true insights into the underlying model. This would undermine the entire purpose of XAI. If important parts of the rationale are excluded in the explanans, it may result in an incomplete (and hence incorrect) understanding of the decision process. While not as crucial, but still important, Consistency and Continuity considerably contribute to the trustworthiness of XAI. Without them, explanations may vary unpredictably, as explanantia can change without reasons rooted in the model's behavior. Given these four desiderata, the explanation can be expected to give solid results from a technical viewpoint. Hence, the interpretability dimensions, Parsimony and Plausibility , can be considered next, as they facilitate understanding, but do not assess whether an explanation is actually reliable. Finally, Coverage and",
      "keywords": [
        "interpretability",
        "xai",
        "explanations",
        "explanantia",
        "explanans",
        "evaluation",
        "explanation",
        "rationale"
      ],
      "match_type": "synonym",
      "matched_terms": [
        "interpretability"
      ],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 75,
      "paragraph": "3.3 Considerations\nEfficiency are useful properties of an explanation, but can be seen as a bonus, making them readily applicable to a wide set of explananda, including data and models. However, depending on the use case and context, individual desiderata may be weighted differently or even disregarded entirely; particularly when some aspects are less relevant for a specific application or domain.\nFinally, the framework is based on the works presented by other authors, as well as the metrics we identified through our systematic survey. While we consider it extensive for now, it is possible to expand it horizontally in the future. Depending on the requirements, further desiderata may be added. We do not rigorously map each metric to a single desideratum, but instead list several desiderata it contributes to. Therefore, our proposed categorization can be readily extended with further desiderata.",
      "keywords": [
        "metrics",
        "efficiency",
        "categorization",
        "explananda",
        "explanation",
        "data",
        "metric",
        "systematic"
      ],
      "match_type": "near_paragraph_global",
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": 0.463081419467926
    },
    {
      "index": 84,
      "paragraph": "5.1.1 Overview\nTo facilitate the selection of evaluation metrics for testing explanations, we propose a categorization scheme that groups the identified metrics into functionally similar approaches. Serving both as a conceptual overview and a practical guide, it is structured along three orthogonal dimensions:\n- (i) the desiderata each metric contributes to,\n- (iii) the suitable explanation types the metric applies to.\n- (ii) the degree of dependency on the model and data ( contextuality ), and\nDesiderata: One contribution of this survey is to introduce an extensive and unifying framework for desiderata in the context of VXAI (see Section 3 for details). We apply this framework to all identified metrics. Specifically, for each metric we annotate to which of the seven desiderata it contributes: Parsimony, Plausibility, Coverage, Fidelity, Consistency, Continuity, and Efficiency. Since some metrics relate to more than one desideratum, we do not enforce an exclusive one-to-one mapping from metric to desiderata.",
      "keywords": [
        "explanations",
        "contextuality",
        "metrics",
        "evaluation",
        "explanation",
        "categorization",
        "annotate",
        "plausibility"
      ],
      "match_type": "near_paragraph_global",
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": 0.5349841713905334
    },
    {
      "index": 89,
      "paragraph": "5.1.2 Explanation Types\nWe differentiate between the following explanation types: Feature Attributions , Concepts Exam-, ples , White-Box Surrogate Models , and Natural Language . We semi-formally introduce each of the individual types. This choice also applies to our formulation of explanation types. Explanations are commonly categorized into their scope (Zhang et al., 2021; Speith, 2022; Bedi et al., 2024), where local explanations consider a single prediction and global explanations aim to elucidate the overall behavior of a black-box model. We include both local and global scope under the same explanation type.",
      "keywords": [
        "explanations",
        "explanation",
        "attributions",
        "prediction",
        "elucidate",
        "feature",
        "concepts",
        "model"
      ],
      "match_type": "near_paragraph_global",
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": 0.5349805951118469
    },
    {
      "index": 91,
      "paragraph": "5.1.2 Explanation Types\nbe global, e.g., showing the global impact of features (Lundberg, 2017; Molnar, 2020).\nConcept Explanations (CEs): Similar to FAs, CEs can also be seen as a vector of importance scores. However, they are conceptually different as they represent a higher-level idea, above individual features, e.g., visual patterns rather than individual pixels. Therefore, there are usually fewer concepts\n7 From latin for 'in place' and 'off-site', respectively.\nthan input features. Further, concepts are usually extracted from some intermediary representation or embedding inside the black-box model. While low-level features in FA are usually only meaningful for a given input (e.g., pixel), concepts are meaningful on their own, and the same concept can be, and usually is, present in multiple inputs. Hence, CE strives a middle ground between local and global explanations, where the concepts themselves and their average contribution to prediction is a global explanation, while the detection of concepts in a single input is a local explanation (Kim et al., 2018).",
      "keywords": [
        "concepts",
        "explanations",
        "explanation",
        "concept",
        "conceptually",
        "features",
        "visual",
        "embedding"
      ],
      "match_type": "near_paragraph_global",
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": 0.4539703130722046
    },
    {
      "index": 92,
      "paragraph": "5.1.2 Explanation Types\nExample Explanations (ExEs): ExEs are located in the input space, and therefore may consist of any type, including tabular data, images, graphs, and more. While the most prominent type of ExEs are Counterfactuals (minimally altered inputs that lead to a different prediction (Wachter et al., 2017)), other types such as Prototypes (typical examples representing a class (Kim et al., 2016)) or 'Factuals' (altered instances that lead to the same prediction (Dhurandhar et al., 2019)) also belong to this category. ExEs may consist of a single explaining instance or can be a list of instances. Local ExE are common in the form of Counterfactuals, showing what would need to change to alter the individual prediction of a model to a desired outcome (Wachter et al., 2017; Karimi et al., 2020; Mothilal et al., 2020; Verma et al., 2024). Conversely, a global ExE could be a list of class prototypes or influential training samples (Kim et al., 2016; Koh & Liang, 2017; Molnar,",
      "keywords": [
        "explanations",
        "explanation",
        "explaining",
        "instances",
        "example",
        "types",
        "examples",
        "prediction"
      ],
      "match_type": "near_paragraph_global",
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": 0.4697636067867279
    },
    {
      "index": 93,
      "paragraph": "5.1.2 Explanation Types\n2020).\nWhite-Box Surrogates (WBSs): WBSs aim to approximate the underlying black-box model. They achieve this through a surrogate model which is considered interpretable itself and therefore can serve as the explanans. This includes reconstructions of the full black-box model over the entire data space (Craven & Shavlik, 1995; Friedman & Popescu, 2008) but also local surrogates, which only serve as explanans for a given subset of instances (Ribeiro et al., 2016; 2018). Common types of inherently interpretable WBS are trees, rule-sets, or linear models.",
      "keywords": [
        "interpretable",
        "surrogates",
        "wbs",
        "wbss",
        "surrogate",
        "explanans",
        "explanation",
        "white"
      ],
      "match_type": "near_paragraph_keywords",
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": 0.6543529629707336
    },
    {
      "index": 94,
      "paragraph": "5.1.2 Explanation Types\nNatural Language Explanations (NLEs): NLEs have been explored well before the rise of Large Language Models (LLMs), particularly through joint training setups that generate textual justifications alongside model predictions (Ras et al., 2022). However, the growing capabilities of LLMs have made NLEs increasingly prominent. Leveraging such models, the explanans may be generated alongside the prediction (Camburu et al., 2018; Wei et al., 2022) or post-hoc, potentially using a separate model (Bills et al., 2023). A more classical approach are template-based NLEs, where predefined building blocks are selected based on the results of other explanation methods (e.g., FAs) (Lucieri et al., 2022; Das et al., 2023). However, since these template-based approaches merely wrap an existing explanans in textual form, we do not consider them to be genuine NLEs. Instead, we propose to evaluate the underlying explanantia and explanations directly.",
      "keywords": [
        "justifications",
        "explanations",
        "explanans",
        "explanantia",
        "explanation",
        "textual",
        "language",
        "predictions"
      ],
      "match_type": "near_paragraph_global",
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": 0.4868192970752716
    },
    {
      "index": 95,
      "paragraph": "5.1.2 Explanation Types\nNotably, similar to the formulation of desiderata, our categorization scheme based on explanation types can be extended horizontally. We note that there is an overlap between different categories, as they represent both the final given explanans and the explanation process. LIME (Ribeiro et al., 2016) is a typical example, as its explanation fits a local WBS to generate a corresponding FA explanans. Similarly, there is a connection between CEs and other types, as WBSs can be leveraged to generate counterfactuals (Pornprasit et al., 2021), as can FAs (Ge et al., 2021; Albini et al., 2022). Rather than being a limitation, this overlap benefits our framework: it enables metrics designed for one explanation type to be applicable to others, facilitating broader reuse and comparison.",
      "keywords": [
        "explanans",
        "categorization",
        "categories",
        "explanation",
        "types",
        "broader",
        "formulation",
        "type"
      ],
      "match_type": "near_paragraph_global",
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": 0.5442603826522827
    },
    {
      "index": 113,
      "paragraph": "5.2 Identified Metrics\nMetric Popularity and References: On average, each metric is supported by 13 4 references . (standard deviation 22 5), . though the median is only 5. While a few metrics are backed by large reference sets (e.g., 119, 75, and 51 citations), eight metrics are supported by just one. This should not be interpreted as lack of relevance: our focus was on original proposals, not reuse or popularity. The most cited metric, Ground-Truth Dataset Evaluation, reflects the ubiquity of using annotated or synthetic datasets for evaluating explanation quality; an intuitive strategy with virtually unlimited implementation variants.\nDesiderata: Most metrics target Fidelity , with 18 doing so directly and 3 more partially. Plausibility follows with 12 metrics. By contrast, Efficiency , Coverage , and Consistency are least represented, with only 1, 2, and 2 metrics respectively. The majority of metrics (32) address a single desideratum, while 4 target two equally and 5 contribute partially to a secondary one.",
      "keywords": [
        "metrics",
        "datasets",
        "annotated",
        "evaluation",
        "quality",
        "intuitive",
        "citations",
        "dataset"
      ],
      "match_type": "near_paragraph_global",
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": 0.4691944718360901
    },
    {
      "index": 115,
      "paragraph": "5.2 Identified Metrics\nContextuality: Most metrics fall into the less restrictive categories. Specifically, 13 are classified as Contextuality I (Explanans-Centric) and another 13 as Contextuality II (Model Observation), both of which do not require altering model or input. As contextual demands increase, metric availability declines: 7 fall into Contextuality III (Input Intervention), and 6 into Contextuality IV (Model Intervention). The most restrictive category, Contextuality V (A Priori Constrained), includes only two metrics that require specific setups with known ground-truth rationales, either via synthetic data or interpretable white-box models.",
      "keywords": [
        "contextuality",
        "contextual",
        "metrics",
        "metric",
        "interpretable",
        "categories",
        "observation",
        "centric"
      ],
      "match_type": "near_paragraph_keywords",
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": 0.6543529629707336
    },
    {
      "index": 116,
      "paragraph": "5.2 Identified Metrics\nDesideratum-Contextuality Interactions: Metrics targeting the interpretability desiderata Parsimony and Plausibility are found almost exclusively in Contextualities I and II. Similarly, the technical desiderata Coverage , Consistency , and Efficiency are addressed only through observational strategies belonging to the same Contextualities of I and II. In contrast, all metrics measuring Continuity require intervention, either through modified inputs (Contextuality III) or altered model internals (Contextuality IV). Fidelity , in turn, is represented across all contextuality levels.\nContextuality-Explanation Interactions: There are no striking anomalies in how metrics available for specific explanation types are distributed across contextuality levels. The only exception is that no metric in Contextuality V targets WBS, likely because it is difficult to define a single ground-truth surrogate when multiple equivalent surrogates may exist.",
      "keywords": [
        "interpretability",
        "contextuality",
        "contextualities",
        "observational",
        "interactions",
        "consistency",
        "contrast",
        "explanation"
      ],
      "match_type": "synonym",
      "matched_terms": [
        "interpretability"
      ],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 117,
      "paragraph": "5.2 Identified Metrics\nDesideratum-Explanation Interactions: Every explanation type is covered by at least one metric for every desideratum, ensuring that all evaluation dimensions are, in principle, addressable regardless of the explanation form. The overall distribution across types is balanced. The only notable concentration occurs for Plausibility in ExE, with 10 applicable metrics. This may reflect the inherently interpretable nature of many ExE techniques (e.g., counterfactuals, prototypes), which naturally lend themselves to human-aligned plausibility assessments.\nThese patterns highlight the breadth of our framework. Overall, the proposed categorization encompasses a wide range of metrics across explanation types, desiderata, and evaluation contexts, offering a comprehensive foundation for structured VXAI assessment.",
      "keywords": [
        "interpretable",
        "plausibility",
        "evaluation",
        "explanation",
        "assessment",
        "assessments",
        "interactions",
        "categorization"
      ],
      "match_type": "near_paragraph_global",
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": 0.5368714928627014
    },
    {
      "index": 119,
      "paragraph": "6 Discussion\nFinally, we reflect on trends observed in the identified VXAI metrics and offer considerations for their interpretation and future application.\nGeneral observations: Our framework reveals a wide and diverse landscape of VXAI metrics. While each aims to quantify a specific property of explanations, few metrics offer clear thresholds or benchmarks to determine what constitutes 'good' explanation quality. This lack of consensus limits interpretability and comparability across studies. Furthermore, although several metrics address similar desiderata, explanation types, or contextuality levels, each typically serves a distinct niche. These differences often stem from variations in what aspect of a desideratum is targeted, or from adapting to specific explanation structures.",
      "keywords": [
        "interpretability",
        "explanations",
        "contextuality",
        "explanation",
        "interpretation",
        "vxai",
        "quantify",
        "aspect"
      ],
      "match_type": "synonym",
      "matched_terms": [
        "interpretability"
      ],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 120,
      "paragraph": "6 Discussion\nEmphasis on Fidelity: Fidelity stands out as the most frequently addressed desideratum, both in terms of metric count and methodological variety. This supports our perspective of Fidelity as a fundamental aspect of explanation quality (see Subsection 3.3). However, the range of proposed metrics also demonstrates that there is no universally accepted approach to quantifying it. The same holds for the desiderata Parsimony, Plausibility, and Continuity, each offering a range of possible evaluation metrics. In contrast, Coverage, Consistency, and Efficiency are addressed by fewer metrics. This is likely due to their more straightforward quantifiability; for instance, Coverage is trivially satisfied if explanantia exists, and Efficiency can be assessed via computation time. Portability, a part of Efficiency, is similarly important, but we identified no functionality-grounded metrics that are associated with it. Hence, it has to be viewed as a descriptive property of the method rather than something measurable on individual explanantia. More broadly, the number of metrics associated with a desideratum does not necessarily reflect how well it can be quantified. Some desiderata may be more reliably approximated by a single",
      "keywords": [
        "quantifying",
        "methodological",
        "metrics",
        "functionality",
        "reliably",
        "evaluation",
        "computation",
        "plausibility"
      ],
      "match_type": "near_paragraph_global",
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": 0.4803992807865143
    },
    {
      "index": 122,
      "paragraph": "V A Priori Constrained\nFigure 6: Overview of the 41 identified metrics, grouped by contextuality. Each metric is represented by a horizontal bar indicating the number of supporting references. Metrics are mapped to their associated desiderata, with arrow colors denoting the corresponding explanations type. These associations do not distinguish between full ( ✓ ) and partial ( ✓ ) alignment (unlike Table 2). For each desideratum, a pie chart summarizes the distribution of linked metrics by explanation type.\nmetrics. Nonetheless, our framework is flexible enough to incorporate emerging metrics in this and further areas.\nTransferability across explanation types: A notable pattern is that many metrics are rarely applied beyond one or two explanation types. In our analysis, several were marked as 'potentially applicable' to additional types, but lacked evidence of actual transfer. This suggests an isolated view of explanation types in current research. We view this as a missed opportunity, as generalizing and adapting metrics across different explanation types could foster a more integrated evaluation practice.",
      "keywords": [
        "contextuality",
        "explanations",
        "explanation",
        "summarizes",
        "references",
        "metrics",
        "generalizing",
        "evaluation"
      ],
      "match_type": "near_paragraph_global",
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": 0.5244593620300293
    },
    {
      "index": 123,
      "paragraph": "V A Priori Constrained\nMetrics embedded in the explanation process: Some explanation types, notably ExE and WBS, commonly embed evaluation metrics as part of their generation objectives. For example, counterfactuals often optimize for plausibility and proximity directly during their construction (Wachter et al., 2017; Dandl et al., 2020; Kanamori et al., 2020). While this tight integration may enhance the generated explanans, it raises concerns about circularity in evaluation, especially in light of Goodhart's Law (Strathern, 1997): If a metric is optimized for, it may no longer serve as a reliable post-hoc assessment.\nSubjectivity and the limits of automation: Metrics for Parsimony and Plausibility, while quantifiable, are ultimately shaped by subjective human interpretation. Functionality-grounded scores offer useful proxies that can be evaluated automatically and without costly human studies. However, they cannot replace human-centered assessments of whether an explanation is truly accessible or helpful in practice.",
      "keywords": [
        "explanans",
        "evaluation",
        "plausibility",
        "functionality",
        "assessment",
        "explanation",
        "assessments",
        "counterfactuals"
      ],
      "match_type": "near_paragraph_global",
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": 0.5541425943374634
    },
    {
      "index": 124,
      "paragraph": "V A Priori Constrained\nCaveats in specific metric designs: Several metric designs warrant closer scrutiny. Groundtruth-based metrics, specifically those relying on human-annotated rationales (see Metric 40), are widely used but have notable limitations. These include their tendency to reflect plausibility rather than explanation fidelity (Camburu et al., 2019; Carmichael & Scheirer, 2023), vulnerability to trivial baselines like central focus (Gu et al., 2019), and their inability to capture model reliance on context or background (Arras et al., 2022; Brandt et al., 2023). There's no reason to assume that the human rationale and the models' rationale are aligned (Khakzar et al., 2022), resulting in low scores for truthful explanations generated from an implausible model (Samek & Müller, 2019).",
      "keywords": [
        "rationale",
        "rationales",
        "reason",
        "metrics",
        "plausibility",
        "explanations",
        "scrutiny",
        "context"
      ],
      "match_type": "near_paragraph_global",
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": 0.5166614055633545
    },
    {
      "index": 126,
      "paragraph": "V A Priori Constrained\nEvaluating the use of post-hoc explanations: These limitations prompt reflection on whether post-hoc explanation of black-box models is always justified. Some authors recommend comparing black-box performance to that of directly trained white-box models (Zhang et al., 2019b; Rosenfeld, 2021; Margot & Luta, 2021). If performance is comparable, choosing an interpretable model may be preferable, especially in high-stakes settings, as advocated by Rudin (2019). The same logic applies when considering surrogate models. They should be benchmarked against white-box models trained directly on the data (Barakat et al., 2010).",
      "keywords": [
        "explanations",
        "interpretable",
        "priori",
        "explanation",
        "hoc",
        "data",
        "model",
        "models"
      ],
      "match_type": "near_paragraph_keywords",
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": 0.6543529629707336
    },
    {
      "index": 127,
      "paragraph": "Conclusion\nIn this survey, we introduced a unified and comprehensive framework for the evaluation of XAI. We conducted a systematic review of the literature, identifying original metrics designed to assess\nexplanation quality. These were grouped into 41 aggregated metrics and categorized using a threedimensional scheme based on desideratum , explanation type , and contextuality .\nOur analysis reveals a broad range of available metrics, covering diverse use cases and goals. At the same time, it highlights the lack of consensus regarding when and how specific metrics should be applied. Moreover, we find that many existing metrics are not yet adapted for all explanation types, indicating potential for extension and generalization.\nOverall, our framework enables XAI practitioners to systematically analyze and compare explanations through an informed selection of suitable and comprehensive evaluation strategies. It also offers the possibility for extension, both by adding new metrics and by expanding the categorization scheme as the field evolves.",
      "keywords": [
        "explanations",
        "contextuality",
        "explanation",
        "xai",
        "evaluation",
        "indicating",
        "quality",
        "categorization"
      ],
      "match_type": "near_paragraph_global",
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": 0.48448699712753296
    },
    {
      "index": 128,
      "paragraph": "Limitations and Future Work\nThis work focuses exclusively on surveying and categorizing existing metrics. Although we provide structured comparisons and insights, we do not conduct empirical benchmarking. Future work should investigate how different metrics behave in practice, under which conditions they agree or contradict, and what trade-offs they impose. This would support the development of a more standardized benchmarking suite for explanation evaluation.\nEstablishing practical thresholds, studying alignment between metrics and desiderata, and exploring aggregation strategies remain open and important questions. Ideally, this could lead to a broadly accepted set of evaluation standards and protocols for verifying explanations across different models and tasks.\nLastly, while many metrics are labeled by us as potentially applicable to additional explanation types, their use remains unvalidated in literature. Future work should adapt and extend these metrics to underexplored explanation types, particularly for natural language and concept-based explanations.",
      "keywords": [
        "explanations",
        "explanation",
        "evaluation",
        "categorizing",
        "metrics",
        "insights",
        "benchmarking",
        "aggregation"
      ],
      "match_type": "near_paragraph_global",
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": 0.5618553161621094
    },
    {
      "index": 130,
      "paragraph": "Scopus\n```\nTITLE-ABS-KEY ( ( \" Explain∗ AI\" OR \" Explain∗ Artif i c i a l I n t e l l i g e n c e \" OR \" Explain∗ ML\" OR \" Explain∗ Machine Learning \" OR \" I n t e r p r e t ∗ AI\" OR \" Interpret ∗ Artif i c i a l I n t e l l i g e n c e \" OR \" Interpret ∗ ML\" OR \" Interpret ∗ Machine Learning \") AND ( \" Evaluation \" OR \" Metric \" OR \" Quantification \") AND ( \" Survey \" OR \"Review \") )\n```",
      "keywords": [
        "scopus",
        "title",
        "evaluation",
        "ai",
        "quantification",
        "interpret",
        "review",
        "ml"
      ],
      "match_type": "synonym",
      "matched_terms": [
        "interpret"
      ],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 153,
      "paragraph": "B.2.3 Similarity Measures\ni Camburu et al. (2018); Chuang et al. (2018); Liu et al. (2018a); Wu & Mooney (2018); Chen et al. (2019d); Rajani et al. (2019); Wickramanayake et al. (2019); Li et al. (2020a); Sun et al. (2020); Jang & Lukasiewicz (2021); Atanasova (2024b)\nbeen proposed specifically for evaluating natural language explanations (Xie et al., 2021; Du et al., 2022; Rodis et al., 2024; Park et al., 2018).\nWhen comparing similarities over multiple instances, the most natural aggregation is to compute the mean similarity (Fan et al., 2020; Fouladgar et al., 2022; Yeh et al., 2019). Depending on the evaluation goal, alternative aggregation strategies may offer more informative insights. For example, worst-case stability, defined as the minimum similarity across inputs, can be used to quantify robustness [j] .",
      "keywords": [
        "similarity",
        "explanations",
        "similarities",
        "comparing",
        "aggregation",
        "evaluation",
        "robustness",
        "compute"
      ],
      "match_type": "near_paragraph_global",
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": 0.45231878757476807
    },
    {
      "index": 159,
      "paragraph": "Desiderata: Parsimony Explanation Type: FA, ExE, WBS, (CE, NLE)\nThe size of an explanans | e | is a common indicator of its complexity. Smaller or more compact explanantia are generally easier to understand and more plausible to human users. The exact method to measure size depends on the explanation type and context.\nFor WBS , size is typically measured via structural properties of the surrogate model: treebased models are assessed by depth, number of nodes, or number of leaves [ a ] , while rule-based systems are evaluated using the number of rules or predicates per rule [ b ] . For explanationgraphs, the number of nodes and edges can serve as a proxy for size (Rustamov & Klosowski, 2018; Topin & Veloso, 2019). Conversely, the (relative) number of instances covered per rule can also express parsimony (Deng, 2019; Guidotti et al., 2019).",
      "keywords": [
        "explanationgraphs",
        "explanans",
        "complexity",
        "treebased",
        "explanantia",
        "predicates",
        "parsimony",
        "explanation"
      ],
      "match_type": "near_paragraph_global",
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": 0.49400073289871216
    },
    {
      "index": 166,
      "paragraph": "Metric I.2: Overlap\nParsimony\nDesiderata:\nExplanation Type: WBS\nReferences: (5)\nLakkaraju et al. (2016; 2017; 2019); Moradi & Samwald (2021); Hosain et al. (2024)\nThe overlap within a rule-based explanans (e.g., rule sets or decision trees) can be measured with respect to either the input space or the rules themselves. A lower degree of overlap is generally preferred, as it implies more distinct, non-redundant rules and enhances interpretability.\n- · Rule Overlap : Measures how many rules share identical or highly similar predicates (Moradi & Samwald, 2021), indicating structural redundancy within the rule set.\n- · Input Overlap : Measures how often input instances are covered by multiple rules or decision paths (Lakkaraju et al., 2016; 2017; 2019; Hosain et al., 2024). High overlap may indicate redundant or conflicting logic. Overlap can also be broken down by class label to distinguish intra-class from inter-class coverage.",
      "keywords": [
        "overlap",
        "interpretability",
        "parsimony",
        "predicates",
        "logic",
        "redundant",
        "explanans",
        "redundancy"
      ],
      "match_type": "synonym",
      "matched_terms": [
        "interpretability"
      ],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 167,
      "paragraph": "Metric I.3: Explanans Cohesion\nDesiderata: Parsimony, Plausibility Explanation Type: FA,(CE)\nReferences: (2)\nFong & Vedaldi (2017); Saifullah et al. (2024)\nIn domains where input features are ordered (e.g., images or time series, as opposed to tabular data), we expect relevant information to be located in a smooth and coherent region of the input. This property can be evaluated in two complementary ways.\nattribution map is often easier to understand. A higher score indicates a more fragmented, less interpretable explanans.",
      "keywords": [
        "cohesion",
        "coherent",
        "interpretable",
        "features",
        "explanans",
        "fragmented",
        "information",
        "data"
      ],
      "match_type": "near_paragraph_keywords",
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": 0.6543529629707336
    },
    {
      "index": 182,
      "paragraph": "Metric I.8: Input Contrastivity\nDesiderata: Plausibility, (Fidelity)\nExplanation Type:\nFA, (ExE, CE, WBS, NLE)\nReferences: (2)\nHonegger (2018); Pornprasit et al. (2021)\nExplanantia should be specific to their corresponding explananda and not overly generic. That is, distinct inputs should typically yield distinct explanantia.\nWhile this metric has been primarily proposed for FAs, it may be applicable to other explanation types as well.\nTo assess this, we calculate the fraction of instances or instance pairs that result in different explanantia (Honegger, 2018; Pornprasit et al., 2021). A higher fraction of distinguishable explanantia indicates greater contrastivity and, by extension, higher plausibility.",
      "keywords": [
        "contrastivity",
        "explanantia",
        "explananda",
        "plausibility",
        "explanation",
        "indicates",
        "distinguishable",
        "type"
      ],
      "match_type": "near_paragraph_global",
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": 0.5267770886421204
    },
    {
      "index": 185,
      "paragraph": "Metric I.10: Model-Agnostic Explanation Consistency\nDesiderata: Plausibility, (Consitency)\nExplanation Type: FA, ExE, (CE, WBS, NLE)\nReferences: (4)\nFan et al. (2020); Nguyen et al. (2020); Hvilshøj et al. (2021); Jiang et al. (2023)\nTo assess whether explanations reflect generalizable patterns rather than model-specific artifacts (such as adversarial shortcuts in counterfactual explanations), several authors evaluate explanations across different models trained on the same dataset.\n- · Nguyen et al. (2020) use perturbation-based evaluation (e.g., Metric 28) on a secondary model to assess whether the explanation highlights features that are generally important across models.",
      "keywords": [
        "explanations",
        "adversarial",
        "explanation",
        "model",
        "generalizable",
        "models",
        "plausibility",
        "consistency"
      ],
      "match_type": "near_paragraph_global",
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": 0.5747473835945129
    },
    {
      "index": 186,
      "paragraph": "Metric I.10: Model-Agnostic Explanation Consistency\nIn general, the approach involves training additional black-box models (oracles) on the same data and task. These oracles are then used to evaluate explanations from the original model. One strategy is to directly compute the similarity between explanantia generated by different models for the same input (Fan et al., 2020). While this has been reported for FA, it is applicable to any explanation type, provided a suitable similarity metric is chosen (see Subsection B.2.3). Alternatively, explanations from the original model are evaluated using the oracles:\n- · Hvilshøj et al. (2021) propose that counterfactuals should change the prediction in both the original model and the oracle. Only such counterfactuals are considered plausible.\n- · An extension of this approach trains one oracle per class and computes the JensenShannon-Divergence between its predictions on the original and counterfactual input.\nIdeally, only the target and original class should exhibit strong divergence (Hvilshøj et al., 2021).",
      "keywords": [
        "explanations",
        "explanation",
        "prediction",
        "explanantia",
        "counterfactuals",
        "predictions",
        "model",
        "counterfactual"
      ],
      "match_type": "near_paragraph_global",
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": 0.5032972097396851
    },
    {
      "index": 187,
      "paragraph": "Metric I.10: Model-Agnostic Explanation Consistency\nThis approach might be extended to other explanation types. However, its interpretive strength remains limited: it is unclear whether explanations should be similar across models, as differing model architectures may learn distinct (yet valid) rationales. As a result, high or low agreement does not always reflect explanation quality, making this metric inherently context-dependent.\n- · Jiang et al. (2023) define a neighborhood of models via small weight perturbations and count how many counterfactuals remain valid across all neighbors.",
      "keywords": [
        "explanations",
        "interpretive",
        "rationales",
        "explanation",
        "counterfactuals",
        "consistency",
        "model",
        "architectures"
      ],
      "match_type": "near_paragraph_global",
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": 0.5884316563606262
    },
    {
      "index": 190,
      "paragraph": "Metric I.12: Output Coverage\nDesiderata: Coverage\nExplanation Type:\nWBS\nLakkaraju et al. (2016)\nReferences: (1)\nInherently interpretable WBS models should be capable of producing predictions for all possible output classes. Lakkaraju et al. (2016) evaluate the model's class-level coverage by calculating the fraction of target classes that appear in the explanans. This could, for instance, be the number of classes that are assigned to at least one rule in a rule set or to a leaf in a decision tree.",
      "keywords": [
        "coverage",
        "wbs",
        "interpretable",
        "output",
        "predictions",
        "evaluate",
        "capable",
        "metric"
      ],
      "match_type": "near_paragraph_keywords",
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": 0.6543529629707336
    },
    {
      "index": 191,
      "paragraph": "Metric I.13: Output Mutual Information\nFidelity\nDesiderata:\nExplanation Type: FA,CE\nNguyen & Martínez (2020)\nReferences: (1)\nWhen high-level features (such as concepts or aggregated input features) are provided by an explanans, they should capture as much information as possible about the model's output. In turn, the prediction should be reflected in the explanans, implying a high degree of mutual dependency between both.\nTo quantify this relationship, Nguyen & Martínez (2020) leverage the Mutual Information (MI) (see Cover (1999)) between the explanans and the output: MI ( e θ,x,y ˆ , θ ( x )). Since MI is symmetric, a high score indicates that the explanation both reflects (i.e. correctness) and encompasses (i.e., completeness) the relevant reasoning of the model.",
      "keywords": [
        "information",
        "prediction",
        "completeness",
        "output",
        "concepts",
        "indicates",
        "features",
        "explanans"
      ],
      "match_type": "near_paragraph_global",
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": 0.4522598087787628
    },
    {
      "index": 194,
      "paragraph": "Metric II.15: Output Contrastivity\nPlausibility\nDesiderata:\nExplanation Type: FA, (ExE, CE, WBS, NLE)\nReferences: (5)\nNie et al. (2018); Pope et al. (2019); Li et al. (2020c); Rebuffi et al. (2020); Sixt et al. (2020)\nTo enhance plausibility, explanations should be class-discriminative, that is, they should differ depending on the target class. In image classification, for instance, the explanantia supporting a 'car' label should highlight different regions than those supporting 'dog'. This makes it easier for humans to understand the specific rationale for each class.\nAny suitable similarity or distance measure can be used to assess the degree of overlap between explanantia, such as the L 0 or L 2 distance (Nie et al., 2018; Pope et al., 2019), rank correlation (Rebuffi et al., 2020), or SSIM for image domain (Sixt et al., 2020). A lower similarity implies a clearer distinction between the rationales for each class.",
      "keywords": [
        "contrastivity",
        "discriminative",
        "classification",
        "explanations",
        "distinction",
        "clearer",
        "similarity",
        "explanation"
      ],
      "match_type": "near_paragraph_global",
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": 0.47655320167541504
    },
    {
      "index": 201,
      "paragraph": "Metric II.18: Significance Check\nIn a more advanced variant, Hemamou et al. (2021) train a classifier to distinguish between real and synthetic (random) explanations. High accuracy in this task indicates that the generated explanantia contain statistically meaningful structure.\nFor CEs, statistical tests are typically used to assess whether the extracted concepts carry significantly more information than randomly sampled concepts. This is done by comparing average relevance or activation scores between the true and random concepts (Adel et al., 2018; Kim et al., 2018).\nWhile commonly reported for FA and CE, the general idea of statistical significance testing could, in principle, be adapted to other explanation types as well. However, doing so may require explanation-specific reformulations.",
      "keywords": [
        "statistical",
        "explanations",
        "explanantia",
        "classifier",
        "explanation",
        "concepts",
        "accuracy",
        "indicates"
      ],
      "match_type": "near_paragraph_global",
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": 0.46408945322036743
    },
    {
      "index": 203,
      "paragraph": "Metric II.19: (Counter-)Factual Relevance\nDesiderata: Fidelity\nExplanation Type: ExE\nLiu et al. (2021b)\nReferences: (1)\nThis metric applies to explanations that generate both a factual and counterfactual explanans, aiming to evaluate how well they reflect and contrast the model's reasoning.\nThe method computes the model's output for the factual and counterfactual explanantia and compares them to the original prediction using the negative symmetric Kullback-Leibler divergence. The difference between these two scores is normalized by the distance between the factual and counterfactual explanantia. A high normalized score indicates that both explanans are informative: the factual preserves the original reasoning, while the counterfactual shifts the decision appropriately.\nOriginally proposed by Liu et al. (2021b) for the graph domain, requiring both explanantia being subgraphs, it can be generalized to other input types.",
      "keywords": [
        "explanations",
        "relevance",
        "explanans",
        "informative",
        "counterfactual",
        "prediction",
        "reasoning",
        "explanation"
      ],
      "match_type": "near_paragraph_global",
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": 0.491581529378891
    },
    {
      "index": 207,
      "paragraph": "Metric II.21: Sufficiency\nDesiderata: Fidelity\nExplanation Type: CE\nYeh et al. (2020); Dasgupta et al. (2022)\nReferences: (2)\nAn explanation should be complete enough to serve as a sufficient reason for a given model output. Ideally, the explanans alone should allow us to predict the outcome. This property reflects the completeness of the explanans and can be assessed in different ways.\nAlternatively, Dasgupta et al. (2022) evaluate whether explanations are sufficient for consistent outcomes: Given an explanans e 0 for an input x 0 , we identify other instances whose explanantia are equivalent (or sufficiently similar) to e 0 and compute the fraction that shares the same model prediction as x 0 . A higher agreement indicates a more sufficient explanation.\nYeh et al. (2020) train a secondary model that maps the explanans back to the black-box's activation space. The predictive performance using the mapped explanans indicates how much information the explanation preserves about the original model's decision process.",
      "keywords": [
        "predictive",
        "explanations",
        "explanans",
        "prediction",
        "explanation",
        "predict",
        "explanantia",
        "completeness"
      ],
      "match_type": "near_paragraph_global",
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": 0.5720829963684082
    },
    {
      "index": 215,
      "paragraph": "Metric II.24: Setup Consistency\n```\nDesiderata: Consistency Explanation Type: FA, ExE, WBS, (CE, NLE) References: (11) Bastani et al. (2017); Honegger (2018); Guidotti et al. (2019); Rajapaksha et al. (2020); Warnecke et al. (2020); Amparore et al. (2021); Graziani et al. (2021); Margot & Luta (2021); Velmurugan et al. (2021b); Dai et al.\n```\n(2022); Vermeire et al. (2022)\nThis metric evaluates how consistent the explanantia remain when generated multiple times for the same input and model. This, however, is only a necessary consideration for nondeterministic explanation methods.\nAlthough most work focuses on local instance-wise explanations, the same principle applies to global explanations, where the similarity between globally constructed explanantia is measured (Bastani et al., 2017; Margot & Luta, 2021).",
      "keywords": [
        "explanations",
        "explanantia",
        "consistency",
        "explanation",
        "nondeterministic",
        "consistent",
        "model",
        "similarity"
      ],
      "match_type": "near_paragraph_global",
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": 0.551328718662262
    },
    {
      "index": 216,
      "paragraph": "Metric II.24: Setup Consistency\nSome authors assess this axiomatically, by counting the fraction of identical explanantia produced across repeated runs (Honegger, 2018; Vermeire et al., 2022). Others calculate distances or similarity scores between different runs and report the aggregated variation, using general or task-specific metrics such as the variance of feature weights or feature presence [ a ] .\nWhile we did not identify examples of this metric applied to every explanations type in the literature, the core idea is general and can easily be extended to any XAI algorithm.\na Guidotti et al. (2019); Rajapaksha et al. (2020); Warnecke et al. (2020); Amparore et al. (2021); Graziani et al. (2021); Velmurugan et al. (2021b); Dai et al. (2022)",
      "keywords": [
        "explanantia",
        "xai",
        "explanations",
        "metrics",
        "consistency",
        "axiomatically",
        "feature",
        "similarity"
      ],
      "match_type": "near_paragraph_global",
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": 0.4716204106807709
    },
    {
      "index": 244,
      "paragraph": "Metric III.29: Counterfactuability\nFidelity\nDesiderata:\nExplanation Type:\nWBS\nPornprasit et al. (2021)\nReferences: (1)\nTo assess the expressiveness and impact of rule-based explanations (either generated directly or extracted from surrogate models such as trees), we can use them to guide the generation of counterfactual perturbations. This evaluates whether the rules have predictive leverage and reflect real decision logic, rather than being purely descriptive or spurious.\nSpecifically, given an input instance that is covered by a rule, Pornprasit et al. (2021) perturb the input such that the rule conditions are violated. If the rule truly captures important rationale, breaking it should influence the black-box model's prediction. This can be quantified in two ways:\n- • By counting the number of perturbed instances that change the predicted class label.\n- · By measuring the aggregate change in predicted class probability before and after perturbation.",
      "keywords": [
        "counterfactuability",
        "predictive",
        "counterfactual",
        "prediction",
        "predicted",
        "explanations",
        "spurious",
        "perturb"
      ],
      "match_type": "near_paragraph_global",
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": 0.45243725180625916
    },
    {
      "index": 246,
      "paragraph": "Metric III.31: Quantification of Unexplainable Features\nDesiderata:\nContinuity, (Fidelity)\nExplanation Type: FA, (CE)\nReferences: (2)\nZhang et al. (2019a); Chen et al. (2022)\nTo assess the continuity of explanations, Zhang et al. (2019a) perturb the unimportant features of an input based on the explanans, and then generate a new one for the perturbed input. The similarity between the original and perturbed explanantia serves as a measure of continuity: a high similarity suggests that irrelevant changes do not affect the explanation, indicating robustness. In addition, it implies a high completeness, as relevant features must have been captured well enough to maintain the model's behavior despite noise.\nThis metric may be extended to CE by perturbing the concept layer corresponding to unimportant concepts, or by mapping concepts back to input features via concept-based localization (e.g., see Lucieri et al. (2020)).",
      "keywords": [
        "explanations",
        "features",
        "explanans",
        "robustness",
        "concepts",
        "explanantia",
        "completeness",
        "explanation"
      ],
      "match_type": "near_paragraph_global",
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": 0.5850693583488464
    },
    {
      "index": 247,
      "paragraph": "Metric III.31: Quantification of Unexplainable Features\nAspecial case of this approach is the Attack Capture Rate proposed by Chen et al. (2022), which applies to FAs in NLP (particularly rationale extraction). Here, insertion attacks introduce distractor phrases to the input, which ideally should not influence the rationale. The metric measures how often the inserted tokens appear in the extracted explanans. This variant relies on artificial attacks, which limits its generalizability beyond NLP.",
      "keywords": [
        "nlp",
        "attacks",
        "explanans",
        "features",
        "rationale",
        "attack",
        "quantification",
        "extraction"
      ],
      "match_type": "near_paragraph_global",
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": 0.48137062788009644
    },
    {
      "index": 252,
      "paragraph": "Metric III.33: Adversarial Input Resilience\nDesiderata: Continuity Explanation Type: FA, (ExE, CE, WBS, NLE) References: (10) Singh et al. (2018); Wang et al. (2018b); Chen et al. (2019b); Dombrowski et al. (2019); Ghorbani et al. (2019); Subramanya et al. (2019); Boopathy et al. (2020); Kuppa & Le-Khac (2020); Zhang et al. (2020); Huang et al. (2023a)\nExplanations can be vulnerable to Adversarial Attacks (AA), where the goal is to manipulate either the explanans (Ghorbani et al., 2019) or prediction (Singh et al., 2018; Wang et al., 2018b; Subramanya et al., 2019). More sophisticated approaches aim to manipulate one, while additionally restricting changes to the other. Two main types exist:",
      "keywords": [
        "adversarial",
        "explanations",
        "explanans",
        "explanation",
        "prediction",
        "attacks",
        "vulnerable",
        "input"
      ],
      "match_type": "near_paragraph_global",
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": 0.5147412419319153
    },
    {
      "index": 255,
      "paragraph": "Metric IV.34: Model Parameter Randomization Test\n```\nDesiderata: Fidelity Explanation Type: FA, (ExE, CE, WBS, NLE) References: (5) Adebayo et al. (2018); Kindermans et al. (2019); Binder et al. (2023); Bommer et al. (2024); Hedström et al. (2024)\n```\nTo verify that explanations are truly reflective of the black-box model's learned reasoning, the model's internal parameters are systematically randomized, and the resulting changes in explanantia are analyzed. The rationale is that if an explanation remains unchanged under randomization, it is likely generic and not informative of the model's decision logic (Adebayo et al., 2018).\nFor gradient-based methods, Sixt et al. (2020) propose inserting a random activation vector at a specific layer, to break the causal connection without modifying the network weights.",
      "keywords": [
        "randomization",
        "explanations",
        "randomized",
        "explanation",
        "random",
        "gradient",
        "explanantia",
        "layer"
      ],
      "match_type": "near_paragraph_global",
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": 0.491717129945755
    },
    {
      "index": 258,
      "paragraph": "Metric IV.35: Data Randomization Test\nDesiderata: Fidelity Explanation Type: FA, (ExE, CE, WBS, NLE) References: (2) Adebayo et al. (2018); Sanchez-Lengeling et al. (2020)\nExplanations should highlight meaningful structures in the data, not artifacts of memorization. To verify this, the training data labels are randomized, forcing the model to fit noise rather than learn semantically relevant features (Adebayo et al., 2018). Label randomization can be applied to the full training set (Adebayo et al., 2018) or a subset only (Sanchez-Lengeling et al., 2020).\nAlthough only reported for FAs, this approach can be extended to any explanation type, provided that a suitable similarity measure between explanantia is available.",
      "keywords": [
        "randomization",
        "randomized",
        "memorization",
        "explanations",
        "explanation",
        "labels",
        "data",
        "label"
      ],
      "match_type": "near_paragraph_global",
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": 0.48050013184547424
    },
    {
      "index": 266,
      "paragraph": "Metric IV.39: Adversarial Model Resilience\nContinuity\nDesiderata:\nExplanation Type:\nFA, (ExE, CE, WBS, NLE)\nReferences: (4)\nHeo et al. (2019); Pruthi et al. (2019); Viering et al. (2019); Dimanov et al. (2020)\nExplanation methods should depend meaningfully on the internal parameters of the black-box model. However, this sensitivity can be exploited to adversarially manipulate them. Specifically, small but carefully chosen changes to the model's weights can alter the generated explanations without significantly changing predictions. This manipulation can serve to obfuscate undesirable behaviors or bias within a model.\nTo assess the robustness of explanation methods against such attacks, various strategies perturb the model parameters and observe the resulting changes in explanantia. These attacks can be:",
      "keywords": [
        "adversarial",
        "adversarially",
        "robustness",
        "explanations",
        "explanation",
        "explanantia",
        "obfuscate",
        "model"
      ],
      "match_type": "near_paragraph_global",
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": 0.5231978297233582
    },
    {
      "index": 276,
      "paragraph": "Metric V.40: GT Dataset Evaluation\n(2022); Wilming et al. (2022); Zhou et al. (2022); Agarwal et al. (2023); Hesse et al. (2023); Miró-Nicolau et al. (2023); Sun et al. (2023); Ya et al. (2023); Rao et al. (2024)\nTo evaluate the quality of explanations, many authors propose comparing them to dataset-based ground truths. This strategy can follow two distinct paradigms, depending on how the ground truth is derived. If the rationale is uniquely defined through the data generation process, we may evaluate the explanation's Fidelity . If, however, the ground truth stems from human judgment or heuristic annotation, we evaluate the Plausibility of the explanation.",
      "keywords": [
        "dataset",
        "explanations",
        "data",
        "rationale",
        "annotation",
        "explanation",
        "plausibility",
        "truth"
      ],
      "match_type": "near_paragraph_global",
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": 0.561954140663147
    },
    {
      "index": 279,
      "paragraph": "Evaluation :\nHuman-Annotated Datasets instead provide plausible rationales grounded in human intuition or derived from proxy labels. In some cases, annotators are explicitly asked to justify their decisions or to highlight which features they consider relevant for a particular prediction (Chen et al., 2019c; Xu et al., 2020b; Cui et al., 2022; Tritscher et al., 2023). In other settings, rationales are implicit, where existing annotations such as segmentation maps, bounding boxes, or other metadata are repurposed to approximate human reasoning [ h ] . These datasets are typically used to evaluate the plausibility of explanations rather than their fidelity, as the ground-truth provided reflects human expectations rather than the actual decision-making logic of the model.\nAcross both paradigms, the central assumption is that accurate model predictions imply align-",
      "keywords": [
        "annotators",
        "annotations",
        "annotated",
        "explanations",
        "datasets",
        "intuition",
        "justify",
        "reasoning"
      ],
      "match_type": "synonym",
      "matched_terms": [
        "justify"
      ],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 289,
      "paragraph": "Metric V.41: White Box Model Check\nIn white-box models, the internal logic is fully accessible and interpretable, providing a groundtruth rationale against which generated explanantia can be directly evaluated. Typical whitebox models used for comparison include linear regressors (Crabbe et al., 2020; Dai et al., 2022), feature-additive models (Carmichael & Scheirer, 2023), small neural networks with manually set parameters (Antwarg et al., 2019; Brandt et al., 2023), or symbolic models such as decision trees (Ribeiro et al., 2016; Dhurandhar et al., 2019).\nFurther, if the white-box model relies only on a constrained feature subset, one can also measure explanation fidelity through accuracy, precision, or recall between explanans and the truly influential features (Ribeiro et al., 2016; Zhou et al., 2019; Jia et al., 2020; Velmurugan et al., 2021a).",
      "keywords": [
        "whitebox",
        "explanans",
        "interpretable",
        "features",
        "neural",
        "explanantia",
        "regressors",
        "feature"
      ],
      "match_type": "near_paragraph_keywords",
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": 0.6543529629707336
    },
    {
      "index": 290,
      "paragraph": "Metric V.41: White Box Model Check\nGiven that the model's reasoning is known, explanation methods can be assessed by comparing their output against this ground-truth explanans. This can be done using general similarity metrics (Guidotti, 2021; Jia et al., 2019) or using error metrics like MSE (Crabbe et al., 2020; Dai et al., 2022; Brandt et al., 2023).\nAlthough originally proposed for FAs, this approach may be extended to ExEs and NLEs, as the white-box model enables verification of whether the provided explanantia are consistent with the known reasoning. While user inspection is straightforward, adapting this check into an automatic, functionality-grounded evaluation remains challenging but potentially feasible.",
      "keywords": [
        "explanans",
        "metrics",
        "explanantia",
        "reasoning",
        "explanation",
        "functionality",
        "evaluation",
        "comparing"
      ],
      "match_type": "near_paragraph_global",
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": 0.4897242486476898
    },
    {
      "index": 292,
      "paragraph": "C Excluded Metrics\n- · Alignment : Etmann et al. (2019) assess plausibility by measuring how well a saliency map aligns with the original input, using correlation as a proxy. However, we argue that input reconstruction does not necessarily enhance interpretability, as simply returning the input or an edge map offers little explanatory value. Furthermore, the metric mixes explanation with input similarity, rewarding explanation that may lack selectivity or meaningful abstraction. Therefore, we exclude this metric due to its flawed underlying assumption.\n- · Feature Diversity : Smyth & Keane (2022) calculate the fraction of different features altered across all counterfactuals in a dataset, where low diversity implies the same features are consistently changed. However, this appears to be a purely statistical property of the explanation, and it remains unclear why higher or lower diversity should indicate better explanation quality. Without a clear link to any desideratum, its practical value as an evaluation metric is doubtful.",
      "keywords": [
        "interpretability",
        "metrics",
        "saliency",
        "features",
        "abstraction",
        "diversity",
        "feature",
        "dataset"
      ],
      "match_type": "synonym",
      "matched_terms": [
        "interpretability"
      ],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 293,
      "paragraph": "C Excluded Metrics\n- · Explanatory Power Arras et al. (2017) evaluate the model-explanation pair by computing k -nearest-neighbor accuracy on document vectors constructed from word-attribution explanation. While the score reflects how semantically useful these explanations are for related tasks, it measures downstream utility rather than explanation quality. As such, it evaluates a combination of model and explanation performance, not the fidelity (or other desideratum) of the explanation itself. Further, it is not applicable beyond text domain or in scenarios without downstream tasks.\n- · Mutual Information : Le et al. (2020) measure how independent the features changed in counterfactuals are, with the goal of encouraging maximal expressiveness. It uses Symmetrical Uncertainty (Press, 2007) to quantify pairwise dependencies among changed features. However, enforcing independence between altered features can easily lead to implausible counterfactuals; especially when high correlations reflect natural dependencies. Breaking these can result in unrealistic or invalid inputs.",
      "keywords": [
        "metrics",
        "counterfactuals",
        "explanations",
        "explanatory",
        "semantically",
        "features",
        "accuracy",
        "information"
      ],
      "match_type": "near_paragraph_global",
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": 0.45221829414367676
    },
    {
      "index": 294,
      "paragraph": "References\nSalim I Amoukou, Tangi Salaün, and Nicolas Brunel. Accurate shapley values for explaining tree-based\n2024.\nHimabindu Lakkaraju,\nStephen H Bach, and Jure Leskovec.\nInterpretable decision\nsets:\nA joint\nDong Nguyen. Comparing automatic and human evaluation of local explanations for text classification.\nWilliam H Press. Numerical recipes 3rd edition: The art of scientific computing . Cambridge university press, 2007.\nKai Shu, Limeng Cui, Suhang Wang, Dongwon Lee, and Huan Liu.\ndefend:\nExplainable fake news",
      "keywords": [
        "interpretable",
        "explainable",
        "explanations",
        "classification",
        "scientific",
        "shapley",
        "evaluation",
        "based"
      ],
      "match_type": "near_paragraph_global",
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": 0.5895784497261047
    }
  ],
  "unmatched": [
    {
      "index": 4,
      "paragraph": "1 Introduction\nDas & Rad, 2020; Markus et al., 2021; Rawal et al., 2021; Agarwal et al., 2022b).",
      "keywords": [
        "das",
        "rawal",
        "rad",
        "2022b",
        "2021",
        "2020",
        "agarwal",
        "markus"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 5,
      "paragraph": "1 Introduction\nHowever, XAI is not a silver bullet. Van der Waa et al. (2021) point out, that humans tend to trust predictions more readily when an explanation is provided, often without carefully examining the explanation itself. This lack of critical scrutiny can lead to unwarranted trust, especially when decisions are taken based on incorrect or misleading explanations (Eiband et al., 2019; Jesus et al., 2021). To make matters worse, different XAI algorithms may result in conflicting explanations for the same model and sample (Krishna et al., 2022). Therefore, simply providing any explanation is not sufficient, but it is important to assess the quality of the explanation at hand (Sovrano et al., 2021). Unfortunately, while there is a plethora of XAI methods, evaluation of explanations is still an immature research area (Ribera & Lapedriza, 2019), with many studies relying on the notion of a good explanation as 'You'll know it when you see it', providing anecdotal evidence (i.e. small-scale qualitative validation) (Doshi-Velez & Kim, 2017; Nauta et al., 2023; Saarela &",
      "keywords": [
        "xai",
        "predictions",
        "explanations",
        "explanation",
        "relying",
        "validation",
        "evaluation",
        "qualitative"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 6,
      "paragraph": "1 Introduction\nPodgorelec, 2024). Especially in computer vision tasks, evaluation through qualitative inspection of a few examples can be appealing (Ibrahim & Shafiq, 2023). However, unstructured qualitative examination leads to highly subjective results, as humans struggle at judging the value of XAI explanations (Adebayo et al., 2018; Buçinca et al., 2020; Hase & Bansal, 2020). In addition, such evaluations risk cherry-picking favorable examples and offer no reliable foundation for comparing different explanation methods across studies or practitioners. For the same given explanation, human ratings vary depending on both the task itself (Franklin & Lagnado, 2022) and the participant's cultural background (Peters & Carman, 2024). Evaluation is further complicated by the lack of ground-truth for the explanations, as this would require knowledge about the model's internal reasoning process (Samek et al., 2019; Markus et al., 2021; Samek et al., 2021; Bommer et al., 2024; Ortigossa et al., 2024).",
      "keywords": [
        "explanations",
        "evaluations",
        "evaluation",
        "qualitative",
        "xai",
        "vision",
        "judging",
        "explanation"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 8,
      "paragraph": "1 Introduction\nOne of the most prevalent taxonomies reported in the literature (Vilone & Longo, 2021; Zhou et al., 2021; Elkhawaga et al., 2023), and illustrated in Figure 1, is the distinction proposed by Doshi-Velez & Kim (2017) between human-grounded and functionality-grounded evaluation methods. The former includes qualitative and quantitative evaluations by laypeople and experts, while the latter consists of (semi-)automatic metrics.",
      "keywords": [
        "functionality",
        "evaluations",
        "evaluation",
        "metrics",
        "qualitative",
        "methods",
        "taxonomies",
        "quantitative"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 10,
      "paragraph": "1 Introduction\nEffort and Cost\nFigure 1: The classification of XAI evaluation into human-grounded and functionality-grounded evaluation, adapted from the classification framework by Doshi-Velez & Kim (2017) and its visualization by Zhou et al. (2021).\nFigure 2: The heatmaps show two alternative example explanantia , indicating which input regions 1 were deemed decisive for the model's decision (the explanandum ). 1 A qualitative inspection allows for multiple interpretations, as it is unclear whether a) both the model and the explanation process (explanation ) are correct or flawed (top), or b) one is correct and the other failed (bottom). 1\nboth cases, the consequences can be severe, either reducing trust in a well-functioning model or, more critically, reinforcing trust in a flawed one. Further, human evaluation, especially through the system's developers, is prone to confirmation bias (Doshi-Velez & Kim, 2017; Lipton, 2018)",
      "keywords": [
        "xai",
        "evaluation",
        "visualization",
        "qualitative",
        "functionality",
        "explanantia",
        "inspection",
        "classification"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 11,
      "paragraph": "1 Introduction\nThere exist a number of surveys and guidelines that address human-centered evaluations (Hoffman et al., 2018; Miller, 2019; Chromik & Schuessler, 2020; Holzinger et al., 2020; Franklin & Lagnado, 2022; Hsiao et al., 2021; Jesus et al., 2021; Langer et al., 2021; Mohseni et al., 2021; van der Waa et al., 2021; Silva et al., 2023). Unfortunately, the range of reviews dedicated to functionality-grounded evaluation is still limited. This is despite the advantage of offering objective, quantitative metrics without requiring human experiments, which can save both time and cost (Doshi-Velez & Kim, 2017; Samek et al., 2019; Zhou et al., 2021). Most existing studies are narrow and restricted to a specific application domain (Giuste et al., 2022; Arreche et al., 2024), including cybersecurity (Pawlicki et al., 2024), medical image classification (Patrício et al., 2023; Chaddad et al., 2024), electronic health record data",
      "keywords": [
        "evaluations",
        "evaluation",
        "functionality",
        "classification",
        "metrics",
        "cybersecurity",
        "data",
        "objective"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 12,
      "paragraph": "1 Introduction\n(Payrovnaziri et al., 2020), data and knowledge engineering (Li et al., 2020b), or timeseries classification (Theissler et al., 2022). Others focus on particular XAI approaches, such as visual explanations in CNNs (Mohamed et al., 2022) or instance-based explanations (Bayrak & Bach, 2024). Moreover, many surveys dedicate only limited attention to evaluation metrics, with their primary focus placed on the XAI methods themselves (Carvalho et al., 2019; Ding et al., 2022; Minh et al., 2022; Mohamed et al., 2022; Ali et al., 2023; Clement et al., 2023; Patrício et al., 2023; Chaddad et al., 2024; Gongane et al., 2024; Xua & Yang, 2024). By contrast, this review focuses exclusively on functionality-grounded evaluation across domains and is applicable to a wide range of XAI approaches.",
      "keywords": [
        "timeseries",
        "xai",
        "cnns",
        "classification",
        "evaluation",
        "explanations",
        "knowledge",
        "data"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 14,
      "paragraph": "Contributions\n- · We perform a systematic literature review based on the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines by Page et al. (2021), identifying 362 relevant publications that introduce or utilize evaluation metrics.\n- · We propose a three-dimensional categorization scheme consisting of desiderata, explanation type, and evaluation contextuality, and use it to organize the identified metrics.\n- · We aggregate these into 41 functionally similar metric groups, capturing common methodological patterns across the literature.\n- · To our knowledge, this results in the most comprehensive and unified VXAI framework to date and provides an extensible foundation for future research.",
      "keywords": [
        "evaluation",
        "systematic",
        "reviews",
        "vxai",
        "methodological",
        "metrics",
        "research",
        "analyses"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 15,
      "paragraph": "Contributions\nThe remainder of this review is structured as follows: In Section 2, we first present related studies on the topic of VXAI to motivate the need for this systematic review, before introducing the desiderata of XAI in Section 3. Further, Section 4 introduces our research method used to search for and identify relevant metrics. The categorization scheme and results are presented in Section 5, where we introduce our categorization framework (Subsection 5.1) and summarize the identified metrics (Subsection 5.2), complemented by a visual overview in Figure 6. A deeper discussion of these findings is provided in Section 6, while comprehensive descriptions of the metrics alongside references are listed in Appendix B. We conclude the review in Section 7, discussing the results and future paths for the area of VXAI.",
      "keywords": [
        "vxai",
        "xai",
        "categorization",
        "metrics",
        "visual",
        "summarize",
        "systematic",
        "overview"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 16,
      "paragraph": "Terminology\nTo avoid ambiguous language, throughout the paper we stick to the terminology of the XAI Handbook by Palacio et al. (2021): The goal of XAI is to facilitate understanding by providing insights into an explanandum ('What is to be explained'), usually a model or a model's decision. To accomplish this, we leverage an explanation , which is the process of getting insight into the explanandum. The resulting output of this process is the explanans , which provides the user with information about the model's inner workings. In a mathematical sense, the explanation can be viewed as a function that maps an explanandum to an explanans. For example, the explanandum could be a CNN's classification of a given input image. The explanation might be an algorithm such as GradCAM (Selvaraju et al., 2017), and the resulting heatmap is the explanans, which highlights important features. We use the Latin plural forms explananda (explanandum) and explanantia (explanans) throughout. When we refer to VXAI, we include both the evaluation of the",
      "keywords": [
        "xai",
        "explanans",
        "explananda",
        "explanandum",
        "explanantia",
        "explanation",
        "vxai",
        "classification"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 18,
      "paragraph": "2 Related Work\nAlthough the field of XAI has gained popularity over the past years, there is still no extensive and unified evaluation framework for XAI metrics. Various surveys have explored XAI and VXAI from different angles, ranging from human-grounded evaluation to technical metrics. Table 1 gives an overview over 30 such XAI reviews from the past years.",
      "keywords": [
        "xai",
        "vxai",
        "evaluation",
        "metrics",
        "ranging",
        "technical",
        "reviews",
        "framework"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 19,
      "paragraph": "2 Related Work\nWhile evaluation of XAI is frequently given less attention in XAI surveys, 23 of these reviews directly focus on the topic of VXAI. Besides functionality-grounded evaluation, a second school of thought is concerned with human-grounded evaluation of explanations through qualitative expert evaluations or quantitative user studies, with representative surveys for this domain available as well (Sokol & Flach, 2020; Rawal et al., 2021; Naveed et al., 2024). Nevertheless, a considerable number of 19 reports focus specifically on the topic of functionality-grounded evaluation. Unfortunately, most of these surveys focus on a subset of well-known metrics, whereas only 14 surveys gathered VXAI metrics in a systematic or semi-systematic literature review. Further, numerous of the referenced reviews either lack an extensive list of desiderata and focus only on a subset of them, or limit their research to specific types of explanations 2 or application domains.",
      "keywords": [
        "xai",
        "evaluations",
        "evaluation",
        "explanations",
        "vxai",
        "functionality",
        "qualitative",
        "attention"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 20,
      "paragraph": "2 Related Work\nThere are five reviews, that we consider most similar to this work, as they present systematic functionality-grounded VXAI surveys. Le et al. (2023) and Nauta et al. (2023) both categorize the identified metrics based on a scheme of 12 properties, namely the Co-12 framework, which we discuss in more detail in Section 3. However, Le et al. (2023) restrict their analysis to metrics available through public XAI or VXAI toolkits (e.g., Quantus (Hedström et al., 2023)) and do not report on metrics introduced in the literature but not implemented in such libraries. Although there is some overlap with the study by Nauta et al. (2023), particularly in the inclusion of some identical metrics, their review also incorporates studies that merely apply VXAI metrics rather than introducing them. In contrast, our work provides detailed descriptions and categorizations of each identified metric. The review by Kadir et al. (2023) covers a broad range of domains and explanation types 2 and reports a wide variety of metrics. It also groups several metrics by method, a strategy shared by",
      "keywords": [
        "vxai",
        "functionality",
        "metrics",
        "xai",
        "toolkits",
        "framework",
        "properties",
        "implemented"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 21,
      "paragraph": "2 Related Work\nour work. However, it does not adopt a categorization scheme based on the desiderata fulfilled by individual metrics. Notably, the recent reviews from Bayrak & Bach (2024) and Pawlicki et al. (2024) report a high number of individual metrics for VXAI. However, both limit the scope of their review considerably, either in terms of application domain (Pawlicki et al., 2024) or explanation type 2 (Bayrak & Bach, 2024). In contrast, our work includes all metrics reported to date and introduces a categorization scheme based on explicitly defined desiderata (see Section 3). Finally, many of the metrics we identified were introduced only recently, underlining the need for this more recent literature review.\nWhile previous reviews report between 10 and 90 individual metrics, our work introduces a unified structure by aggregating over 360 individual metrics into 41 conceptually related groups. This enables clearer comparison and interpretation across metrics. Unlike most surveys, we do not limit our analysis to specific explanation types or application domains, ensuring broader applicability across the XAI landscape.",
      "keywords": [
        "xai",
        "vxai",
        "explanation",
        "categorization",
        "metrics",
        "conceptually",
        "clearer",
        "contrast"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 23,
      "paragraph": "3 Desiderata of XAI\nThis work, Primary VXAI Focus = ✓. This work, Primarily Functionality- Grounded VXAI = ✓. This work, (Semi-)Systematic Review = ✓. This work, Date ↓ = Jan 2025. This work, Desiderata = Parsimony, Plausibility, Coverage, Fidelity, Continuity, Consistency, Effi- ciency. This work, Limited to = . This work, Reported Metrics = 41 metrics from 362 sources. Klein et al., Primary VXAI Focus = ✓. Klein et al., Primarily Functionality- Grounded VXAI = ✓. Klein et al., (Semi-)Systematic Review = . Klein et al., Date ↓ = Jan 2025. Klein et al., Desiderata = Faithfulness, Robustness, Complexity. Klein et al., Limited to = Feature Attributions; Computer Vision. Klein et al., Reported Metrics = 20 metrics. Pawlicki et al., Primary VXAI Focus = ✓. Pawlicki et al., Primarily Functionality- Grounded VXAI = ✓. Pawlicki et al., (Semi-)Systematic Review = ✓. Pawlicki et",
      "keywords": [
        "xai",
        "vxai",
        "vision",
        "feature",
        "robustness",
        "systematic",
        "functionality",
        "metrics"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 24,
      "paragraph": "3 Desiderata of XAI\nal., Date ↓ = Oct 2024. Pawlicki et al., Desiderata = . Pawlicki et al., Limited to = Cybersecurity. Pawlicki et al., Reported Metrics = 86 metrics. Awal & Roy, Primary VXAI Focus = ✓. Awal & Roy, Primarily Functionality- Grounded VXAI = ✓. Awal & Roy, (Semi-)Systematic Review = . Awal & Roy, Date ↓ = Jun 2024. Awal & Roy, Desiderata = Reliability, Consistency. Awal & Roy, Limited to = Rule Explanations. Awal & Roy, Reported Metrics = 6 metrics. Bayrak & Bach, Primary VXAI Focus = ✓. Bayrak & Bach, Primarily Functionality- Grounded VXAI = ✓. Bayrak & Bach, (Semi-)Systematic Review = ✓. Bayrak & Bach, Date ↓ = Apr 2024. Bayrak & Bach, Desiderata = . Bayrak & Bach, Limited to = Counterfactuals. Bayrak & Bach, Reported Metrics = 66 metrics. Bommer et al., Primary VXAI Focus = ✓. Bommer",
      "keywords": [
        "cybersecurity",
        "reliability",
        "vxai",
        "functionality",
        "metrics",
        "systematic",
        "xai",
        "explanations"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 25,
      "paragraph": "3 Desiderata of XAI\net al., Primarily Functionality- Grounded VXAI = ✓. Bommer et al., (Semi-)Systematic Review = . Bommer et al., Date ↓ = Mar 2024. Bommer et al., Desiderata = Robustness, Faithfulness, Complexity, Lo- calization, Randomization. Bommer et al., Limited to = Climate Science. Bommer et al., Reported Metrics = 10 metrics. Li et al., Primary VXAI Focus = ✓. Li et al., Primarily Functionality- Grounded VXAI = ✓. Li et al., (Semi-)Systematic Review = . Li et al., Date ↓ = Dec 2023. Li et al., Desiderata = Faithfulness. Li et al., Limited to = Feature Attributions. Li et al., Reported Metrics = 6 metrics. Alangari et al., Primary VXAI Focus = ✓. Alangari et al., Primarily Functionality- Grounded VXAI = . Alangari et al., (Semi-)Systematic Review = . Alangari et al., Date ↓ = Aug 2023. Alangari et",
      "keywords": [
        "climate",
        "robustness",
        "science",
        "vxai",
        "feature",
        "randomization",
        "xai",
        "functionality"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 26,
      "paragraph": "3 Desiderata of XAI\nal., Desiderata = Correctness, Comprehensibility, Stability. Alangari et al., Limited to = . Alangari et al., Reported Metrics = 59 metrics. Le et al., Primary VXAI Focus = ✓. Le et al., Primarily Functionality- Grounded VXAI = ✓. Le et al., (Semi-)Systematic Review = ✓. Le et al., Date ↓ = Aug 2023. Le et al., Desiderata = Co-12 †. Le et al., Limited to = . Le et al., Reported Metrics = 86 metrics from 17 toolkits. Salih et al., Primary VXAI Focus = ✓. Salih et al., Primarily Functionality- Grounded VXAI = . Salih et al., (Semi-)Systematic Review = ✓. Salih et al., Date ↓ = Aug 2023. Salih et al., Desiderata = . Salih et al., Limited to = Cardiology. Salih et al., Reported Metrics = 27 metrics. Kadir et al., Primary VXAI Focus = ✓.",
      "keywords": [
        "cardiology",
        "vxai",
        "toolkits",
        "metrics",
        "systematic",
        "functionality",
        "xai",
        "desiderata"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 27,
      "paragraph": "3 Desiderata of XAI\nKadir et al., Primarily Functionality- Grounded VXAI = ✓. Kadir et al., (Semi-)Systematic Review = ✓. Kadir et al., Date ↓ = Jul 2023. Kadir et al., Desiderata = . Kadir et al., Limited to = . Kadir et al., Reported Metrics = 80 metrics. Hedström et al., Primary VXAI Focus = ✓. Hedström et al., Primarily Functionality- Grounded VXAI = ✓. Hedström et al., (Semi-)Systematic Review = . Hedström et al., Date ↓ = Apr 2023. Hedström et al., Desiderata = Faithfulness, Robustness, Localization, Complexity, Axiomatic, Randomization. Hedström et al., Limited to = Feature Attribution. Hedström et al., Reported Metrics = 27 metrics. Schwalbe & Finzel, Primary VXAI Focus = . Schwalbe & Finzel, Primarily Functionality- Grounded VXAI = . Schwalbe & Finzel,",
      "keywords": [
        "xai",
        "vxai",
        "feature",
        "robustness",
        "functionality",
        "randomization",
        "localization",
        "axiomatic"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 28,
      "paragraph": "3 Desiderata of XAI\n(Semi-)Systematic Review = ✓. Schwalbe & Finzel, Date ↓ = Jan 2023. Schwalbe & Finzel, Desiderata = . Schwalbe & Finzel, Limited to = . Schwalbe & Finzel, Reported Metrics = 11 metrics (already grouped). Agarwal et al., Primary VXAI Focus = ✓. Agarwal et al., Primarily Functionality- Grounded VXAI = ✓. Agarwal et al., (Semi-)Systematic Review = . Agarwal et al., Date ↓ = Nov 2022. Agarwal et al., Desiderata = Faithfulness, Stability. Agarwal et al., Limited to = Feature Attributions. Agarwal et al., Reported Metrics = 11 metrics. Coroama & Groza, Primary VXAI Focus = ✓. Coroama & Groza, Primarily Functionality- Grounded VXAI = . Coroama & Groza, (Semi-)Systematic Review = . Coroama & Groza, Date ↓ = Nov 2022. Coroama & Groza, Desiderata = .",
      "keywords": [
        "xai",
        "vxai",
        "functionality",
        "feature",
        "systematic",
        "metrics",
        "stability",
        "desiderata"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 29,
      "paragraph": "3 Desiderata of XAI\nCoroama & Groza, Limited to = . Coroama & Groza, Reported Metrics = 26 metrics. Verma et al., Primary VXAI Focus = . Verma et al., Primarily Functionality- Grounded VXAI = ✓. Verma et al., (Semi-)Systematic Review = . Verma et al., Date ↓ = Nov 2022. Verma et al., Desiderata = . Verma et al., Limited to = Counterfactuals. Verma et al., Reported Metrics = 9 metrics (already grouped). Belaid et al., Primary VXAI Focus = ✓. Belaid et al., Primarily Functionality- Grounded VXAI = ✓. Belaid et al., (Semi-)Systematic Review = . Belaid et al., Date ↓ = Oct 2022. Belaid et al., Desiderata = Fidelity, Fragility, Stability, Simplicity, Stress, Other. Belaid et al., Limited to = Feature Attributions. Belaid et al., Reported Metrics = 22 metrics. Cugny et al., Primary VXAI Focus =",
      "keywords": [
        "vxai",
        "feature",
        "functionality",
        "xai",
        "fragility",
        "systematic",
        "metrics",
        "stability"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 30,
      "paragraph": "3 Desiderata of XAI\n✓. Cugny et al., Primarily Functionality- Grounded VXAI = ✓. Cugny et al., (Semi-)Systematic Review = . Cugny et al., Date ↓ = Oct 2022. Cugny et al., Desiderata = . Cugny et al., Limited to = . Cugny et al., Reported Metrics = 6 metrics. Lopes et al., Primary VXAI Focus = ✓. Lopes et al., Primarily Functionality- Grounded VXAI = . Lopes et al., (Semi-)Systematic Review = ✓. Lopes et al., Date ↓ = Aug 2022. Lopes et al., Desiderata = Fidelity (Completeness, Soundness), Inter- pretability, Broadness, Simplicity, Clarity). Lopes et al., Limited to = . Lopes et al., Reported Metrics = 43 metrics. Yuan et al., Primary VXAI Focus = . Yuan et al., Primarily Functionality- Grounded VXAI = ✓. Yuan et al., (Semi-)Systematic Review = ✓. Yuan et al., Date ↓ = Jul 2022. Yuan et",
      "keywords": [
        "xai",
        "vxai",
        "systematic",
        "functionality",
        "soundness",
        "desiderata",
        "clarity",
        "review"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 31,
      "paragraph": "3 Desiderata of XAI\nal., Desiderata = Fidelity, Sparsity, Stability, Accuracy. Yuan et al., Limited to = Graph Neural Networks. Yuan et al., Reported Metrics = 7 metrics. Löfström et al., Primary VXAI Focus = ✓. Löfström et al., Primarily Functionality- Grounded VXAI = . Löfström et al., (Semi-)Systematic Review = ✓. Löfström et al., Date ↓ = Mar 2022. Löfström et al., Desiderata = . Löfström et al., Limited to = . Löfström et al., Reported Metrics = 10 metrics. Vilone & Longo, Primary VXAI Focus = ✓. Vilone & Longo, Primarily Functionality- Grounded VXAI = . Vilone & Longo, (Semi-)Systematic Review = ✓. Vilone & Longo, Date ↓ = Dec 2021. Vilone & Longo, Desiderata = . Vilone & Longo, Limited to = . Vilone & Longo, Reported Metrics = 36 metrics. Bodria et al., Primary VXAI Focus = . Bodria",
      "keywords": [
        "neural",
        "graph",
        "accuracy",
        "sparsity",
        "vxai",
        "networks",
        "xai",
        "metrics"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 32,
      "paragraph": "3 Desiderata of XAI\net al., Primarily Functionality- Grounded VXAI = ✓. Bodria et al., (Semi-)Systematic Review = ✓. Bodria et al., Date ↓ = Nov 2021. Bodria et al., Desiderata = . Bodria et al., Limited to = . Bodria et al., Reported Metrics = 6 metrics. Sovrano et al., Primary VXAI Focus = ✓. Sovrano et al., Primarily Functionality- Grounded VXAI = . Sovrano et al., (Semi-)Systematic Review = . Sovrano et al., Date ↓ = Oct 2021. Sovrano et al., Desiderata = Similarity, Exactness, Fruitfulness. Sovrano et al., Limited to = . Sovrano et al., Reported Metrics = 22 metrics. Ras et al., Primary VXAI Focus = . Ras et al., Primarily Functionality- Grounded VXAI = . Ras et al., (Semi-)Systematic Review = . Ras et al., Date ↓ = Sep 2021. Ras et al., Desiderata = . Ras et",
      "keywords": [
        "fruitfulness",
        "vxai",
        "desiderata",
        "systematic",
        "similarity",
        "xai",
        "review",
        "functionality"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 33,
      "paragraph": "3 Desiderata of XAI\nal., Limited to = . Ras et al., Reported Metrics = 13 sources in text (metrics not listed). Mohseni et al., Primary VXAI Focus = . Mohseni et al., Primarily Functionality- Grounded VXAI = . Mohseni et al., (Semi-)Systematic Review = ✓. Mohseni et al., Date ↓ = Aug 2021. Mohseni et al., Desiderata = Fidelity, Trustworthiness. Mohseni et al., Limited to = . Mohseni et al., Reported Metrics = 15 metrics. Yeh & Ravikumar, Primary VXAI Focus = ✓. Yeh & Ravikumar, Primarily Functionality- Grounded VXAI = ✓. Yeh & Ravikumar, (Semi-)Systematic Review = . Yeh & Ravikumar, Date ↓ = Jun 2021. Yeh & Ravikumar, Desiderata = . Yeh & Ravikumar, Limited to = . Yeh & Ravikumar, Reported Metrics = 7 metrics. Nauta et al., Primary VXAI Focus = ✓. Nauta et al., Primarily Functionality- Grounded VXAI =",
      "keywords": [
        "vxai",
        "systematic",
        "functionality",
        "fidelity",
        "trustworthiness",
        "metrics",
        "xai",
        "sources"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 34,
      "paragraph": "3 Desiderata of XAI\n✓. Nauta et al., (Semi-)Systematic Review = ✓. Nauta et al., Date ↓ = May 2021. Nauta et al., Desiderata = Co-12 † Fidelity (Completeness, Soundness), Inter-. Nauta et al., Limited to = . Nauta et al., Reported Metrics = 28 metrics (already grouped). Zhou et al., Primary VXAI Focus = ✓. Zhou et al., Primarily Functionality- Grounded VXAI = ✓. Zhou et al., (Semi-)Systematic Review = . Zhou et al., Date ↓ = Jan 2021. Zhou et al., Desiderata = pretability, Broadness, Simplicity, Clarity). Zhou et al., Limited to = . Zhou et al., Reported Metrics = 17 metrics. Samek & Müller, Primary VXAI Focus = . Samek & Müller, Primarily Functionality- Grounded VXAI = . Samek & Müller, (Semi-)Systematic Review = . Samek & Müller, Date ↓ = Sep 2019. Samek & Müller, Desiderata = . Samek & Müller, Limited to = . Samek",
      "keywords": [
        "xai",
        "vxai",
        "systematic",
        "functionality",
        "soundness",
        "review",
        "broadness",
        "desiderata"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 35,
      "paragraph": "3 Desiderata of XAI\n& Müller, Reported Metrics = 16 sources in text (metrics not listed) 40 sources in text. Yang et al., Primary VXAI Focus = ✓. Yang et al., Primarily Functionality- Grounded VXAI = ✓. Yang et al., (Semi-)Systematic Review = ✓. Yang et al., Date ↓ = Aug 2019. Yang et al., Desiderata = Generalizability, Fidelity, Persuasibility. Yang et al., Limited to = . Yang et al., Reported Metrics = (metrics not listed)\n† Co-12: Correctness, Output-Completeness, Consistency, Continuity, Contrastivity, Covariate Complexity, Compactness, Composition, Confidence, Context, Coherence, Controllability",
      "keywords": [
        "coherence",
        "functionality",
        "vxai",
        "generalizability",
        "contrastivity",
        "persuasibility",
        "complexity",
        "systematic"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 36,
      "paragraph": "3 Desiderata of XAI\nTable 1: Overview of recent XAI reviews, sorted by date. The table indicates whether each survey primarily focused on evaluation metrics, whether it reported mainly functionality-grounded metrics, and whether a (semi-)structured review was conducted. The date refers to the earliest available point in the article's timeline; either the database query, submission, or publication, depending on what was reported. For each survey, we also report the desiderata used to classify the metrics and any limitations regarding explanation type or application domain. Note that not all surveys systematically listed their assessed metrics, so the reported metric count may vary depending on the method of extraction.\nof both stages of the explanation process. In this section, we first provide an overview of existing desiderata proposed in prior work. We then introduce a unified framework that systematically describes the requirements for ensuring technical soundness and for bridging the interpretation gap.",
      "keywords": [
        "xai",
        "metrics",
        "functionality",
        "evaluation",
        "overview",
        "explanation",
        "describes",
        "soundness"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 37,
      "paragraph": "3.1 Common Formulation of Desiderata\nSeveral XAI surveys report that there is no ubiquitous consensus on appropriate desiderata, with some of the categories related to goals pursued through XAI, rather than standalone desiderata of XAI, e.g., Trustworthiness, Acceptance, or Fairness (Doshi-Velez & Kim, 2017; Langer et al., 2021; Vilone & Longo, 2021; Elkhawaga et al., 2023). Hence, we conduct a scoping review, reporting the main desiderata used by different authors and analyzing the similarities as well as differences in their formulations. For the sake of brevity, we exclude some of the papers listed in Table 1, as the missing ones either overlap considerably (e.g. Awal & Roy (2024) and Klein et al. (2024)), rely on a different notion of desiderata (e.g., Sovrano et al. (2021)), or use no desiderata at all. We will first present these frameworks using the authors' original terminology before introducing our own categorization scheme.",
      "keywords": [
        "acceptance",
        "trustworthiness",
        "xai",
        "fairness",
        "desiderata",
        "notion",
        "scheme",
        "conduct"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 38,
      "paragraph": "3.1 Common Formulation of Desiderata\nThe famous Co-12 properties, introduced by Nauta et al. (2023) and reused by Le et al. (2023), constitute one of the most extensive existing frameworks for categorizing XAI metrics. They group the properties along three different dimensions: Content ( Correctness, Completeness, Consistency, Continuity, Contrastivity, Covariate Complexity ), Presentation ( Compactness, Composition, Confidence ), and User ( Context, Coherence, Controllability ). While the first dimension focuses on the information contained in the explanans, the second and third dimensions address the way the information is conveyed. Although some of these human-centered properties can be measured through proxies, others may mainly be evaluated through human-grounded evaluation.",
      "keywords": [
        "coherence",
        "xai",
        "contrastivity",
        "metrics",
        "evaluation",
        "complexity",
        "correctness",
        "information"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 44,
      "paragraph": "3.1 Common Formulation of Desiderata\nWhile many existing frameworks overlap conceptually, a unified and practically usable categorization scheme for VXAI metrics is still lacking. This requires a structured set of desiderata that defines what makes a good explanation and supports consistent classification of metrics. Prior work often enforces a rigid one-to-one mapping between metrics and desiderata; in contrast, we decouple these dimensions, defining a set of mostly independent desiderata to which each metric may contribute individually or jointly. Lightweight frameworks tend to omit critical aspects of explanation quality, while broader ones sometimes include goals that are not intrinsic to the explanation itself (e.g., accuracy). We restrict our scope to properties that reflect the explanation rather than the underlying model and clarify excluded cases after presenting our set. Although all desiderata rely on proxies, we limit ourselves to properties that are quantifiable in principle. Highly abstract or vague notions lacking empirical grounding are omitted. Lastly, our framework is designed to be extensible, allowing the integration of future desiderata as the field evolves.",
      "keywords": [
        "metrics",
        "vxai",
        "classification",
        "categorization",
        "conceptually",
        "accuracy",
        "explanation",
        "contrast"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 56,
      "paragraph": "3.2.4 Fidelity\nThe explanation should make the explanans reflect the model's true reasoning.\nFidelity is one of the most frequently discussed concepts in the literature and combines two closely related aspects: Correctness and Completeness. While some works introduce these as separate desiderata, others group them together under the umbrella of Fidelity.",
      "keywords": [
        "fidelity",
        "completeness",
        "concepts",
        "correctness",
        "aspects",
        "reasoning",
        "model",
        "explanation"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 59,
      "paragraph": "3.2.4 Fidelity\nAlthough it is theoretically possible to have an explanation that is partially correct but incomplete (e.g., providing a heatmap that highlights only one of several relevant features), or complete but partially incorrect (e.g., including all the right features alongside irrelevant ones), neither scenario is desirable. If key features are missing or irrelevant ones are included, the explanans ultimately misrepresents the model's behavior. While Correctness and Completeness can be distinguished conceptually, they are tightly interwoven in practice and difficult to evaluate in isolation. Since our desiderata are intended to capture orthogonal evaluation dimensions, and these two cannot be meaningfully disentangled, we combine them under the unified criterion of Fidelity.",
      "keywords": [
        "features",
        "heatmap",
        "completeness",
        "conceptually",
        "misrepresents",
        "fidelity",
        "model",
        "evaluation"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 65,
      "paragraph": "3.2.7 Efficiency\nSecondly, the Algorithmic Complexity (Robnik-Šikonja & Bohanec, 2018; Carvalho et al., 2019; Molnar, 2020) considers the time it takes to generate an explanans. Naturally, the amount of necessary time depends not only on the inherent complexity of the explanation algorithm, but also on the Scalability, i.e. its ability to efficiently handle larger models and input spaces (Johansson et al., 2004). Using the 'Stress test', Belaid et al. (2022) explicitly evaluate the runtime behavior with respect to increasing input size.\nWe subsume both of these aspects under a general desideratum called Efficiency. It includes the algorithmic or computational properties of the explanation, which might influence the choice of a specific XAI algorithm over another.",
      "keywords": [
        "explanans",
        "complexity",
        "xai",
        "explanation",
        "computational",
        "algorithmic",
        "scalability",
        "efficiency"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 66,
      "paragraph": "3.2.8 Excluded Desiderata\nSeveral of the desiderata introduced in the referenced frameworks were not included in our catalog. We will briefly present each of these and justify why they were excluded.\nIn the Co-12 properties for XAI, Nauta et al. (2023) introduce Controllability , which we exclude for two reasons. First, it measures the extent to which a user can interact with the explanans, which can be relevant in given applications, but is a property of the presentation and not necessarily the explanans itself. Second, the only measurement they provide is through user interaction, which we want to avoid in this study, as we focus on functionality-grounded metrics. Their Composition property is not measurable either, as it describes the explanans' presentation format. Similarly, Expressive Power (Robnik-Šikonja & Bohanec, 2018; Carvalho et al., 2019; Molnar, 2020) describes the format of the explanans and is therefore not included.",
      "keywords": [
        "frameworks",
        "functionality",
        "controllability",
        "xai",
        "properties",
        "explanans",
        "interaction",
        "expressive"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 67,
      "paragraph": "3.2.8 Excluded Desiderata\nThe Context property (Nauta et al., 2023) describes how relevant a given explanans is to a user. This is mostly covered in the Plausibility desideratum, although their definition is more focused on the needs of specific users and hence is very subjective and not measurable through proxies.",
      "keywords": [
        "context",
        "desideratum",
        "desiderata",
        "plausibility",
        "property",
        "excluded",
        "explanans",
        "measurable"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 69,
      "paragraph": "3.2.8 Excluded Desiderata\nAs discussed in Subsection 3.2.6, we do not include some of the aspects of Consistency as defined by Robnik-Šikonja & Bohanec (2018), Carvalho et al. (2019), and Molnar (2020), who expect several models to generate similar explanantia. This is rooted in the assumption that different models should all follow the same reasoning, which we reject. In contrast, Anders et al. (2020) showed that two models exhibiting the same external behavior, such as predictions, can have radically different inner workings.",
      "keywords": [
        "consistency",
        "predictions",
        "explanantia",
        "models",
        "assumption",
        "reasoning",
        "aspects",
        "behavior"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 71,
      "paragraph": "3.2.8 Excluded Desiderata\nLastly, the Quantus toolkit (Hedström et al., 2023; Bommer et al., 2024) includes a category labeled as Axiomatic , which groups metrics based on their functionality. Specifically, it refers to whether they test for formal properties. This categorization reflects the underlying mechanism of the metric rather than a specific quality criterion. Accordingly, we reassign these metrics to the corresponding desiderata they evaluate.",
      "keywords": [
        "axiomatic",
        "quantus",
        "metrics",
        "criterion",
        "desiderata",
        "metric",
        "excluded",
        "functionality"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 72,
      "paragraph": "3.3 Considerations\nWe want to end the presentation of our desiderata and the resulting classification scheme with a few final considerations.\nFirst, all seven of our desiderata are introduced as independent dimensions. However, in practice, they can not always be separated entirely, which is reflected in some metrics being associated with multiple desiderata. Especially, where a single criterion in other authors' frameworks covers multiple of our desiderata, an inherent connection is given. This can be seen especially in Consistency and Continuity, which both measure some sort of robustness. Conversely, some of our desiderata subsume multiple sub-aspects themselves. This applies especially to Fidelity, which is comprised of Correctness and Completeness.",
      "keywords": [
        "classification",
        "robustness",
        "metrics",
        "desiderata",
        "aspects",
        "completeness",
        "criterion",
        "correctness"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 76,
      "paragraph": "4 Method\nIn our review, we aim to systematically collect and classify all functionality-grounded metrics relevant to evaluating explanations in the context of XAI. We base our review on the PRISMA (Page et al., 2021) guidelines to make the process transparent. In this section, we describe the entire process used to identify VXAI metrics from the literature. Figure 4 gives an overview of our process.",
      "keywords": [
        "xai",
        "vxai",
        "explanations",
        "functionality",
        "metrics",
        "evaluating",
        "context",
        "overview"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 77,
      "paragraph": "Preliminary Consideration\nThe research was preceded by two key observations for the design of the study: First, searching for general XAI terms is not feasible, as the database is intractable (searching Google Scholar for 'XAI' gives over 200 thousand results). Second, VXAI metrics are usually introduced alongside an XAI method rather than in a dedicated publication, and there exists no unified vocabulary, making it hard to identify relevant sources from keyword searches in titles or abstracts alone. Hence, we decided to split our research into two stages. First, we performed a database search using a limited set of search terms to build an initial body of potentially relevant sources. In the second phase, we expanded this\nFigure 4: Our search strategy building upon the PRSIMA guidelines (Page et al., 2021). Notably, we split the process into an initial database search phase and a snowballing phase, identifying the most relevant literature in the second phase.\nset through recursive backward snowballing by reviewing the reference lists of the already identified publications.",
      "keywords": [
        "xai",
        "vxai",
        "publications",
        "research",
        "publication",
        "abstracts",
        "searches",
        "metrics"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 78,
      "paragraph": "Stage 1: Initial Search\nWe started with a database meta-search using Scopus 3 , IEEE 4 and ACM 5 , to identify existing reviews and surveys that point towards further evaluation metrics. Using the advanced search features of each database, we designed research queries based on the following key terms:\n```\n[ Explain∗ | I n t e r p r e t ∗ ] × 6 [ AI | Artif i c i a l I n t e l l i g e n c e | ML | Machine Learning ] AND [ Evaluation | Metric | Quantification ] AND [ Survey | Review ]\n```\nThe exact search string for every database is given in Appendix A. The last search was conducted on January 15, 2025. This resulted in a total of 673 identified articles after de-duplication. We first screened titles and abstracts, excluding sources that were not research articles, did not involve XAI or VXAI, or focused on human-based evaluation. Through this first screening, we excluded 504 articles, using the following exclusion criteria:",
      "keywords": [
        "scopus",
        "abstracts",
        "research",
        "ai",
        "articles",
        "ieee",
        "evaluation",
        "reviews"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 79,
      "paragraph": "Stage 1: Initial Search\n- • ER-1: General Issues\n- • ER-1b: Not english\n- • ER-1a: Not a research article\n- · ER-2: No XAI (or VXAI) at all, or with explainability outside our scope (e.g. ante-hoc data analysis)\n- • ER-4: Contains systematic VXAI, but only human-centered methods\n- • ER-3: Contains XAI, but no systematic evaluation of XAI (e.g. 'anecdotal evidence')\nOut of the 169 articles that passed the initial title and abstract screening, 9 could not be retrieved. For the remaining 160 articles, we conducted full-text screening, applying the exclusion criteria described above. To avoid an intractable number of sources with limited added value, we additionally excluded works that used functionality-grounded VXAI metrics introduced by other articles, introducing the following exclusion criterion:\n- • ER-5: Contains functionality-grounded evaluation, but does not introduce a new metric itself.",
      "keywords": [
        "articles",
        "functionality",
        "article",
        "vxai",
        "abstract",
        "research",
        "xai",
        "methods"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 80,
      "paragraph": "Stage 1: Initial Search\nInstead, we added the cited metric to our corpus for the snowballing phase, resulting in 515 potentially relevant sources. Of the articles retrieved though the initial database search, only 6 were ultimately included in our review.\nWe followed a similar procedure for 65 additional articles identified through other sources, including Google Scholar and personal databases. After excluding 26 of these, 39 articles were directly included in our review. An additional 20 references from these articles were added to the corpus of the snowballing phase.\nIn total, during the first stage, we included 45 articles in our review (6 from database search and 39 from additional sources). We also compiled 535 additional references for the second-stage backward snowballing (515 from the database search and 20 from the additional sources).",
      "keywords": [
        "cited",
        "references",
        "corpus",
        "articles",
        "snowballing",
        "google",
        "sources",
        "search"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 81,
      "paragraph": "Stage 2: Snowballing\nStarting with the corpus of 535 references identified in the first stage, we conducted full-text screening on all records as described above. During the recursive backward snowballing process, we identified\n3 https://www.scopus.com/search/form.uri?display=advanced\n4 https://ieeexplore.ieee.org/search/advanced/command\n5 https://dl.acm.org/search/\n6 The × denotes a logical combination of terms.\nand assessed additional 98 references for eligibility. We observed that the set of relevant records quickly began to converge: most newly encountered papers that did not present original metrics (ER-5) cited articles already included in our corpus. We did not perform forward snowballing (i.e., identifying articles that cite our included works), as our focus was on original metric proposals. Including such follow-up papers would have significantly increased the number of records beyond a manageable scope.",
      "keywords": [
        "scopus",
        "cite",
        "cited",
        "references",
        "metrics",
        "papers",
        "corpus",
        "articles"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 82,
      "paragraph": "Stage 2: Snowballing\nIn total, we assessed 628 articles during the second phase (535 from the first stage and 98 from recursive snowballing). Out of these we included 317. Compared to the 20% inclusion rate in the first stage, the nearly 50% inclusion rate in the second phase confirms the effectiveness of our strategy. The initial database search primarily yielded secondary sources (e.g., XAI and VXAI reviews), which helped identify original metric proposals during snowballing.\nIn total, we reviewed 1459 articles, screened 866 in full, and included 362 that originally proposed a VXAI metric or one of its variants.",
      "keywords": [
        "snowballing",
        "articles",
        "reviews",
        "metric",
        "assessed",
        "rate",
        "vxai",
        "effectiveness"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 83,
      "paragraph": "5 Results\nIn this section, we present the main results of our literature search. We identified a total of 362 individual references, some of which introduce or modify multiple metrics simultaneously. We sorted and aggregated these metrics based on their methodology, grouping together those that are functionally similar and measure the same properties. Each such group is referred to as an aggregated metric For each aggregated metric, we assessed the associated desiderata, the metric's contextuality, and the suitable explanation types as we define in our categorization scheme below. Having introduced the seven desiderata in Section 3, we now formalize the remaining dimensions of the categorization scheme, before giving an overview of the aggregated metrics. However, to ensure readability, this section will mainly focus on meta-results and statistics of the metrics. Table 2 gives an overview over the metrics, while detailed descriptions with the associated references are presented in Appendix B. A browsable version of the framework will be made available at https://placeholder.link/available-upon-acceptance .",
      "keywords": [
        "metrics",
        "categorization",
        "contextuality",
        "readability",
        "functionally",
        "descriptions",
        "overview",
        "aggregated"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 85,
      "paragraph": "5.1.1 Overview\nContextuality: Second, we propose to distinguish metrics based on their evaluation context. We identify five different levels, each one pushing the contextuality of the evaluation further than the previous one. They are defined as follows:\n- I) Explanans-Centric: Evaluates only the explanans in relation to the raw input instance. It is entirely agnostic to the black-box model and does not rely on its predictions or behavior.\n- III) Input Intervention: Perturbs or modifies the input data, then observes the resulting predictions and explanantia to assess their changes.\n- II) Model Observation: Assumes access to the black-box model's outputs or internal processes (e.g., activations), enabling evaluation based on observed behavior.\n- IV) Model Intervention: Actively alters the black-box model itself, e.g., through retraining or parameter randomization.\n- V) A Priori Constrained: Requires specific setups of explananda, requiring particular data or models.",
      "keywords": [
        "contextuality",
        "metrics",
        "evaluation",
        "context",
        "activations",
        "evaluates",
        "predictions",
        "retraining"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 86,
      "paragraph": "5.1.1 Overview\nWith each level, we further shift the evaluation focus from In-Situ to Ex-Situ evaluation 7 . The first three levels consist of In-Situ metrics, as they present an evaluation of concrete explanantia in the context of their original explanandum. The final two levels instead encompass Ex-Situ metrics, evaluating an explanation method more generally, either by imposing restrictions on the explanandum or by significantly altering the original context. This is similar to the common categorization of XAI methods into ante-hoc, in-hoc and post-hoc approaches (Carvalho et al., 2019; Zhang et al., 2021; Speith, 2022; Nauta et al., 2023; Bedi et al., 2024). Although Ex-Situ evaluations can serve to establish general comparisons between explanation methods, only In-Situ metrics provide information about the explanans in a given scenario.",
      "keywords": [
        "explanans",
        "explanantia",
        "explanandum",
        "evaluations",
        "evaluation",
        "xai",
        "explanation",
        "evaluating"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 87,
      "paragraph": "5.1.1 Overview\nExplanation Types: Third, we categorize VXAI metrics based on the accepted input. Apart from a few exceptions, all metrics are agnostic to the underlying black-box model or data format (e.g., tabular or image). Therefore, we do not consider this dimension separately. Instead, we draw inspiration from several XAI classification schemes, which propose a division based on the explanation approach or resulting explanans (Carvalho et al., 2019; Markus et al., 2021; Zhang et al., 2021; Speith, 2022; Nauta et al., 2023). We introduce and discuss the different explanation types in Subsection 5.1.2 below.",
      "keywords": [
        "vxai",
        "classification",
        "xai",
        "categorize",
        "metrics",
        "explanation",
        "explanans",
        "dimension"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 88,
      "paragraph": "5.1.1 Overview\nFor XAI algorithms, a distinction exists between local and global methods (Zhang et al., 2021; Speith, 2022; Bedi et al., 2024), which we do not translate to VXAI metrics (unlike Robnik-Šikonja & Bohanec (2018)). Although not every metric is suited to calculate results for a single data input, the metrics can usually be adapted easily, e.g., calculating the change in logits for an individual input instead of calculating a change in accuracy over the dataset. Conversely, each metric that provides results for a single local explanans can be easily aggregated over a set of explanantia to obtain a global metric.",
      "keywords": [
        "xai",
        "metrics",
        "vxai",
        "algorithms",
        "accuracy",
        "aggregated",
        "metric",
        "dataset"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 90,
      "paragraph": "5.1.2 Explanation Types\nFeature Attributions (FAs): A FA explanation returns a vector e ∈ R d , which usually (but not necessarily) has the same dimension as the input x . Each dimension represents an input feature, e.g., column in tabular data, (super-)pixel in images, or node in graphs. The value e j ∈ e then represents the relevance of the given feature towards the explained prediction. Depending on the underlying explanation algorithm, values can be positive or negative, and they may be inherently bounded to a given range or unbounded. Some FA methods assign continuous importance scores to each feature, while others produce binary or thresholded outputs that identify a subset of important features. Furthermore, Saliency Maps (which highlight important regions in the input, typically used in image-based tasks) present a special case of FAs, as features are not completely independent but exhibit spatial relationships. Usually, FAs are used as local explanations (Bach et al., 2015; Ribeiro et al., 2016; Lundberg, 2017; Shrikumar et al., 2017; Sundararajan et al., 2017), assigning feature importance for a single input prediction. However, they can also",
      "keywords": [
        "saliency",
        "explanations",
        "features",
        "explanation",
        "attributions",
        "feature",
        "spatial",
        "explained"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 96,
      "paragraph": "5.2 Identified Metrics\nNow that we have established our classification scheme, we turn to the results of our literature review and report key statistics about the identified metrics.\nWe identified metrics from 362 individual sources. As some works proposed multiple variants, the total number of originally found metrics exceeded 400. These were grouped into 41 functionally distinct, aggregated metrics based on shared goals, methodology, or assumptions. Each was then categorized using the three-dimensional scheme introduced above. Since a single metric may serve multiple desiderata and apply to different explanation types, we organize them primarily by their contextuality level. Table 2 presents a complete overview of all metrics and their classification. A summary of key patterns follows below; for detailed descriptions of each individual metric, see Appendix B. Four metrics\nTable 2: The 41 aggregated metrics identified in our study, as presented in Appendix B. Each metric is classified according to its associated desiderata, applicable explanation types, and level of contextuality. ✓ indicates full alignment or reported usage in the literature, while ( ✓ ) denotes partial contribution or unreported but plausible applicability. The final column shows the number of references per metric.",
      "keywords": [
        "metrics",
        "contextuality",
        "classification",
        "applicability",
        "descriptions",
        "metric",
        "methodology",
        "references"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 97,
      "paragraph": "5.2 Identified Metrics\n, Aggregated Metric = (1) Explanans Size. , Parsimony = ✓. , Desiderata.Plausibility = . , Fidelity = . , Continuity = . , Consistency = . , Efficiency = ✓. , Explanation Type.FA = ✓. , Explanation Type.ExE = ( ✓. , Explanation Type.CE = ) ✓. , Explanation Type.WBS = . , Explanation Type.NLE = ( ✓ ). , # References = 51. , Aggregated Metric = (2) Overlap. , Parsimony = ✓. , Desiderata.Plausibility = . , Fidelity = . , Continuity = . , Consistency = . , Efficiency = . , Explanation Type.FA = . , Explanation Type.ExE = . , Explanation Type.CE = . , Explanation Type.WBS = ✓. , Explanation Type.NLE = . , # References = 5. , Aggregated Metric = (3) Explanans Cohesion. , Parsimony = ✓. , Desiderata.Plausibility = ✓. , Fidelity = . , Continuity = . , Consistency = . , Efficiency = ✓. , Explanation Type.FA = . , Explanation",
      "keywords": [
        "metrics",
        "cohesion",
        "metric",
        "aggregated",
        "explanans",
        "consistency",
        "overlap",
        "size"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 98,
      "paragraph": "5.2 Identified Metrics\nType.ExE = . , Explanation Type.CE = ( ✓ ). , Explanation Type.WBS = . , Explanation Type.NLE = . , # References = 2. , Aggregated Metric = (4) Minimality. , Parsimony = ✓. , Desiderata.Plausibility = ✓. , Fidelity = . , Continuity = . , Consistency = . , Efficiency = . , Explanation Type.FA = . , Explanation Type.ExE = ✓. , Explanation Type.CE = . , Explanation Type.WBS = . , Explanation Type.NLE = . , # References = 25. , Aggregated Metric = (5) Autoencoder Plausibility. , Parsimony = . , Desiderata.Plausibility = ✓. , Fidelity = . , Continuity = . , Consistency = . , Efficiency = . , Explanation Type.FA = ✓. , Explanation Type.ExE = . , Explanation Type.CE = . , Explanation Type.WBS = . , Explanation Type.NLE = . , # References = 1. , Aggregated Metric = (6) Diversity. , Parsimony = . , Desiderata.Plausibility = ✓. ,",
      "keywords": [
        "metrics",
        "autoencoder",
        "metric",
        "minimality",
        "aggregated",
        "diversity",
        "nle",
        "consistency"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 99,
      "paragraph": "5.2 Identified Metrics\nFidelity = . , Continuity = . , Consistency = . , Efficiency = . , Explanation Type.FA = . , Explanation Type.ExE = ✓. , Explanation Type.CE = . , Explanation Type.WBS = . , Explanation Type.NLE = . , # References = 6. , Aggregated Metric = (7) Input Similarity. , Parsimony = . , Desiderata.Plausibility = ✓. , Fidelity = . , Continuity = . , Consistency = . , Efficiency = . , Explanation Type.FA = . , Explanation Type.ExE = ✓. , Explanation Type.CE = . , Explanation Type.WBS = . , Explanation Type.NLE = . , # References = 11. I, Aggregated Metric = (8) Input Contrastivity. I, Parsimony = . I, Desiderata.Plausibility = ✓. I, Fidelity = ( ✓ ). I, Continuity = . I, Consistency = . I, Efficiency = . I, Explanation Type.FA = ✓. I, Explanation Type.ExE = ( ✓ ). I, Explanation Type.CE = ( ✓ ). I, Explanation Type.WBS = ( ✓ ). I,",
      "keywords": [
        "metrics",
        "contrastivity",
        "similarity",
        "metric",
        "consistency",
        "aggregated",
        "parsimony",
        "fidelity"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 100,
      "paragraph": "5.2 Identified Metrics\nExplanation Type.NLE = ( ✓ ). I, # References = 2. , Aggregated Metric = (9) Actionability. , Parsimony = . , Desiderata.Plausibility = ✓. , Fidelity = ✓. , Continuity = . , Consistency = . , Efficiency = . , Explanation Type.FA = . , Explanation Type.ExE = ✓. , Explanation Type.CE = . , Explanation Type.WBS = . , Explanation Type.NLE = . , # References = 7. , Aggregated Metric = (10) Model-Agnostic Explanation Consistency. , Parsimony = . , Desiderata.Plausibility = ✓. , Fidelity = . , Continuity = . , Consistency = ( ✓ ). , Efficiency = ✓. , Explanation Type.FA = . , Explanation Type.ExE = ✓. , Explanation Type.CE = ( ✓ ). , Explanation Type.WBS = ( ✓ ). , Explanation Type.NLE = ( ✓ ). , # References = 4. , Aggregated Metric = (11) Input Coverage. , Parsimony = . , Desiderata.Plausibility = ✓. , Fidelity = . , Continuity = . ,",
      "keywords": [
        "metrics",
        "actionability",
        "metric",
        "coverage",
        "consistency",
        "aggregated",
        "efficiency",
        "explanation"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 101,
      "paragraph": "5.2 Identified Metrics\nConsistency = . , Efficiency = ✓. , Explanation Type.FA = ✓. , Explanation Type.ExE = (. , Explanation Type.CE = ). , Explanation Type.WBS = ✓. , Explanation Type.NLE = ( ✓ ). , # References = 8. , Aggregated Metric = (12) Output Coverage. , Parsimony = . , Desiderata.Plausibility = ✓. , Fidelity = . , Continuity = . , Consistency = . , Efficiency = . , Explanation Type.FA = . , Explanation Type.ExE = ✓. , Explanation Type.CE = . , Explanation Type.WBS = ✓. , Explanation Type.NLE = . , # References = 1. , Aggregated Metric = (13) Output Mutual Information. , Parsimony = . , Desiderata.Plausibility = . , Fidelity = ✓. , Continuity = . , Consistency = . , Efficiency = ✓. , Explanation Type.FA = . , Explanation Type.ExE = . , Explanation Type.CE = ✓. , Explanation Type.WBS = . , Explanation Type.NLE = . , # References = 1. , Aggregated Metric = (14) Input Mutual",
      "keywords": [
        "metrics",
        "metric",
        "information",
        "consistency",
        "output",
        "aggregated",
        "efficiency",
        "coverage"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 102,
      "paragraph": "5.2 Identified Metrics\nInformation. , Parsimony = ✓. , Desiderata.Plausibility = . , Fidelity = . , Continuity = . , Consistency = . , Efficiency = ✓. , Explanation Type.FA = . , Explanation Type.ExE = . , Explanation Type.CE = ✓. , Explanation Type.WBS = . , Explanation Type.NLE = . , # References = 1. , Aggregated Metric = (15) Output Contrastivity. , Parsimony = . , Desiderata.Plausibility = ✓. , Fidelity = . , Continuity = . , Consistency = . , Efficiency = ✓. , Explanation Type.FA = ( ✓ ). , Explanation Type.ExE = ( ✓ ). , Explanation Type.CE = ( ✓. , Explanation Type.WBS = ). , Explanation Type.NLE = ( ✓ ). , # References = 5. , Aggregated Metric = (16) Output Similarity. , Parsimony = . , Desiderata.Plausibility = ✓. , Fidelity = . , Continuity = . , Consistency = . , Efficiency = . , Explanation Type.FA = . , Explanation Type.ExE = ✓. , Explanation Type.CE = .",
      "keywords": [
        "metrics",
        "contrastivity",
        "similarity",
        "metric",
        "information",
        "aggregated",
        "output",
        "consistency"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 103,
      "paragraph": "5.2 Identified Metrics\n, Explanation Type.WBS = . , Explanation Type.NLE = . , # References = 1. , Aggregated Metric = (17) Mutual Coherence. , Parsimony = . , Desiderata.Plausibility = ✓. , Fidelity = ( ✓ ). , Continuity = . , Consistency = . , Efficiency = ✓. , Explanation Type.FA = ( ✓ ). , Explanation Type.ExE = ( ✓. , Explanation Type.CE = ) (. , Explanation Type.WBS = ✓ ). , Explanation Type.NLE = ( ✓ ). , # References = 19. , Aggregated Metric = (18) Significance Check. , Parsimony = . , Desiderata.Plausibility = . , Fidelity = ✓. , Continuity = . , Consistency = . , Efficiency = ✓. , Explanation Type.FA = (. , Explanation Type.ExE = ✓ ). , Explanation Type.CE = ✓ (. , Explanation Type.WBS = ✓ ). , Explanation Type.NLE = ( ✓ ). , # References = 15. , Aggregated Metric = (19) (Counter-)Factual Relevance. , Parsimony = . ,",
      "keywords": [
        "metrics",
        "coherence",
        "metric",
        "relevance",
        "references",
        "consistency",
        "aggregated",
        "significance"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 104,
      "paragraph": "5.2 Identified Metrics\nDesiderata.Plausibility = . , Fidelity = ✓. , Continuity = . , Consistency = . , Efficiency = . , Explanation Type.FA = . , Explanation Type.ExE = ✓. , Explanation Type.CE = . , Explanation Type.WBS = . , Explanation Type.NLE = . , # References = 1. II, Aggregated Metric = (20) Prediction Validity. II, Parsimony = . II, Desiderata.Plausibility = . II, Fidelity = ✓. II, Continuity = . II, Consistency = . II, Efficiency = . II, Explanation Type.FA = . II, Explanation Type.ExE = ✓. II, Explanation Type.CE = . II, Explanation Type.WBS = . II, Explanation Type.NLE = . II, # References = 18. , Aggregated Metric = (21) Sufficency. , Parsimony = . , Desiderata.Plausibility = . , Fidelity = ✓. , Continuity = . , Consistency = . , Efficiency = . , Explanation Type.FA = . , Explanation Type.ExE = . , Explanation Type.CE = ✓. , Explanation Type.WBS = .",
      "keywords": [
        "metrics",
        "sufficency",
        "prediction",
        "metric",
        "consistency",
        "aggregated",
        "fidelity",
        "validity"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 105,
      "paragraph": "5.2 Identified Metrics\n, Explanation Type.NLE = . , # References = 2. , Aggregated Metric = (22) Output Faithfulness. , Parsimony = . , Desiderata.Plausibility = . , Fidelity = ✓. , Continuity = . , Consistency = . , Efficiency = . , Explanation Type.FA = . , Explanation Type.ExE = . , Explanation Type.CE = . , Explanation Type.WBS = ✓. , Explanation Type.NLE = . , # References = 34. , Aggregated Metric = (23) Internal Faithfulness. , Parsimony = . , Desiderata.Plausibility = . , Fidelity = ✓. , Continuity = . , Consistency = . , Efficiency = . , Explanation Type.FA = . , Explanation Type.ExE = . , Explanation Type.CE = . , Explanation Type.WBS = ✓. , Explanation Type.NLE = . , # References = 3. , Aggregated Metric = (24) Setup Consistency. , Parsimony = . , Desiderata.Plausibility = . , Fidelity = . , Continuity = . , Consistency = ✓. , Efficiency = . , Explanation Type.FA = ✓. , Explanation",
      "keywords": [
        "metrics",
        "consistency",
        "faithfulness",
        "metric",
        "output",
        "aggregated",
        "fidelity",
        "efficiency"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 106,
      "paragraph": "5.2 Identified Metrics\nType.ExE = ✓. , Explanation Type.CE = ( ✓ ). , Explanation Type.WBS = ✓. , Explanation Type.NLE = ( ✓ ). , # References = 11. , Aggregated Metric = (25) Hyperparameter Sensitivity. , Parsimony = . , Desiderata.Plausibility = . , Fidelity = . , Continuity = . , Consistency = ✓. , Efficiency = ✓. , Explanation Type.FA = ( ✓ ). , Explanation Type.ExE = ( ✓ ). , Explanation Type.CE = ( ✓. , Explanation Type.WBS = ). , Explanation Type.NLE = ( ✓ ). , # References = 6. , Aggregated Metric = (26) Execution Time. , Parsimony = . , Desiderata.Plausibility = . , Fidelity = . , Continuity = . , Consistency = . , Efficiency = ✓ ✓. , Explanation Type.FA = ✓. , Explanation Type.ExE = . , Explanation Type.CE = ( ✓ ). , Explanation Type.WBS = ✓. , Explanation Type.NLE = ( ✓ ). , # References = 34. , Aggregated Metric =",
      "keywords": [
        "metrics",
        "hyperparameter",
        "metric",
        "sensitivity",
        "execution",
        "aggregated",
        "efficiency",
        "consistency"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 107,
      "paragraph": "5.2 Identified Metrics\n(27) Unguided Perturbation Fidelity. , Parsimony = . , Desiderata.Plausibility = . , Fidelity = ✓. , Continuity = . , Consistency = . , Efficiency = ✓. , Explanation Type.FA = . , Explanation Type.ExE = . , Explanation Type.CE = . , Explanation Type.WBS = . , Explanation Type.NLE = . , # References = 10. , Aggregated Metric = (28) Guided Perturbation Fidelity. , Parsimony = . , Desiderata.Plausibility = . , Fidelity = ✓. , Continuity = . , Consistency = . , Efficiency = ✓. , Explanation Type.FA = . , Explanation Type.ExE = . , Explanation Type.CE = ✓. , Explanation Type.WBS = . , Explanation Type.NLE = . , # References = 75. , Aggregated Metric = (29) Counterfactuability. , Parsimony = . , Desiderata.Plausibility = . , Fidelity = ✓. , Continuity = . , Consistency = . , Efficiency = . , Explanation Type.FA = . , Explanation Type.ExE = . , Explanation Type.CE =",
      "keywords": [
        "metrics",
        "perturbation",
        "counterfactuability",
        "metric",
        "consistency",
        "continuity",
        "fidelity",
        "guided"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 108,
      "paragraph": "5.2 Identified Metrics\n. , Explanation Type.WBS = ✓. , Explanation Type.NLE = . , # References = 1. III, Aggregated Metric = (30) Prediction Neighborhood Continuity. III, Parsimony = . III, Desiderata.Plausibility = . III, Fidelity = ✓. III, Continuity = ✓. III, Consistency = . III, Efficiency = . III, Explanation Type.FA = . III, Explanation Type.ExE = . III, Explanation Type.CE = . III, Explanation Type.WBS = ✓. III, Explanation Type.NLE = . III, # References = 1. , Aggregated Metric = (31) Quantification of Unexplainable Features. , Parsimony = . , Desiderata.Plausibility = . , Fidelity = ( ✓ ). , Continuity = ✓. , Consistency = . , Efficiency = . , Explanation Type.FA = ✓. , Explanation Type.ExE = . , Explanation Type.CE = ( ✓ ). , Explanation Type.WBS = . , Explanation Type.NLE = . , # References = 2. , Aggregated Metric = (32) Neighborhood Continuity (33) Adversarial Input",
      "keywords": [
        "metrics",
        "adversarial",
        "prediction",
        "metric",
        "quantification",
        "features",
        "identified",
        "unexplainable"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 109,
      "paragraph": "5.2 Identified Metrics\nResilience. , Parsimony = . , Desiderata.Plausibility = . , Fidelity = . , Continuity = ✓ ✓. , Consistency = . , Efficiency = . , Explanation Type.FA = ✓ ✓. , Explanation Type.ExE = ✓ ( ✓ ). , Explanation Type.CE = ( ✓ ) ( ✓ ). , Explanation Type.WBS = ✓ ( ✓ ). , Explanation Type.NLE = ✓ ( ✓ ). , # References = 19 10. , Aggregated Metric = (34) Model Parameter Randomization Test. , Parsimony = . , Desiderata.Plausibility = . , Fidelity = ✓. , Continuity = . , Consistency = . , Efficiency = . , Explanation Type.FA = ✓. , Explanation Type.ExE = ( ✓ ). , Explanation Type.CE = ( ✓ ). , Explanation Type.WBS = ( ✓ ). , Explanation Type.NLE = ( ✓ ). , # References = 5. , Aggregated Metric = (35) Data Randomization Test. , Parsimony = . , Desiderata.Plausibility = . , Fidelity = ✓. , Continuity = . , Consistency = . ,",
      "keywords": [
        "metrics",
        "resilience",
        "metric",
        "consistency",
        "randomization",
        "data",
        "aggregated",
        "efficiency"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 110,
      "paragraph": "5.2 Identified Metrics\nEfficiency = . , Explanation Type.FA = ✓. , Explanation Type.ExE = ( ✓ ). , Explanation Type.CE = ( ✓ ). , Explanation Type.WBS = ( ✓ ). , Explanation Type.NLE = ( ✓ ). , # References = 2. , Aggregated Metric = (36) Retrained Model Evaluation. , Parsimony = . , Desiderata.Plausibility = . , Fidelity = ✓. , Continuity = . , Consistency = . , Efficiency = ✓. , Explanation Type.FA = . , Explanation Type.ExE = . , Explanation Type.CE = . , Explanation Type.WBS = . , Explanation Type.NLE = . , # References = 10. IV, Aggregated Metric = (37) Influence Fidelity. IV, Parsimony = . IV, Desiderata.Plausibility = . IV, Fidelity = ✓. IV, Continuity = . IV, Consistency = . IV, Efficiency = . IV, Explanation Type.FA = ✓. IV, Explanation Type.ExE = . IV, Explanation Type.CE = . IV, Explanation Type.WBS = . IV, Explanation Type.NLE = . IV, #",
      "keywords": [
        "metrics",
        "metric",
        "retrained",
        "efficiency",
        "evaluation",
        "consistency",
        "influence",
        "model"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 111,
      "paragraph": "5.2 Identified Metrics\nReferences = 2. , Aggregated Metric = (38) Normalized Movement Rate. , Parsimony = . , Desiderata.Plausibility = . , Fidelity = . , Continuity = ✓. , Consistency = . , Efficiency = . , Explanation Type.FA = ✓. , Explanation Type.ExE = . , Explanation Type.CE = . , Explanation Type.WBS = . , Explanation Type.NLE = . , # References = 2. , Aggregated Metric = (39) Adversarial Model Resilience. , Parsimony = . , Desiderata.Plausibility = . , Fidelity = . , Continuity = ✓. , Consistency = . , Efficiency = ✓. , Explanation Type.FA = ( ✓. , Explanation Type.ExE = ). , Explanation Type.CE = ( ✓ ). , Explanation Type.WBS = ( ✓ ). , Explanation Type.NLE = ( ✓ ). , # References = 4. , Aggregated Metric = (40) GT Dataset Evaluation. , Parsimony = . , Desiderata.Plausibility = ✓. , Fidelity = ✓. , Continuity = . , Consistency = . , Efficiency = . ,",
      "keywords": [
        "adversarial",
        "metrics",
        "dataset",
        "metric",
        "movement",
        "normalized",
        "resilience",
        "consistency"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 112,
      "paragraph": "5.2 Identified Metrics\nExplanation Type.FA = ✓. , Explanation Type.ExE = . , Explanation Type.CE = ✓. , Explanation Type.WBS = . , Explanation Type.NLE = ✓. , # References = 119. V, Aggregated Metric = (41) White Box Model Check. V, Parsimony = . V, Desiderata.Plausibility = . V, Fidelity = ✓. V, Continuity = . V, Consistency = . V, Efficiency = . V, Explanation Type.FA = ✓. V, Explanation Type.ExE = ( ✓ ). V, Explanation Type.CE = . V, Explanation Type.WBS = . V, Explanation Type.NLE = ( ✓ ). V, # References = 12\ndeemed conceptually flawed or misaligned with any desideratum were excluded and are discussed in Appendix C.",
      "keywords": [
        "metrics",
        "metric",
        "conceptually",
        "deemed",
        "consistency",
        "plausibility",
        "efficiency",
        "fidelity"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 114,
      "paragraph": "5.2 Identified Metrics\nExplanation Types: A total of 25 metrics are applicable to FA methods. An equal number is available for ExE, although only 15 are directly supported by the literature; the remaining 10 are marked as potentially applicable (see ( ✓ ) in Table 2 and the opaque bars in Figure 5). For other explanation types, direct literature support is more limited: 11 metrics for WBS, 6 for CE, and just 2 for NLE. However, we consider many metrics adaptable even in the absence of published usage, increasing the totals to 22 for CE, 21 for WBS, and 17 for NLE. In total, 15 metrics are applicable (or adaptable) across all explanation types, and 19 are proprietary to a single type. Notably, no metric is exclusive to NLE, and only one is unique to CE. The largest overlap exists between FA and CE, with 21 metrics covering both. FA also has the highest number of adaptable metrics: while only 3 are exclusive to FA alone, 22 are shared with other types.",
      "keywords": [
        "metrics",
        "metric",
        "methods",
        "types",
        "explanation",
        "totals",
        "fa",
        "type"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 118,
      "paragraph": "5.2 Identified Metrics\nFigure 5: Overview of the distribution of metrics within the categorization scheme. Each metric may be associated with multiple desiderata and explanation types, but only a single level of contextuality. Light-colored bars indicate partial alignment with a desideratum. For explanation types, light bars denote cases where no usage has been reported in the literature, though the metric is considered adaptable. The bottom histogram shows the number of metrics grouped by their reference count.",
      "keywords": [
        "metrics",
        "categorization",
        "metric",
        "contextuality",
        "denote",
        "desideratum",
        "types",
        "indicate"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 121,
      "paragraph": "6 Discussion\nwell-aligned metric than others by a diverse set of weaker proxies.\nExplanation type focus: Among explanation types, FA is the most extensively studied, which is reflected both in literature prevalence and metric availability. This focus has also enabled the adoption of classical techniques from adjacent fields. For example, treating FA explanations as feature selectors allows the use of established stability metrics from feature selection research, such as those used by Nogueira et al. (2018). Conversely, the explanation type NLE is still underrepresented in dedicated\nIV Model Intervention",
      "keywords": [
        "explanations",
        "intervention",
        "feature",
        "stability",
        "explanation",
        "model",
        "metrics",
        "selectors"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 125,
      "paragraph": "V A Priori Constrained\nLikewise, metrics based on input perturbation or retraining (e.g., Metric 28, Metric 32, or Metric 36) can yield misleading results. Feature removal may not affect model output if redundant features are present (D'Amour et al., 2022), and retraining can alter the model in uncontrolled ways, obscuring meaningful evaluation (Hooker et al., 2019).\nMetric aggregation: While most studies report multiple metric scores separately, some recent work proposes aggregating them into a single composite score (Farruque et al., 2021; Margot & Luta, 2021; Poppi et al., 2021; Bommer et al., 2024). Suggested methods include scaling by theoretical bounds, comparison to random or perfect baselines, and aggregation via weighted sums or harmonic means. Despite these proposals, the field lacks consensus on how to combine metrics meaningfully and robustly.",
      "keywords": [
        "metrics",
        "metric",
        "baselines",
        "features",
        "scaling",
        "feature",
        "retraining",
        "evaluation"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 129,
      "paragraph": "Acknowledgment\nThis research is funded by the German Federal Ministry for Digital and Transport (BMDV) as part of the project MISSION KI - Nationale Initiative für Künstliche Intelligenz und Datenökonomie with the funding code 45KI22B021.\nWe would like to thank our colleagues Kevin Iselborn, and Jayanth Siddamsetty for reviewing this survey and sharing helpful comments and insights throughout its development.",
      "keywords": [
        "künstliche",
        "digital",
        "intelligenz",
        "transport",
        "german",
        "ki",
        "datenökonomie",
        "insights"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 131,
      "paragraph": "IEEE Access\n- ( \" All Metadata \" : \" Explain∗ AI\" OR \" All Metadata \" : \" Explain∗ Artif i c i a l I n t e l l i g e n c e \" OR \" All Metadata \" : \" Explain∗ ML\" OR \" All Metadata \" : \" Explain∗ Machine Learning \" OR\nAND\n- \" All Metadata \" : \" I n t e r p r e t ∗ AI\" OR \" All Metadata \" : \" I n t e r p r e t ∗ Artif i c i a l I n t e l l i g e n c e \" OR \" All Metadata \" : \" I n t e r p r e t ∗ ML\" OR \" All Metadata \" : \" I n t e r p r e t ∗ Machine Learning \")\n```\n( \" All Metadata \" : \" Evaluation \" OR \" All Metadata \" : \" Metric \" OR \" All Metadata \" : \" Quantification \") AND ( \" All Metadata \" : \" Survey \" OR \" All Metadata \" : \" Review \")\n```",
      "keywords": [
        "metadata",
        "ieee",
        "access",
        "ai",
        "artif",
        "ml",
        "evaluation",
        "quantification"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 132,
      "paragraph": "IEEE Access\n```\nACM ANYWHERE: [ ( \" Explain∗ AI\" OR \" Explain∗ Artif i c i a l I n t e l l i g e n c e \" OR \" Explain∗ ML\"\n```\n```\nOR \" Explain∗ Machine Learning \" OR \" I n t e r p r e t ∗ AI\" OR \" Interpret ∗ Artif i c i a l I n t e l l i g e n c e \" OR \" Interpret ∗ ML\" OR \" Interpret ∗ Machine Learning \") AND ( \" Evaluation \" OR \" Metric \" OR \" Quantification \") AND ( \" Survey \" OR \"Review \") ]\n```",
      "keywords": [
        "ai",
        "ieee",
        "acm",
        "ml",
        "evaluation",
        "learning",
        "machine",
        "quantification"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 133,
      "paragraph": "B.1 Notation\nThe mathematical expressions used throughout the appendix are intended to clarify concepts. We do not enforce strict formalism as long as the notation remains unambiguous. We define the following symbols, which are used across all metrics:\nThe mathematical expressions used throughout the appendix are intended to clarify concepts. We do not enforce strict formalism as long as the notation remains unambiguous. In cases where clarity is not compromised, we slightly overload notation, but specific meanings are always disambiguated by context. While the notation is predominantly tailored to classification tasks, where the model assigns scores to discrete class labels, it can be adapted to suit regression settings or, in part, other paradigms. We define the following symbols, which are used across all metrics:",
      "keywords": [
        "notation",
        "regression",
        "classification",
        "labels",
        "metrics",
        "meanings",
        "scores",
        "clarify"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 134,
      "paragraph": "B.1 Notation\n- · Let X and Y denote the input (data) and output (label) spaces, respectively.\n- · For a given input x ∈ X , we write θ x ( ) as shorthand for the vector of scores across all labels in Y , i.e., θ x ( ) := ( θ x, y ( 1 ) , θ ( x, y 2 ) , . . . , θ ( x, y k )).\n- · The black-box model is defined as a scoring function θ : X ×Y → R that assigns a real-valued predictive score (e.g., logits) to each input-label pair.\n- · The predicted label is then given by ˆ y x := arg max y ∈Y θ x, y ( ).\n- · Let X y ⊆ X be the set of all instances with ground-truth class y .\n- · We denote an arbitrary second input as x ′ ∈ X and a perturbed version of x as ˙ x (see Subsection B.2.1).",
      "keywords": [
        "predictive",
        "label",
        "predicted",
        "labels",
        "box",
        "model",
        "data",
        "black"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 135,
      "paragraph": "B.1 Notation\n- · An explanandum is a triple ( θ, x, y ), where θ is the model, x ∈ X is the input instance, and y is the label to be explained (which is often, but not necessarily, ˆ). y\n- · An explanans for a given explanandum is denoted e θ,x,y , or simply e when unambiguous.\n- · For ExEs, we write z := e (ExE) θ,x,y ∗ z , where z ∈ X is typically a counterfactual input such that ˆ y z := arg max y ∈Y θ z, y ( ) = y ∗ z .\n- · For WBS, the surrogate model is denoted θ e := e (WBS) θ,x,y ˆ . The surrogate returns a label prediction for arbitrary input via ˆ y e x ′ := θ e ( x ′ ).",
      "keywords": [
        "explanandum",
        "explanans",
        "notation",
        "prediction",
        "denoted",
        "label",
        "surrogate",
        "model"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 136,
      "paragraph": "B.1 Notation\n- · δ ( · , · ) denotes an arbitrary distance or dissimilarity function, which may be applied to inputs, explanations, or predictions depending on context. Similarity measures (see Subsection B.2.3) may be applied by inverting them appropriately (e.g., via negation or reciprocal).\n- · ✶ [ · ] is the indicator function, returning 1 if the condition holds and 0 otherwise.\n- · | · | indicates size or cardinality; ‖ · ‖ p denotes the L p -norm.\n- · k refers to a generic parameter (e.g., number of selected features, neighbors, or examples); /epsilon1 denotes a small threshold or tolerance constant.\n- · Φ y denotes an autoencoder trained specifically on inputs from class y (i.e., x ∈ X y ).\nFor convenience, we recall the abbreviations of the explanation types introduced in Subsection 5.1:\n- • Feature Attribution (FA)\n- • Example Explanation (ExE)\n- • Concept Explanation (CE)\n- • White-Box Surrogate (WBS)\n- • Natural Language Explanation (NLE)",
      "keywords": [
        "autoencoder",
        "explanations",
        "explanation",
        "indicates",
        "notation",
        "similarity",
        "feature",
        "features"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 137,
      "paragraph": "B.1 Notation\nAdditionally, we adopt the following terminology: Metrics refer to the individual or aggregated VXAI criteria we evaluate and identified from the literature survey. Measures denote performance-scoring functions (e.g., accuracy, similarity, overlap), often also called metrics in literature; but we use the term 'measure' to distinguish them clearly.\nTo improve readability while maintaining completeness throughout the Appendix, we move extensively long reference lists into footnotes using the notation: [a] .",
      "keywords": [
        "metrics",
        "accuracy",
        "vxai",
        "similarity",
        "measures",
        "readability",
        "aggregated",
        "performance"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 138,
      "paragraph": "B.2 Helper Functions\nThe following components are recurring elements that serve as functional building blocks across multiple metrics. They can be understood as interchangeable parameters that shape how specific evaluation metrics are instantiated and interpreted.",
      "keywords": [
        "functions",
        "metrics",
        "components",
        "helper",
        "functional",
        "evaluation",
        "parameters",
        "elements"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 139,
      "paragraph": "B.2.1 Perturbation Approach\nPerturbations are small changes typically applied to input features and are recurring components in metrics targeting Fidelity and Continuity. There exists a wide range of perturbation strategies, and the choice of approach can significantly affect both metric results and their interpretation (Brunke et al., 2020; Funke et al., 2022; Rong et al., 2022).",
      "keywords": [
        "perturbations",
        "metrics",
        "perturbation",
        "features",
        "metric",
        "continuity",
        "changes",
        "input"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 140,
      "paragraph": "B.2.1 Perturbation Approach\nWe distinguish first by the Perturbation Scope , i.e., the parts of the input that are modified. Perturbations can be applied at a fine-grained level (e.g., individual features) or on more structured, high-level groupings. For image data, scope definitions may involve aggregating pixels into fixed grids (Schulz et al., 2020) or segmenting into superpixels (Ribeiro et al., 2016; Kapishnikov et al., 2019; Rieger & Hansen, 2020). In time-series data, one may perturb fixed-length windows, with the target time-step at the beginning or middle (Schlegel et al., 2019; 2020). In topologically ordered domains (e.g., images, time-series, or graphs), adjacent features can be perturbed together, such as modifying the area surrounding a focal pixel (Samek et al., 2016; Brahimi et al., 2019). Higher-level approaches include perturbing Concepts (El Shawi et al., 2021) or internal activations associated with object parts",
      "keywords": [
        "perturbing",
        "perturbations",
        "perturbation",
        "segmenting",
        "superpixels",
        "features",
        "modifying",
        "perturb"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 141,
      "paragraph": "B.2.1 Perturbation Approach\n(Zhang et al., 2019b).",
      "keywords": [
        "perturbation",
        "zhang",
        "approach",
        "2019b",
        "et",
        "al"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 142,
      "paragraph": "B.2.1 Perturbation Approach\nOnce the scope is defined, a variety of Perturbation Functions can be applied. In fact, any perturbation function might be suitable (Hameed et al., 2022; Schlegel & Keim, 2023). However, here we present some of the most common in literature. At the simplest level, features may be removed by setting them to zero or dropping them entirely, especially in structured domains such as graphs, text, or time-series data, as employed by many authors, (e.g., Bach et al. (2015); Ancona et al. (2017); Alvarez-Melis & Jaakkola (2018b); Chu et al. (2018); Arya et al. (2019); DeYoung et al. (2019); Schlegel et al. (2019); Cong et al. (2020); Singh et al. (2020); Warnecke et al. (2020); Bajaj et al. (2021); Faber et al. (2021); Singh et al. (2021); Jin et al. (2023)). Alternatively, features can be replaced by a",
      "keywords": [
        "perturbation",
        "features",
        "graphs",
        "data",
        "text",
        "functions",
        "function",
        "dropping"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 143,
      "paragraph": "B.2.1 Perturbation Approach\nfixed value, e.g., the per-channel or per-instance mean (Petsiuk, 2018; Schlegel et al., 2019; 2020; Hameed et al., 2022; Jin et al., 2023).",
      "keywords": [
        "perturbation",
        "channel",
        "mean",
        "fixed",
        "instance",
        "jin",
        "approach",
        "2022"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 144,
      "paragraph": "B.2.1 Perturbation Approach\nTo generate less deterministic perturbations, authors propose adding random noise (e.g., Gaussian) or drawing values from uniform distributions (Yeh et al., 2019; Bhatt et al., 2020; Sturmfels et al., 2020; Bajaj et al., 2021; Funke et al., 2022; Veerappa et al., 2022). Other strategies leverage the spatial structure of the data: for instance, applying blurring or interpolation (Sturmfels et al., 2020; Rong et al., 2022), or reordering spatial regions (Schlegel et al., 2019; Chen et al., 2020). Instead of applying synthetic noise, values can be resampled from the marginal distribution of a feature, from its nearest neighbor, or even from an opposite-class example (Guo et al., 2018b; Hameed et al., 2022). Where influence regions are known, perturbations can be constrained to lie inside or outside these intervals (Velmurugan et al., 2021a).",
      "keywords": [
        "perturbations",
        "perturbation",
        "interpolation",
        "resampled",
        "spatial",
        "noise",
        "blurring",
        "feature"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 145,
      "paragraph": "B.2.1 Perturbation Approach\nIn the NLP domain, word embeddings can be noised, or tokens substituted using synonym sets and domain knowledge (Yin et al., 2021). For image inputs, another strategy is cropping and resizing to emphasize or suppress local information (Dabkowski & Gal, 2017).\nFinally, when perturbations are guided by a FA, their intensity can be scaled proportionally to the assigned importance scores (Chattopadhay et al., 2018; Guo et al., 2018b; Jung & Oh, 2021).\na Author et al.",
      "keywords": [
        "embeddings",
        "nlp",
        "emphasize",
        "perturbation",
        "perturbations",
        "cropping",
        "synonym",
        "image"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 146,
      "paragraph": "B.2.2 Normalization of Explanantia\nSince FAs and CEs are typically represented as real-valued vectors, computed through various mechanisms, their value ranges are not inherently standardized. However, many metrics either explicitly require the explanans to lie within a fixed range or implicitly assume comparability across explanantia, making normalization a necessary preprocessing step. A widely used normalization method is Min-Max Scaling (Binder et al., 2023; Brandt et al., 2023), which maps all values into a fixed interval (typically [0 , 1]). Alternative strategies include normalization based on the square root of the average second-moment estimate, offering robustness to outliers and variance shifts (Binder et al., 2023). This limited selection can be extended through any suitable normalization approach.",
      "keywords": [
        "normalization",
        "scaling",
        "outliers",
        "metrics",
        "ranges",
        "vectors",
        "standardized",
        "variance"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 147,
      "paragraph": "B.2.3 Similarity Measures\nAcross various metrics, it is necessary to calculate similarities or distances between two explanantia, especially when concerned with the desideratum of Continuity and in metrics relying on GroundTruth evaluations. Throughout the reported literature, various approaches have been reported, which differ based on the type of explanation. While some measures directly compute similarity, others quantify distance or disparity. In this work, we adopt a similarity-based framing, either directly or by transforming distance measures, to ensure that higher values uniformly indicate greater explanatory agreement. Analogously, we can compute the similarity between explananda, adopting measures that are presented in the following.\nFor FA s, similarity may be measured using arbitrary inverted distance or loss measures (e.g., L p , MSE, cosine distance, Jensen-Shannon-Divergence, or Bhattacharyya Coefficient), potentially normalized (e.g., by standard deviation) [b] . Alternatively, rank correlation measures such as Spearman's or Kendall's Tau can be applied [c] .",
      "keywords": [
        "similarity",
        "similarities",
        "metrics",
        "explanatory",
        "analogously",
        "explanantia",
        "correlation",
        "explananda"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 148,
      "paragraph": "B.2.3 Similarity Measures\nWhen binarizing FA outputs through thresholding, feature-wise evaluation measures such as accuracy, precision, F 1 , or AUROC are commonly used [d] . Similarly, IoU can be calculated over binarized features [e] , or topk intersection measures may be used [f] . For saliency maps, more specialized similarity measures are available, such as SSIM [g] , Earth Movers Distance (Park et al., 2018; Wu & Mooney, 2018), Normalized Cross Correlation (Baumgartner et al., 2018; Bass et al., 2020), or Mutual Information (Sun et al., 2023). While primarily established for FAs, similar similarity functions can be naturally applied to CE -based explanations as well.\nFor WBS s and ExE s, the choice of similarity measure depends strongly on the underlying model or domain. For linear predictive models, coefficient mismatch is a common choice (Lakkaraju et al., 2020), whereas rule- and tree-based explanantia may be compared by their rule overlap, feature usage, or node structures [h] .",
      "keywords": [
        "saliency",
        "binarizing",
        "similarity",
        "thresholding",
        "predictive",
        "accuracy",
        "features",
        "explanations"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 149,
      "paragraph": "B.2.3 Similarity Measures\nNLE s can be compared using standard natural language processing measures [i] . Those include for instance BLEU (Papineni et al., 2002), METEOR (Banerjee & Lavie, 2005), ROUGE (Lin, 2004), CIDEr (Vedantam et al., 2015), or SPICE (Anderson et al., 2016). In addition, several measures have",
      "keywords": [
        "similarity",
        "compared",
        "nle",
        "language",
        "measures",
        "bleu",
        "processing",
        "spice"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 150,
      "paragraph": "B.2.3 Similarity Measures\nb Alvarez-Melis & Jaakkola (2018a;b); Chu et al. (2018); Wu & Mooney (2018); Jain & Wallace (2019); Jia et al. (2019); Mitsuhara et al. (2019); Pope et al. (2019); Trokielewicz et al. (2019); Yeh et al. (2019); Zhang et al. (2019a); Jia et al. (2020); Agarwal et al. (2022a); Atanasova et al. (2022); Dai et al. (2022); Fouladgar et al. (2022); Agarwal et al. (2023); Huang et al. (2023a); Nematzadeh et al. (2023)",
      "keywords": [
        "similarity",
        "measures",
        "jia",
        "atanasova",
        "fouladgar",
        "huang",
        "zhang",
        "mitsuhara"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 151,
      "paragraph": "B.2.3 Similarity Measures\nc Das et al. (2017); Adebayo et al. (2018); Chen et al. (2019b); Dombrowski et al. (2019); Ghorbani et al. (2019); Nguyen & Martínez (2020); Rajapaksha et al. (2020); Sanchez-Lengeling et al. (2020); Liu et al. (2021a); Yin et al. (2021); Krishna et al. (2022); Huang et al. (2023a)\nd Chen et al. (2018a); Yang et al. (2018b); Jia et al. (2019; 2020); Sanchez-Lengeling et al. (2020); Bykov et al. (2021); Joshi et al. (2021); Park & Wallraven (2021); Amoukou et al. (2022); Chen et al. (2022); Funke et al. (2022); Tjoa & Guan (2022); Wilming et al. (2022); Agarwal et al. (2023)",
      "keywords": [
        "similarity",
        "jia",
        "yang",
        "chen",
        "measures",
        "huang",
        "yin",
        "liu"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 152,
      "paragraph": "B.2.3 Similarity Measures\ne Oramas et al. (2017); Fan et al. (2020); Kim et al. (2021); Situ et al. (2021); Vermeire et al. (2022)\nf Ghorbani et al. (2019); Mishra et al. (2020); Rajapaksha et al. (2020); Warnecke et al. (2020); Amparore et al. (2021); Bajaj et al. (2021)\ng Adebayo et al. (2018); Dombrowski et al. (2019); Rebuffi et al. (2020); Graziani et al. (2021); Sun et al. (2023)\nh\nBastani et al. (2017); Guidotti et al. (2019); Lakkaraju et al. (2020); Rajapaksha et al. (2020); Margot & Luta (2021)",
      "keywords": [
        "similarity",
        "bajaj",
        "ghorbani",
        "amparore",
        "measures",
        "vermeire",
        "bastani",
        "mishra"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 154,
      "paragraph": "B.2.3 Similarity Measures\nj Alvarez-Melis & Jaakkola (2018a;b); Yeh et al. (2019); Yin et al. (2021); Fouladgar et al. (2022)",
      "keywords": [
        "similarity",
        "measures",
        "yin",
        "fouladgar",
        "melis",
        "2018a",
        "yeh",
        "2022"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 155,
      "paragraph": "B.3.I Explanans-Centric\nThe following metrics directly evaluate the explanans, potentially looking into the data inputs but without having any access to the underlying model.",
      "keywords": [
        "metrics",
        "explanans",
        "evaluate",
        "data",
        "model",
        "inputs",
        "underlying",
        "centric"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 156,
      "paragraph": "Desiderata: Parsimony Explanation Type: FA, ExE, WBS, (CE, NLE)\nReferences: (51)",
      "keywords": [
        "parsimony",
        "desiderata",
        "ce",
        "references",
        "type",
        "explanation",
        "wbs",
        "fa"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 157,
      "paragraph": "Desiderata: Parsimony Explanation Type: FA, ExE, WBS, (CE, NLE)\nCraven & Shavlik (1995); Stefanowski & Vanderpooten (2001); Nauck (2003); Alonso et al. (2008); Augasta & Kathirvalavakumar (2012); Lakkaraju et al. (2016); Samek et al. (2016); Zilke et al. (2016); Lakkaraju et al. (2017); Guidotti et al. (2018); Hara & Hayashi (2018); Rustamov & Klosowski (2018); Wang et al. (2018b); Wang (2018); Wang et al. (2018a); Wu et al. (2018a;b); Deng (2019); Evans et al. (2019); Fong et al. (2019); Guidotti et al. (2019); Ignatiev et al. (2019); Lakkaraju et al. (2019); Polato & Aiolli (2019); Pope et al. (2019); Shakerin & Gupta (2019); Slack et al.",
      "keywords": [
        "parsimony",
        "desiderata",
        "augasta",
        "polato",
        "shavlik",
        "gupta",
        "alonso",
        "ce"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 158,
      "paragraph": "Desiderata: Parsimony Explanation Type: FA, ExE, WBS, (CE, NLE)\n(2019); Topin & Veloso (2019); Verma & Ganguly (2019); Wang (2019); Yoo & Sael (2019); Bhatt et al. (2020); Chalasani et al. (2020); Molnar et al. (2020); Nguyen & Martínez (2020); Panigutti et al. (2020); Rajapaksha et al. (2020); Rawal & Lakkaraju (2020); Stepin et al. (2020); Warnecke et al. (2020); Wu et al. (2020); Liu et al. (2021b); Margot & Luta (2021); Moradi & Samwald (2021); Poppi et al. (2021); Rosenfeld (2021); Samek et al. (2021); Dai et al. (2022); Funke et al. (2022); Huang et al. (2023b); Stevens & De Smedt (2024)",
      "keywords": [
        "parsimony",
        "desiderata",
        "verma",
        "chalasani",
        "bhatt",
        "panigutti",
        "sael",
        "veloso"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 160,
      "paragraph": "Desiderata: Parsimony Explanation Type: FA, ExE, WBS, (CE, NLE)\nA generally applicable method is to compute the file size (in bytes) of the explanans, based on the assumption that sparse explanantia can be more easily compressed (Samek et al., 2016; 2021).\nIn FA , size is typically based on the number of relevant features. This may be computed via:\n- · The L 0 norm [ c ] ,\n- • Normalization by input dimensionality, e.g., in graph settings (Pope et al., 2019),\n- · A count of features exceeding a relevance threshold [ d ] ,\n- • Integration over multiple thresholds to form a size curve (Warnecke et al., 2020), or\n- · Threshold-free statistics like entropy (Samek et al., 2016; Bhatt et al., 2020; Funke et al., 2022) and Gini index (Chalasani et al., 2020).",
      "keywords": [
        "sparse",
        "thresholds",
        "dimensionality",
        "normalization",
        "parsimony",
        "features",
        "compressed",
        "compute"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 161,
      "paragraph": "Desiderata: Parsimony Explanation Type: FA, ExE, WBS, (CE, NLE)\nWhile not common in the literature, for CE , similar counting mechanisms may be applied. One may count concepts exceeding a relevance threshold or use the total number of tested concepts (which is not tied to input size).\nFor NLE , the number of words or sentences provides a straightforward measure of size.\nIn ExE , size naturally corresponds to the number of examples used in the explanans (Nguyen & Martínez, 2020; Huang et al., 2023b).",
      "keywords": [
        "parsimony",
        "concepts",
        "relevance",
        "examples",
        "explanans",
        "threshold",
        "words",
        "sentences"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 162,
      "paragraph": "Desiderata: Parsimony Explanation Type: FA, ExE, WBS, (CE, NLE)\nIn general, size can be measured per-instance or aggregated over the dataset. For instance, one can report the number of predicates in the rule explaining a single instance (Augasta & Kathirvalavakumar, 2012), or aggregate the number of predicates across a global rule set with statistics such as the average (Lakkaraju et al., 2016; 2017; 2019), sum (Margot & Luta, 2021; Moradi & Samwald, 2021), or maximum (Moradi & Samwald, 2021). Similarly, rule counts can be aggregated over classes (e.g., average number per class (Nauck, 2003)). Optional adjustments include applying a tolerance margin (e.g., max(0 , | e | -k ) (Rosenfeld, 2021)) or aggregating over multiple class-specific explanantia per instance (Pope et al., 2019).",
      "keywords": [
        "predicates",
        "parsimony",
        "aggregated",
        "explanantia",
        "dataset",
        "aggregate",
        "rule",
        "statistics"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 163,
      "paragraph": "Desiderata: Parsimony Explanation Type: FA, ExE, WBS, (CE, NLE)\na Craven & Shavlik (1995); Alonso et al. (2008); Guidotti et al. (2018); Hara & Hayashi (2018); Wu et al. (2018b); Evans et al. (2019); Slack et al. (2019); Yoo & Sael (2019); Molnar et al. (2020); Rawal & Lakkaraju (2020); Wu et al. (2020)",
      "keywords": [
        "parsimony",
        "desiderata",
        "alonso",
        "shavlik",
        "ce",
        "guidotti",
        "sael",
        "rawal"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 164,
      "paragraph": "Desiderata: Parsimony Explanation Type: FA, ExE, WBS, (CE, NLE)\nb Craven & Shavlik (1995); Stefanowski & Vanderpooten (2001); Nauck (2003); Alonso et al. (2008); Augasta & Kathirvalavakumar (2012); Lakkaraju et al. (2016); Zilke et al. (2016); Lakkaraju et al. (2017); Wang (2018); Wu et al. (2018a); Deng (2019); Lakkaraju et al. (2019); Polato & Aiolli (2019); Shakerin & Gupta (2019); Wang (2019); Panigutti et al. (2020); Rajapaksha et al. (2020); Stepin et al. (2020); Margot & Luta (2021); Moradi & Samwald (2021); Rosenfeld (2021)",
      "keywords": [
        "parsimony",
        "desiderata",
        "augasta",
        "polato",
        "gupta",
        "shavlik",
        "panigutti",
        "lakkaraju"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 165,
      "paragraph": "Desiderata: Parsimony Explanation Type: FA, ExE, WBS, (CE, NLE)\nc Wang et al. (2018a;b); Fong et al. (2019); Polato & Aiolli (2019); Verma & Ganguly (2019); Nguyen & Martínez (2020); Poppi et al. (2021); Rosenfeld (2021); Stevens & De Smedt (2024)\nd Ignatiev et al. (2019); Pope et al. (2019); Warnecke et al. (2020); Liu et al. (2021b); Dai et al. (2022)",
      "keywords": [
        "parsimony",
        "desiderata",
        "verma",
        "polato",
        "stevens",
        "ce",
        "dai",
        "2022"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 168,
      "paragraph": "Metric I.3: Explanans Cohesion\nFirst, Fong & Vedaldi (2017) assess the locality of the explanans: a higher degree of cohesion is achieved when relevant information lies within a small, contiguous region of interest, making the explanans more condensed and easier to interpret. In saliency maps, this can be quantified by computing the area of the smallest bounding box that encloses the thresholded explanans. Second, Saifullah et al. (2024) assess the smoothness of the explanans: a more continuous This can be measured by summing the absolute differences in attribution between neighboring features (e.g., in x- and y-directions for images).\nBoth variants are applicable to data with ordered features and extend naturally to temporal domains. While originally proposed for FAs, these metrics can also be applied to CE when concept-based saliency maps are available (e.g., as presented by Lucieri et al. (2020)).",
      "keywords": [
        "saliency",
        "metrics",
        "cohesion",
        "maps",
        "smoothness",
        "metric",
        "features",
        "thresholded"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 169,
      "paragraph": "Metric I.4: Minimality\nDesiderata: Parsimony, Plausibility Explanation Type: ExE References: (25) Tolomei et al. (2017); Wachter et al. (2017); Karlsson et al. (2018); Zhang et al. (2018c); Guidotti et al. (2019); Albini et al. (2020); Artelt & Hammer (2020); Dandl et al. (2020); Kanamori et al. (2020); Karimi et al. (2020); Le et al. (2020); Mothilal et al. (2020); Pawelczyk et al. (2020); Ramon et al. (2020); Sharma et al. (2020);",
      "keywords": [
        "minimality",
        "metric",
        "parsimony",
        "desiderata",
        "kanamori",
        "tolomei",
        "guidotti",
        "dandl"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 170,
      "paragraph": "Metric I.4: Minimality\nAbrate & Bonchi (2021); Hvilshøj et al. (2021); Pawelczyk et al. (2021); Rasouli & Yu (2021); Van Looveren & Klaise (2021); Albini et al. (2022); Chou et al. (2022); Bayrak & Bach (2023a); Huang et al. (2023b); Verma et al. (2024)\nTo ensure that counterfactuals are understandable and believable, the induced changes should be minimal. This can be assessed by evaluating both the proximity of the counterfactual to the original instance and the sparsity of the changes.\nSparsity , in contrast, concerns the number of features changed rather than the extent of change. It is often quantified via the L 0 norm, that is, the number (or fraction) of altered features [ b ] .",
      "keywords": [
        "sparsity",
        "counterfactuals",
        "minimality",
        "features",
        "counterfactual",
        "contrast",
        "changes",
        "altered"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 171,
      "paragraph": "Metric I.4: Minimality\nProximity is commonly measured as the distance δ x, z ( ) between the counterfactual z and the original instance x . A variety of distance measures are used, with the choice significantly impacting the results (Wachter et al., 2017; Artelt & Hammer, 2020; Bayrak & Bach, 2023a; Verma et al., 2024) Common options include (feature-wise weighted) L p distances (especially L 2 ) [ a ] , as well as Jaccard or cosine distance (Tolomei et al., 2017), or combinations thereof (Karimi et al., 2020; Hvilshøj et al., 2021). Other notable choices are Mahalanobis distance (Artelt & Hammer, 2020; Kanamori et al., 2020; Verma et al., 2024), Gower distance (Dandl et al., 2020; Karimi et al., 2020), or feature-wise cumulative density-based distances (Pawelczyk et al., 2020). Dataset-specific alternatives include computing the quantile-shift per feature (Albini et al., 2022). Different data",
      "keywords": [
        "distances",
        "metric",
        "distance",
        "proximity",
        "measures",
        "minimality",
        "dataset",
        "measured"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 172,
      "paragraph": "Metric I.4: Minimality\ntypes require suitable distance measures, e.g.: for graphs, the symmetric difference of adjacency matrices or cosine similarity between node features (Abrate & Bonchi, 2021); for images, the inverse SSIM score (Sharma et al., 2020); and for time series, distances measured per time step (Karlsson et al., 2018).\nBridging sparsity and proximity, measures like L 1 (Wachter et al., 2017) or elastic-net regularization (Van Looveren & Klaise, 2021) are sometimes used. Depending on the representation, alternative definitions may apply, for example, counting the number of changed rules in rulebased counterfactuals (Guidotti et al., 2019).\na Wachter et al. (2017); Artelt & Hammer (2020); Le et al. (2020); Mothilal et al. (2020); Sharma et al. (2020); Rasouli & Yu (2021); Chou et al. (2022); Verma et al. (2024)",
      "keywords": [
        "sparsity",
        "metric",
        "measures",
        "regularization",
        "minimality",
        "distances",
        "similarity",
        "adjacency"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 173,
      "paragraph": "Metric I.4: Minimality\nb Albini et al. (2020); Dandl et al. (2020); Le et al. (2020); Mothilal et al. (2020); Ramon et al. (2020); Pawelczyk et al. (2021); Bayrak & Bach (2023a); Verma et al. (2024)",
      "keywords": [
        "metric",
        "minimality",
        "albini",
        "dandl",
        "verma",
        "mothilal",
        "pawelczyk",
        "2023a"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 174,
      "paragraph": "Metric I.5: Autoencoder Plausibility\nPlausibility\nDesiderata:\nExplanation Type:\nExE\nVan Looveren & Klaise (2021)\nReferences: (1)\nAutoencoders are trained to capture the underlying structure of the dataset. Based on this idea, Van Looveren & Klaise (2021) propose evaluating the plausibility of counterfactual examples via reconstruction errors from class-specific and general-purpose autoencoders.\nTwo specific scores are introduced:",
      "keywords": [
        "autoencoders",
        "autoencoder",
        "counterfactual",
        "dataset",
        "plausibility",
        "class",
        "errors",
        "examples"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 175,
      "paragraph": "Metric I.5: Autoencoder Plausibility\n- •\n- IM1 (Class-specific reconstruction comparison): This metric compares how well a counterfactual z is reconstructed by an autoencoder trained on the target class (Φ y ∗ z ) versus the originally predicted class (Φ ˆ y x ): ‖ z -Φ y z ∗ ( z ) ‖ 2 2 ‖ z -Φ ˆ yx ( z ) ‖ 2 2 + /epsilon1 . A low IM1 score implies that the counterfactual is more representative of the target\n- · (General manifold plausibility): This score compares the reconstructions from a\n- class than of its original class, indicating class-specific plausibility.\n- IM2 general autoencoder Φ Y and a class-specific one: ‖ Φ ( ) Y z -Φ y z ∗ ( z ) ‖ 2 2 ‖ z ‖ 1 + /epsilon1 . A low IM2 score implies that the counterfactual aligns well with both the general data manifold and the target class distribution, thus indicating higher plausibility.",
      "keywords": [
        "autoencoder",
        "manifold",
        "counterfactual",
        "metric",
        "predicted",
        "class",
        "reconstructions",
        "epsilon1"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 176,
      "paragraph": "Metric I.5: Autoencoder Plausibility\nWhile these metrics provide valuable insight into how realistic or semantically valid counterfactuals are, they come with caveats. Training multiple autoencoders (e.g., per class) introduces\nsignificant computational overhead. Further, reconstruction quality may vary across classes, and Hvilshøj et al. (2021) showed that autoencoders can be sensitive to small perturbations, potentially undermining the consistency of the plausibility assessment.",
      "keywords": [
        "autoencoders",
        "autoencoder",
        "counterfactuals",
        "undermining",
        "plausibility",
        "computational",
        "classes",
        "class"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 177,
      "paragraph": "Metric I.6: Diversity\n```\nDesiderata: Plausibility Explanation Type: ExE References: (6) Wachter et al. (2017); Karimi et al. (2020); Mothilal et al. (2020); Nguyen & Martínez (2020); Stepin et al. (2020); Verma et al. (2024)\n```\nIn cases where multiple counterfactuals are generated for a single instance, they should be diverse to offer distinct and meaningful alternatives (Wachter et al., 2017; Stepin et al., 2020; Verma et al., 2024).\nDiversity is typically quantified by computing pairwise distances among the set of counterfactuals, either as the average pairwise distance (Mothilal et al., 2020; Nguyen & Martínez, 2020), or as the number of counterfactual pairs that exceed a predefined distance threshold (Karimi et al., 2020).",
      "keywords": [
        "diversity",
        "counterfactuals",
        "diverse",
        "counterfactual",
        "distinct",
        "pairwise",
        "plausibility",
        "alternatives"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 178,
      "paragraph": "Metric I.7: Input Similarity\n```\nDesiderata: Plausibility Explanation Type: ExE References: (11) Laugel et al. (2019a;b); Mahajan et al. (2019); Singla et al. (2019); Artelt & Hammer (2020); Dandl et al. (2020); Kanamori et al. (2020); Delaney et al. (2021); Pawelczyk et al. (2021); Rasouli & Yu (2021); Smyth & Keane (2022)\n```\nTo ensure that counterfactuals remain realistic and trustworthy, they should lie close to the true data manifold. Counterfactuals that deviate significantly from the distribution of training data are unlikely to be plausible. Two principal approaches are used to evaluate this alignment: direct estimation of data conformity and distance-based proximity to the training data.",
      "keywords": [
        "similarity",
        "counterfactuals",
        "data",
        "metric",
        "alignment",
        "proximity",
        "distance",
        "manifold"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 179,
      "paragraph": "Metric I.7: Input Similarity\nAlternatively, proximity can be evaluated through distance-based approaches, relying on any suitable distance measure (or inverted similarity function, see Subsection B.2.3). These include distances to the k nearest neighbors in the training data (Laugel et al., 2019a; Dandl et al., 2020), or to the nearest training instances that share the same class label as the counterfactual or the original instance (Rasouli & Yu, 2021; Smyth & Keane, 2022). Specific domains may require tailored metrics such as the Fréchet Inception Distance (FID) for images (Singla et al., 2019).",
      "keywords": [
        "metrics",
        "similarity",
        "metric",
        "proximity",
        "distances",
        "distance",
        "nearest",
        "measure"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 180,
      "paragraph": "Metric I.7: Input Similarity\nIn the direct approach , several statistical or unsupervised methods determine whether the counterfactual is an outlier. This includes calculating its likelihood under a kernel density estimator (Artelt & Hammer, 2020) or from known data distributions in synthetic setups (Mahajan et al., 2019). Other methods include Local Outlier Factor (Kanamori et al., 2020; Delaney et al., 2021) and Isolation Forests (Delaney et al., 2021), which can be applied either across the full dataset or restricted to samples of the counterfactual's target class (Artelt & Hammer, 2020). Pawelczyk et al. (2021) assess how many of a counterfactual's nearest neighbors share its class label.",
      "keywords": [
        "outlier",
        "unsupervised",
        "similarity",
        "counterfactual",
        "dataset",
        "statistical",
        "estimator",
        "forests"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 181,
      "paragraph": "Metric I.7: Input Similarity\nRaw distance scores are often used directly (Dandl et al., 2020), but some methods normalize distances, for example, by comparing the counterfactual-input distance δ x, z ( ) to average distances between random pairs δ x , x ( ′ ′′ ) in the dataset (Laugel et al., 2019a;b), or to the original instance-counterfactual distance (Smyth & Keane, 2022).",
      "keywords": [
        "similarity",
        "distances",
        "metric",
        "distance",
        "normalize",
        "comparing",
        "dataset",
        "input"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 183,
      "paragraph": "Metric I.9: Actionability\nPlausibility, Fidelity\nDesiderata:\nExplanation Type:\nExE\nReferences: (7)\nMahajan et al. (2019); Karimi et al. (2020); Le et al. (2020); Pawelczyk et al. (2021); Ma et al. (2022); Smyth & Keane (2022); Verma et al. (2024)\nFor a counterfactual to be helpful, it should only introduce changes that are feasible or allowed. This is often enforced by defining explicit constraints, such as:\n- · Immutable features (e.g., sex, age) (Karimi et al., 2020; Pawelczyk et al., 2021; Smyth & Keane, 2022; Verma et al., 2024), or\n- · Value boundaries for individual features (Karimi et al., 2020; Le et al., 2020; Ma et al., 2022; Smyth & Keane, 2022).",
      "keywords": [
        "counterfactual",
        "features",
        "actionability",
        "constraints",
        "immutable",
        "feasible",
        "changes",
        "enforced"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 184,
      "paragraph": "Metric I.9: Actionability\nThe actionability of a counterfactual set can then be assessed by calculating the fraction of counterfactuals that satisfy all constraints [ a ] , or by computing the harmonic mean of satisfied constraints per counterfactual (Mahajan et al., 2019).\na Karimi et al. (2020); Le et al. (2020); Pawelczyk et al. (2021); Ma et al. (2022); Smyth & Keane (2022)",
      "keywords": [
        "counterfactuals",
        "counterfactual",
        "actionability",
        "constraints",
        "metric",
        "set",
        "assessed",
        "2022"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 188,
      "paragraph": "Metric I.11: Input Coverage\nDesiderata: Coverage Explanation Type: FA, ExE, WBS, (CE,NLE References: (8) Lakkaraju et al. (2016; 2017; 2019); Ribeiro et al. (2018); Rawal & Lakkaraju (2020); Warnecke et al. (2020);\nMoradi & Samwald (2021); Huang et al. (2023b)\nAn explanation method should ideally be capable of generating a valid explanans for every input instance, regardless of its position on the data manifold. To assess this, we calculate the fraction of inputs for which the method provides a valid result.\nThe notion of 'valid result' is intentionally broad and depends on the explanation type and the use case. A few prominent definitions include:",
      "keywords": [
        "explanans",
        "explanation",
        "coverage",
        "inputs",
        "data",
        "manifold",
        "assess",
        "type"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 189,
      "paragraph": "Metric I.11: Input Coverage\n- · For perturbation-based FAs like LIME (see Ribeiro et al. (2016)), a valid explanans can be given if a sufficient number of perturbed samples belong to the target or opposite class, enabling a reliable local approximation (Warnecke et al., 2020).\n- · For global ExEs, coverage may be defined as the number of inputs that have at least one sufficiently similar explaining instance within a pre-defined distance (Huang et al., 2023b).\n- · For rule-based or tree-based WBSs, coverage is the number of instances that are captured by at least one rule or decision path (Lakkaraju et al., 2016; 2017; 2019; Ribeiro et al., 2018; Rawal & Lakkaraju, 2020), optionally restricted to rules associated with the correct class (Moradi & Samwald, 2021).",
      "keywords": [
        "coverage",
        "wbss",
        "inputs",
        "fas",
        "explanans",
        "instances",
        "samples",
        "approximation"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 192,
      "paragraph": "B.3.II Model Observation\nThis set of metrics requires access to the model to obtain information about the model's output or its internal activations.",
      "keywords": [
        "metrics",
        "activations",
        "model",
        "observation",
        "output",
        "information",
        "internal",
        "obtain"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 193,
      "paragraph": "Metric II.14: Input Mutual Information\nParsimony\nDesiderata:\nExplanation Type:\nFA,CE\nReferences: (1)\nNguyen & Martínez (2020)\nWhen high-level features are provided by an explanans, they should ideally represent an abstracted, human-understandable form of the input, omitting unnecessary detail. This promotes parsimony by simplifying the input space. Such features can be obtained either through feature selection in FAs or through concept-level representations in CEs.\nTo quantify this abstraction, Nguyen & Martínez (2020) propose measuring the Mutual Information (MI) (see (Cover, 1999)) between the input and the explanans: MI ( x, e θ,x,y ˆ ) A lower mutual information score suggests that the explanans captures less granular input detail and thus constitutes a more compact and parsimonious representation.",
      "keywords": [
        "abstraction",
        "representations",
        "explanans",
        "features",
        "representation",
        "parsimony",
        "information",
        "input"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 195,
      "paragraph": "Metric II.15: Output Contrastivity\nClass discriminativeness is commonly measured by comparing explanantia generated for different classes on the same input. Several comparison setups have been proposed: between the most and least likely classes (Li et al., 2020c; Rebuffi et al., 2020), the top predicted vs. a randomly chosen class (Sixt et al., 2020), or simply between the two classes in binary settings (Pope et al., 2019).\nAlthough the reported implementations target FA, the idea of class-discriminative explanations can be naturally extended to other type, wherever explanations can be generated for multiple class hypotheses.",
      "keywords": [
        "discriminativeness",
        "contrastivity",
        "discriminative",
        "classes",
        "class",
        "output",
        "explanantia",
        "explanations"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 196,
      "paragraph": "Metric II.16: Output Similarity\nDesiderata: Plausibility\nExplanation Type:\nExE\nPlumb et al. (2020)\nReferences:\n(1)\nTo ensure plausibility, counterfactuals should yield output distribution similar to real instances of the target class. Plumb et al. (2020) evaluate whether each counterfactual z matches the output activation of at least one training sample, i.e.: ∃ x ′ ∈ X y ∗ z : δ ( θ x ( ′ ) , θ ( z ) ) < /epsilon1 This confirms that the counterfactual aligns with typical model behavior for that class.",
      "keywords": [
        "counterfactuals",
        "counterfactual",
        "similarity",
        "output",
        "plausibility",
        "similar",
        "epsilon1",
        "sample"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 197,
      "paragraph": "Metric II.17: Mutual Coherence\nDesiderata: Plausibility, (Fidelity) Explanation Type: FA, (ExE, CE, WBS, NLE) References: (19)\nSelvaraju et al. (2017); Guo et al. (2018a); Ancona et al. (2019); Fernando et al. (2019); Fusco et al. (2019); Jain & Wallace (2019); Zhang et al. (2019a); Marques-Silva et al. (2020); Nguyen & Martínez (2020); Wang et al. (2020b); Warnecke et al. (2020); Graziani et al. (2021); Malik et al. (2021); Rajbahadur et al. (2021); Krishna et al. (2022); Mercier et al. (2022); Jin et al. (2023); Duan et al. (2024); Tekkesinoglu & Pudas (2024)",
      "keywords": [
        "coherence",
        "metric",
        "mutual",
        "fidelity",
        "plausibility",
        "nle",
        "guo",
        "ce"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 198,
      "paragraph": "Metric II.17: Mutual Coherence\nSeveral authors propose to evaluate their explanations by comparing them against other XAI methods that are assumed to produce trustworthy or well-understood outputs. Although only reported for FAs, this approach is applicable to any explanation type, as long as a suitable reference XAI method is available and an appropriate similarity metric can be defined.\nOther works compute similarity across multiple explanation methods, assuming that high agreement between methods reflects convergence toward a reliable explanans [ c ] . Any suitable similarity measure may be applied (see Subsection B.2.3 for an overview).\nA common reference is the Shapley value framework (see Shapley (1953)), often operationalized via SHAP (see Lundberg (2017)) due to its solid theoretical grounding. FAs are frequently compared to SHAP estimates using similarity measures [ a ] . In the context of saliency maps, Image Occlusion (see Zeiler & Fergus (2013)) is similarly treated as a proxy ground truth (Selvaraju et al., 2017). Comparisons against further explanation methods are also reported in literature [ b ] .",
      "keywords": [
        "saliency",
        "coherence",
        "explanations",
        "similarity",
        "explanation",
        "xai",
        "maps",
        "shap"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 199,
      "paragraph": "Metric II.17: Mutual Coherence\nHowever, this evaluation approach is not without criticism. Kumar et al. (2020) caution that validating one explanation method using another may propagate shared biases or assumptions, providing limited evidence for actual correctness.\n- a Ancona et al. (2019); Zhang et al. (2019a); Malik et al. (2021); Jin et al. (2023); Tekkesinoglu & Pudas (2024)\nb Guo et al. (2018a); Fernando et al. (2019); Fusco et al. (2019); Marques-Silva et al. (2020); Nguyen & Martínez (2020) c Jain & Wallace (2019); Wang et al. (2020b); Warnecke et al. (2020); Graziani et al. (2021); Rajbahadur\n- et al. (2021); Krishna et al. (2022); Mercier et al. (2022)",
      "keywords": [
        "coherence",
        "explanation",
        "correctness",
        "biases",
        "validating",
        "evidence",
        "evaluation",
        "propagate"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 200,
      "paragraph": "Metric II.18: Significance Check\nDesiderata:\nFidelity\nExplanation Type: FA, CE, (ExE, WBS, NLE)\nReferences: (15)\nSamek et al. (2016); Adel et al. (2018); Chen et al. (2018a); Kim et al. (2018); Chen et al. (2019a); DeYoung et al. (2019); Gu et al. (2019); Wickramanayake et al. (2019); Nam et al. (2020); Hemamou et al. (2021); Park & Wallraven (2021); Pornprasit et al. (2021); Agarwal et al. (2022c); Hameed et al. (2022); Bommer et al. (2024)\nStatistical significance testing is a common strategy for verifying whether an explanation is meaningfully different from random or naïve baseline explanations. This approach is frequently applied to FAs [ a ] .",
      "keywords": [
        "statistical",
        "explanations",
        "significance",
        "explanation",
        "fas",
        "verifying",
        "fa",
        "testing"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 202,
      "paragraph": "Metric II.18: Significance Check\na Samek et al. (2016); Chen et al. (2018a; 2019a); DeYoung et al. (2019); Gu et al. (2019); Wickramanayake et al. (2019); Nam et al. (2020); Hemamou et al. (2021); Park & Wallraven (2021); Pornprasit et al. (2021); Agarwal et al. (2022c); Hameed et al. (2022); Bommer et al. (2024)",
      "keywords": [
        "metric",
        "check",
        "significance",
        "2022c",
        "ii",
        "gu",
        "agarwal",
        "2018a"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 204,
      "paragraph": "Metric II.20: Prediction Validity\nFidelity\nDesiderata:\nExplanation Type:\nExE\nReferences: (18)\nWachter et al. (2017); Karlsson et al. (2018); Dhurandhar et al. (2019); Guidotti et al. (2019); Mahajan et al. (2019); Dandl et al. (2020); Le et al. (2020); Molnar (2020); Mothilal et al. (2020); Nguyen & Martínez (2020); Pedapati et al. (2020); Pawelczyk et al. (2021); Rasouli & Yu (2021); Ma et al. (2022); Tan et al. (2022); Vermeire et al. (2022); Guidotti (2024); Verma et al. (2024)\n/negationslash",
      "keywords": [
        "prediction",
        "validity",
        "negationslash",
        "fidelity",
        "metric",
        "2022",
        "2024",
        "desiderata"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 205,
      "paragraph": "Metric II.20: Prediction Validity\nMost authors assess the fraction of generated counterfactuals that meet this condition [ a ] . To capture more nuance, authors also propose measuring the model's confidence in the target class for the counterfactual (Rasouli & Yu, 2021), or using a continuous loss between the predicted and target class probabilities, such as the L 1 distance (Dandl et al., 2020).\nBy definition, a counterfactual must result in a different prediction from the original input. For untargeted counterfactuals, this simply requires ˆ y z = ˆ , while targeted counterfactuals must y x satisfy ˆ y z = y ∗ z (Wachter et al., 2017; Molnar, 2020; Guidotti, 2024).\nFor factual explanations, the class should remain unchanged, i.e., ˆ y z = ˆ , which can also be y x verified either binary (Dhurandhar et al., 2019) or using loss-based similarity measures (Nguyen & Martínez, 2020).",
      "keywords": [
        "counterfactuals",
        "counterfactual",
        "prediction",
        "predicted",
        "validity",
        "class",
        "nuance",
        "similarity"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 206,
      "paragraph": "Metric II.20: Prediction Validity\na Karlsson et al. (2018); Dhurandhar et al. (2019); Guidotti et al. (2019); Mahajan et al. (2019); Le et al. (2020); Mothilal et al. (2020); Pedapati et al. (2020); Pawelczyk et al. (2021); Ma et al. (2022); Tan et al. (2022); Vermeire et al. (2022); Verma et al. (2024)",
      "keywords": [
        "prediction",
        "metric",
        "2022",
        "2024",
        "2021",
        "2020",
        "validity",
        "2019"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 208,
      "paragraph": "Metric II.22: Output Faithfulness\nFidelity\nDesiderata:\nExplanation Type:\nWBS\nReferences: (34)",
      "keywords": [
        "faithfulness",
        "output",
        "metric",
        "fidelity",
        "wbs",
        "references",
        "desiderata",
        "ii"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 209,
      "paragraph": "Metric II.22: Output Faithfulness\nAndrews et al. (1995); Craven & Shavlik (1995); Stefanowski & Vanderpooten (2001); Barakat et al. (2010); Augasta & Kathirvalavakumar (2012); Zilke et al. (2016); Bastani et al. (2017); Krishnan & Wu (2017); Lakkaraju et al. (2017); Guo et al. (2018b); Laugel et al. (2018); Peake & Wang (2018); Plumb et al. (2018); Tan et al. (2018); Wu et al. (2018a); Zhang et al. (2018b); Chen et al. (2019a); Guidotti et al. (2019); Kanehira & Harada (2019); Lakkaraju et al. (2019); Zhou et al. (2019); Anders et al. (2020); Hatwell et al. (2020); Lakkaraju et al. (2020); Panigutti et al. (2020); Pedapati et al. (2020); Rajapaksha",
      "keywords": [
        "metric",
        "faithfulness",
        "output",
        "kanehira",
        "guo",
        "krishnan",
        "2020",
        "zhang"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 210,
      "paragraph": "Metric II.22: Output Faithfulness\net al. (2020); Rawal & Lakkaraju (2020); Amparore et al. (2021); Chen et al. (2021); Moradi & Samwald (2021); Pornprasit et al. (2021); Bayrak & Bach (2023b); Bo et al. (2024)\nAsurrogate model should closely mimic the behavior of the original black-box model. Therefore, a common evaluation strategy is to compare the outputs of the surrogate and black-box models using a similarity or performance measure.\nFor imbalanced datasets, Moradi & Samwald (2021) calculate fidelity per class, using either the true labels or the black-box predictions as reference. For individual rules, fidelity may be expressed as the rule's precision (Stefanowski & Vanderpooten, 2001; Hatwell et al., 2020).",
      "keywords": [
        "imbalanced",
        "predictions",
        "similarity",
        "mimic",
        "datasets",
        "faithfulness",
        "fidelity",
        "metric"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 211,
      "paragraph": "Metric II.22: Output Faithfulness\nThis can be assessed with standard measures such as accuracy, F 1 -score, MSE, or L p distances [ a ] , or with similarity measures such as SSIM, Pearson correlation, or KL divergence (Chen et al., 2019a; Anders et al., 2020). Arbitrary loss functions may also be used (Amparore et al., 2021).\nWhen evaluating local surrogates, Local Output Fidelity is defined by computing fidelity within a neighborhood of the input instance (Laugel et al., 2018; Plumb et al., 2018; Guidotti et al., 2019; Rajapaksha et al., 2020), or within a synthetic neighborhood (Panigutti et al., 2020; Pornprasit et al., 2021). For unseen data, Lakkaraju et al. (2020) validate explanations by comparing outputs to those of the nearest training instances.",
      "keywords": [
        "accuracy",
        "similarity",
        "output",
        "outputs",
        "metric",
        "faithfulness",
        "nearest",
        "synthetic"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 212,
      "paragraph": "Metric II.22: Output Faithfulness\na Andrews et al. (1995); Craven & Shavlik (1995); Barakat et al. (2010); Augasta & Kathirvalavakumar (2012); Zilke et al. (2016); Bastani et al. (2017); Krishnan & Wu (2017); Lakkaraju et al. (2017); Guo et al. (2018b); Laugel et al. (2018); Plumb et al. (2018); Tan et al. (2018); Zhang et al. (2018b); Guidotti et al. (2019); Kanehira & Harada (2019); Lakkaraju et al. (2019); Zhou et al. (2019); Anders et al. (2020); Lakkaraju et al. (2020); Panigutti et al. (2020); Pornprasit et al. (2021); Bayrak & Bach (2023b)",
      "keywords": [
        "metric",
        "faithfulness",
        "output",
        "kanehira",
        "krishnan",
        "2023b",
        "2021",
        "bastani"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 213,
      "paragraph": "Metric II.23: Internal Faithfulness\nWBS\nDesiderata:\nExplanation Type:\nFidelity\nReferences: (3)\nMessalas et al. (2019); Anders et al. (2020); Amparore et al. (2021)\nSince the WBS models can achieve similar predictive performance as the original black-box without relying on the same underlying reasoning (Messalas et al., 2019; Anders et al., 2020), it is essential to evaluate their internal fidelity, meaning the similarity in how both models justify their predictions. This can be achieved by comparing post-hoc explanations of the\noriginal and surrogate model for the same inputs. Using feature attribution methods (e.g., SHAP from Lundberg (2017)), a typical approach is to measure the average overlap of the topk features between both models' explanantia (Messalas et al., 2019). Other similarity metrics and explanation types may also be used.",
      "keywords": [
        "predictive",
        "similarity",
        "metrics",
        "wbs",
        "predictions",
        "faithfulness",
        "comparing",
        "features"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 214,
      "paragraph": "Metric II.23: Internal Faithfulness\nAlternatively, Amparore et al. (2021) compare counterfactuals generated from each model, treating their similarity as a proxy for the alignment of decision boundaries. This provides a structural view of how well the surrogate captures the black-box model's rationale beyond mere output agreement.",
      "keywords": [
        "counterfactuals",
        "faithfulness",
        "similarity",
        "structural",
        "model",
        "alignment",
        "internal",
        "rationale"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 217,
      "paragraph": "Metric II.25: Hyperparameter Sensitivity\n```\nDesiderata: Consistency Explanation Type: FA, (ExE, CE, WBS, NLE) References: (6) Chen et al. (2019a); Verma & Ganguly (2019); Bansal et al. (2020); Mishra et al. (2020); Sanchez-Lengeling et al. (2020); Graziani et al. (2021)\n```",
      "keywords": [
        "hyperparameter",
        "sensitivity",
        "metric",
        "consistency",
        "ce",
        "chen",
        "nle",
        "25"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 218,
      "paragraph": "Metric II.25: Hyperparameter Sensitivity\nThis metric evaluates how robust an explanation method is to changes in its configuration. Since many XAI methods rely on hyperparameters, high sensitivity may indicate instability, making tuning more difficult and reducing user trust in the method (Bansal et al., 2020). A common approach is to generate explanantia for the same input across different hyperparameter settings and measure the similarity between them [ a ] . Alternatively, Chen et al. (2019a) propose to compare the stability of performance metrics such as fidelity over varying hyperparameters. Beyond identifying general sensitivity, Graziani et al. (2021) use this technique to guide hyperparameter selection: by progressively adjusting hyperparameters and tracking when the generated explanantia converge, one can identify stable regions in the hyperparameter space.\nAlthough most work focuses on FAs, the general idea is applicable to any method with tunable hyperparameters.\na Verma & Ganguly (2019); Bansal et al. (2020); Mishra et al. (2020); Sanchez-Lengeling et al. (2020); Graziani et al. (2021)",
      "keywords": [
        "hyperparameters",
        "hyperparameter",
        "xai",
        "explanantia",
        "explanation",
        "sensitivity",
        "metrics",
        "robust"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 219,
      "paragraph": "Metric II.26: Execution Time\nDesiderata:\nEfficiency\nExplanation Type:\nFA, ExE, WBS, (CE, NLE)",
      "keywords": [
        "efficiency",
        "execution",
        "metric",
        "exe",
        "ce",
        "time",
        "ii",
        "fa"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 220,
      "paragraph": "References: (34)\nRoss et al. (2017); Ribeiro et al. (2018); Zhang et al. (2018c); Chen et al. (2019a); Cheng et al. (2019); Fusco et al. (2019); Ignatiev et al. (2019); Shakerin & Gupta (2019); Slack et al. (2019); Topin & Veloso (2019); Albini et al. (2020); Guo et al. (2020); Marques-Silva et al. (2020); Rajapaksha et al. (2020); Ramon et al. (2020); Warnecke et al. (2020); Abrate & Bonchi (2021); Bajaj et al. (2021); Faber et al. (2021); Lin et al. (2021); Malik et al. (2021); Pawelczyk et al. (2021); Rasouli & Yu (2021); Van Looveren & Klaise (2021); Wang et al. (2021); Amoukou et al. (2022); Belaid et al. (2022); Ma et al.",
      "keywords": [
        "zhang",
        "guo",
        "wang",
        "chen",
        "lin",
        "cheng",
        "shakerin",
        "ribeiro"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 221,
      "paragraph": "References: (34)\n(2022); Mercier et al. (2022); Vermeire et al. (2022); Bayrak & Bach (2023a); Brandt et al. (2023); Jin et al. (2023); Verma et al. (2024)\nMost authors measure the time required to generate an explanans for a fixed input, model, and dataset, on specified hardware [ a ] .\nIn addition to empirical runtime, some authors analyze algorithmic complexity using O -notation to characterize the worst-case or average-case number of steps required to run an explanation [ b ] . Chen et al. (2019a) further evaluate parallelizability to understand potential runtime improvements through hardware acceleration or distributed computation.",
      "keywords": [
        "explanans",
        "complexity",
        "computation",
        "explanation",
        "parallelizability",
        "algorithmic",
        "runtime",
        "analyze"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 222,
      "paragraph": "References: (34)\na Ross et al. (2017); Ribeiro et al. (2018); Zhang et al. (2018c); Cheng et al. (2019); Fusco et al. (2019); Ignatiev et al. (2019); Shakerin & Gupta (2019); Guo et al. (2020); Marques-Silva et al. (2020); Rajapaksha et al. (2020); Ramon et al. (2020); Warnecke et al. (2020); Bajaj et al. (2021); Faber et al. (2021); Lin et al. (2021); Malik et al. (2021); Pawelczyk et al. (2021); Rasouli & Yu (2021); Van Looveren & Klaise (2021); Wang et al. (2021); Amoukou et al. (2022); Belaid et al. (2022); Ma et al. (2022); Mercier et al. (2022); Vermeire et al. (2022); Bayrak & Bach (2023a); Brandt et al.",
      "keywords": [
        "zhang",
        "wang",
        "guo",
        "silva",
        "2022",
        "lin",
        "ribeiro",
        "shakerin"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 223,
      "paragraph": "References: (34)\n(2023); Jin et al. (2023); Verma et al. (2024)\nSlack et al. (2019); Topin & Veloso (2019); Albini et al. (2020); Abrate & Bonchi (2021); Malik et al. (2021);\nb Van Looveren & Klaise (2021)",
      "keywords": [
        "2023",
        "2024",
        "2021",
        "topin",
        "34",
        "2020",
        "verma",
        "veloso"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 224,
      "paragraph": "B.3.III Input Intervention\nThe metrics listed in this group all require changes to the input, which is then presented to the model again and potentially explained once more.",
      "keywords": [
        "input",
        "metrics",
        "intervention",
        "changes",
        "iii",
        "model",
        "require",
        "presented"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 225,
      "paragraph": "Metric III.27: Unguided Perturbation Fidelity\n```\nDesiderata: Fidelity Explanation Type: FA References: (10) Ancona et al. (2017); Shrikumar et al. (2017); Sundararajan et al. (2017); Alvarez-Melis & Jaakkola (2018b); Arya et al. (2019); Cheng et al. (2019); Yeh et al. (2019); Zhang et al. (2019b); Bhatt et al. (2020); Elkhawaga et al. (2023)\n```\nSome of the most influential XAI metrics evaluate how well an explanans aligns with observed model behavior when the input is perturbed. Together, these metrics assess whether the explanation faithfully captures how the model responds to its inputs.",
      "keywords": [
        "xai",
        "perturbation",
        "perturbed",
        "metrics",
        "explanans",
        "explanation",
        "model",
        "inputs"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 226,
      "paragraph": "Metric III.27: Unguided Perturbation Fidelity\nComplementing these are completeness-based approaches such as Completeness (Sundararajan et al., 2017) and Summation-to-Delta (Shrikumar et al., 2017), which assume that the explanans must fully account for the model's behavior. These compare the difference in output between an instance and a baseline (e.g., a fully perturbed input) with the sum of the attributions across",
      "keywords": [
        "perturbation",
        "perturbed",
        "metric",
        "completeness",
        "fidelity",
        "attributions",
        "complementing",
        "model"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 227,
      "paragraph": "Metric III.27: Unguided Perturbation Fidelity\nThis includes Sensitivity-n from Ancona et al. (2017), Infidelity from Yeh et al. (2019), and Faithfulness presented by Alvarez-Melis & Jaakkola (2018b) and Arya et al. (2019). In these approaches, input perturbations are introduced either randomly across all features (Yeh et al., 2019), by zeroing features individually or in small groups (Alvarez-Melis & Jaakkola, 2018b; Arya et al., 2019; Cheng et al., 2019; Bhatt et al., 2020), or by manipulating subsets of fixed size (Ancona et al., 2017). The model's change in prediction is then compared to the explanans, using different strategies: directly against the raw attribution vector, against a version scaled by perturbation magnitude, or by summing attributions of the changed features. The deviation is measured using standard metrics like Pearson correlation [ a ] or mean squared error (Yeh et al., 2019).",
      "keywords": [
        "perturbations",
        "perturbation",
        "sensitivity",
        "metrics",
        "prediction",
        "features",
        "deviation",
        "correlation"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 228,
      "paragraph": "Metric III.27: Unguided Perturbation Fidelity\nchanged features. Ideally, the two should match exactly. For practical purposes, however, the relative deviation between attribution sum and output delta can be used as a softer criterion.\na Ancona et al. (2017); Alvarez-Melis & Jaakkola (2018b); Arya et al. (2019); Cheng et al. (2019); Bhatt et al. (2020)",
      "keywords": [
        "perturbation",
        "deviation",
        "features",
        "metric",
        "delta",
        "changed",
        "relative",
        "fidelity"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 229,
      "paragraph": "Metric III.28: Guided Perturbation Fidelity\nFidelity\nDesiderata:\nExplanation Type:\nFA,CE\nReferences: (75)",
      "keywords": [
        "perturbation",
        "guided",
        "metric",
        "fidelity",
        "desiderata",
        "fa",
        "iii",
        "references"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 230,
      "paragraph": "Metric III.28: Guided Perturbation Fidelity\nBach et al. (2015); Samek et al. (2016); Ancona et al. (2017); Dabkowski & Gal (2017); Shrikumar et al. (2017); Chattopadhay et al. (2018); Chu et al. (2018); Guo et al. (2018a;b); Liu et al. (2018b); Nguyen (2018); Petsiuk (2018); Wang et al. (2018b); Yang et al. (2018a); Annasamy & Sycara (2019); Arya et al. (2019); Brahimi et al. (2019); DeYoung et al. (2019); Fong et al. (2019); Kanehira et al. (2019); Kapishnikov et al. (2019); Lin et al. (2019); Pope et al. (2019); Schlegel et al. (2019); Serrano & Smith (2019); Wagner et al. (2019); Yuan et al. (2019); Ayush et al. (2020);",
      "keywords": [
        "perturbation",
        "metric",
        "guided",
        "yang",
        "dabkowski",
        "guo",
        "fidelity",
        "liu"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 231,
      "paragraph": "Metric III.28: Guided Perturbation Fidelity\nBrunke et al. (2020); Carton et al. (2020); Chen et al. (2020); Cong et al. (2020); Fan et al. (2020); Hsieh et al. (2020); Nguyen et al. (2020); Pan et al. (2020b); Rieger & Hansen (2020); Schlegel et al. (2020); Schulz et al. (2020); Singh et al. (2020); Wang et al. (2020a); Warnecke et al. (2020); El Shawi et al. (2021); Ge et al. (2021); Jethani et al. (2021); Jung & Oh (2021); Liu et al. (2021b); Luss et al. (2021); Poppi et al. (2021); Singh et al. (2021); Situ et al. (2021); Sun et al. (2021); Velmurugan et al. (2021a;b); Vu et al. (2021); Wang et al. (2021); Yin et al.",
      "keywords": [
        "perturbation",
        "metric",
        "guided",
        "fidelity",
        "liu",
        "ge",
        "wang",
        "chen"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 232,
      "paragraph": "Metric III.28: Guided Perturbation Fidelity\n(2021); Albini et al. (2022); Atanasova et al. (2022); Dai et al. (2022); De Silva et al. (2022); Funke et al. (2022); Hameed et al. (2022); Müller et al. (2022); Ngai & Rudzicz (2022); Rong et al. (2022); Tan et al. (2022); Veerappa et al. (2022); Šimić et al. (2022); Zou et al. (2022); Agarwal et al. (2023); Alangari et al. (2023b); Jin et al. (2023); Schlegel & Keim (2023); Awal & Roy (2024)",
      "keywords": [
        "perturbation",
        "metric",
        "guided",
        "atanasova",
        "fidelity",
        "silva",
        "2023b",
        "dai"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 233,
      "paragraph": "Metric III.28: Guided Perturbation Fidelity\nA common and versatile strategy for evaluating explanation quality is to assess how perturbing features based on the explanans affects model predictions. This approach captures both the necessity (correctness) and sufficiency (completeness) of features highlighted by an explanans (Carton et al., 2020; DeYoung et al., 2019; Alangari et al., 2023b). Details on perturbation strategies are outlined in Subsection B.2.1.\nPerturbation Order: The order in which features are perturbed is critical. Most relevant first (MoRF) evaluates correctness: removing the most important features should lead to a sharp performance drop if the explanation correctly highlights necessary features [ e ] . Least relevant first (LeRF) tests completeness: performance should remain high if unimportant features are removed first [ f ] . Alternatively, some authors reverse the entire process by starting from a blank input and adding features incrementally [ g ] , which is functionally equivalent, with MoRF addition corresponding to LeRF deletion, and vice-versa (for an illustration see Figure 7).",
      "keywords": [
        "perturbing",
        "features",
        "perturbation",
        "perturbed",
        "explanation",
        "predictions",
        "performance",
        "explanans"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 234,
      "paragraph": "Metric III.28: Guided Perturbation Fidelity\nPerturbation Scope: Perturbations can be applied either in a fixed-size or iterative manner. Fixed-size perturbations change a predefined amount of information at once [ a ] . The amount of removed features can be selected as a fixed number [ b ] or based on a threshold over cumulative importance scores [ c ] . Iterative perturbation gradually removes features one by one or in batches [ d ] .\nMeasure Score: The performance change is measured using various scoring mechanisms. These include raw prediction difference θ x, y ( ˆ) -θ x, y ( ˙ ˆ) [ h ] , normalized or relative variants [ i ] , or evaluation of loss and accuracy metrics [ j ] . Some works use statistical distances such as Kullback-Leibler divergence (Liu et al., 2021b; Agarwal et al., 2023), Kendall's Tau (Singh et al., 2020; 2021), or Pearson correlation (Poppi et al., 2021).",
      "keywords": [
        "metrics",
        "accuracy",
        "perturbation",
        "prediction",
        "perturbations",
        "metric",
        "features",
        "distances"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 235,
      "paragraph": "Metric III.28: Guided Perturbation Fidelity\nAggregation: For iterative perturbations, the results can be aggregated as the average performance change (Samek et al., 2016; Ancona et al., 2017), the area over the perturbation curve (AOPC) (Samek et al., 2016; Brahimi et al., 2019; DeYoung et al., 2019; Rieger & Hansen, 2020), or the area under it (AUPC) (Petsiuk, 2018; Ngai & Rudzicz, 2022; Šimić et al., 2022; Jin et al., 2023). The area between MoRF and LeRF (ABPC) may also be computed (Nguyen et al., 2020; Schulz et al., 2020), optionally with decay weighting to emphasize early changes (Šimić et al., 2022).",
      "keywords": [
        "perturbations",
        "perturbation",
        "iterative",
        "guided",
        "performance",
        "metric",
        "weighting",
        "changes"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 236,
      "paragraph": "Metric III.28: Guided Perturbation Fidelity\nBaselines: To contextualize results, explanation can be compared against various baselines, including random feature selections [ k ] , zero-inputs (Schulz et al., 2020), or naïve explanation such as edge detectors (Hooker et al., 2019). Advanced setups may compare against models trained on random labels or with inserted irrelevant features to bound the quality of explanation (Hameed et al., 2022).",
      "keywords": [
        "features",
        "baselines",
        "contextualize",
        "perturbation",
        "explanation",
        "feature",
        "detectors",
        "quality"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 237,
      "paragraph": "Variants:\n- · Evaluating explanantia across all classes per instance, not only the predicted class (Pope et al., 2019; Awal & Roy, 2024).\n- · Normalizing the performance drop by the input difference to mitigate distribution shift: δ ( θ x ,θ x ( ) ( ˙ ) ) δ x,x ( ˙ ) (Ge et al., 2021; Schlegel & Keim, 2023).\n- · Sanity checks verifying whether the perturbation curve behaves monotonically (Arya et al., 2019; Fong et al., 2019; Luss et al., 2021), or whether MoRF always performs worse than LeRF (Šimić et al., 2022).\n- · Plotting performance over remaining entropy rather than number of perturbed features (Kapishnikov et al., 2019).\n- · Training with randomized feature masking to improve model stability, although this may reduce causality fidelity (Jethani et al., 2021).\nAlternatives: Further, we may change the perspective through alternative formulations:",
      "keywords": [
        "perturbation",
        "perturbed",
        "predicted",
        "morf",
        "features",
        "entropy",
        "class",
        "feature"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 238,
      "paragraph": "Variants:\n- · Counting the minimum number of features needed to change the model's prediction (in MoRF deletion) (Nguyen, 2018) or using differences between class-specific explanantia to guide perturbations (Shrikumar et al., 2017).\n- · Replacing fixed perturbations with adversarial optimization over k features to evaluate minimum change necessary for altering predictions (Hsieh et al., 2020; Vu et al., 2021).\n- · Treating perturbed inputs as counterfactuals and applying metrics such as Minimality (see Metric 4) (Fan et al., 2020; Ge et al., 2021; Albini et al., 2022).\nThe approach may be extended to CEs by either mapping concepts to features (e.g., conceptbased saliency maps by Lucieri et al. (2020)), or perturbing internal activations at the concept layer (El Shawi et al., 2021).",
      "keywords": [
        "saliency",
        "perturbing",
        "altering",
        "conceptbased",
        "adversarial",
        "perturbations",
        "prediction",
        "ces"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 239,
      "paragraph": "Variants:\na Guo et al. (2018b); DeYoung et al. (2019); Warnecke et al. (2020); Jethani et al. (2021); Jung & Oh (2021); Velmurugan et al. (2021a;b); Wang et al. (2021); De Silva et al. (2022); Agarwal et al. (2023)\nb Bach et al. (2015); Samek et al. (2016); Ancona et al. (2017); Chu et al. (2018); Schlegel et al. (2019); Nguyen et al. (2020); Alangari et al. (2023b); Schlegel & Keim (2023)\nc Brahimi et al. (2019); Pope et al. (2019); Schlegel et al. (2019); Ngai & Rudzicz (2022); Schlegel & Keim (2023)",
      "keywords": [
        "velmurugan",
        "alangari",
        "2021a",
        "ngai",
        "variants",
        "2018",
        "silva",
        "2023b"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 240,
      "paragraph": "Variants:\nd Bach et al. (2015); Samek et al. (2016); Ancona et al. (2017); Petsiuk (2018); DeYoung et al. (2019); Rieger & Hansen (2020); Jin et al. (2023)\ne Bach et al. (2015); Samek et al. (2016); Chu et al. (2018); DeYoung et al. (2019); Pope et al. (2019); Schlegel et al. (2019); Carton et al. (2020); Rieger & Hansen (2020); Ngai & Rudzicz (2022); Alangari et al. (2023b); Jin et al. (2023); Schlegel & Keim (2023)",
      "keywords": [
        "deyoung",
        "variants",
        "pope",
        "ngai",
        "2018",
        "2023b",
        "samek",
        "2017"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 241,
      "paragraph": "Variants:\nf Bach et al. (2015); Ancona et al. (2017); Dabkowski & Gal (2017); Guo et al. (2018b); Annasamy & Sycara (2019); DeYoung et al. (2019); Wagner et al. (2019); Carton et al. (2020); Singh et al. (2020); Wang et al. (2020a); Jethani et al. (2021); Liu et al. (2021b); Singh et al. (2021); Wang et al. (2021); Dai et al. (2022); De Silva et al. (2022); Alangari et al. (2023b)\ng Petsiuk (2018); Arya et al. (2019); Kapishnikov et al. (2019); Luss et al. (2021); Situ et al. (2021); Sun et al. (2021); Atanasova et al. (2022); Funke et al. (2022); Tan et al. (2022)",
      "keywords": [
        "liu",
        "atanasova",
        "alangari",
        "singh",
        "wang",
        "2018",
        "2022",
        "guo"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 242,
      "paragraph": "Variants:\nh Bach et al. (2015); Chu et al. (2018); DeYoung et al. (2019); Kapishnikov et al. (2019); Chen et al. (2020); Cong et al. (2020); Schulz et al. (2020); Ngai & Rudzicz (2022)\ni Chattopadhay et al. (2018); Kapishnikov et al. (2019); Schulz et al. (2020); Jung & Oh (2021); Velmurugan et al. (2021a;b); Ngai & Rudzicz (2022)\nj Bach et al. (2015); Chu et al. (2018); Kapishnikov et al. (2019); Lin et al. (2019); Schlegel et al. (2019); Serrano & Smith (2019); Wagner et al. (2019); Warnecke et al. (2020); Sun et al. (2021); Wang et al. (2021); De Silva et al. (2022); Schlegel & Keim (2023)",
      "keywords": [
        "chen",
        "jung",
        "serrano",
        "wang",
        "velmurugan",
        "cong",
        "variants",
        "ngai"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 243,
      "paragraph": "Variants:\nk Samek et al. (2016); Schlegel et al. (2019); Schlegel & Keim (2023); Serrano & Smith (2019); Jin et al. (2023)\nPerformance\n- (a) Deletion perturbs features iteratively.\nPerformance\n- (b) Insertion restores features iteratively.\nFigure 7: Examples for ideal Perturbation Curves to evaluate feature attributions. Insertion and Deletion approaches are equivalent when switching MoRF for LeRF ordering and vice versa. Deletion MoRF (Insertion LeRF) can be used to evaluate correctness, and Insertion MoRF (Deletion LeRF) to evaluate completeness.",
      "keywords": [
        "features",
        "feature",
        "deletion",
        "attributions",
        "restores",
        "variants",
        "insertion",
        "perturbs"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 245,
      "paragraph": "Metric III.30: Prediction Neighborhood Continuity\nFidelity, Continuity\nDesiderata:\nExplanation Type:\nWBS\nReferences: (1)\nLakkaraju et al. (2020)\nA WBS is only useful if its behavior remains consistent with the black-box even under slight perturbations to the input.\nA lower difference indicates a more robust surrogate, as it preserves faithfulness across perturbations. High differences may signal that the surrogate captures only superficial model behavior or overfits to specific input patterns.\nTo assess this, Lakkaraju et al. (2020) propose measuring the difference in Output Faithfulness (see Metric 22) between the original and perturbed inputs. Specifically, for each input a perturbed variant is created (e.g., through noise or adversarial modification). The metric then compares the agreement between the black-box model and the surrogate model before and after the perturbations",
      "keywords": [
        "adversarial",
        "prediction",
        "robust",
        "continuity",
        "consistent",
        "perturbations",
        "faithfulness",
        "perturbed"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 248,
      "paragraph": "Metric III.32: Neighborhood Continuity\nDesiderata: Continuity Explanation Type: FA, ExE, WBS, NLE, (CE)\nReferences: (19)\nAlvarez-Melis & Jaakkola (2018a;b); Chu et al. (2018); Honegger (2018); Yeh et al. (2019); Zhang et al. (2019a); Artelt & Hammer (2020); Fan et al. (2020); Lakkaraju et al. (2020); Artelt et al. (2021); Bajaj et al. (2021); Situ et al. (2021); Yin et al. (2021); Agarwal et al. (2022a); Atanasova et al. (2022); Fouladgar et al. (2022); Agarwal et al. (2023); Bayrak & Bach (2023a); Tekkesinoglu & Pudas (2024)",
      "keywords": [
        "continuity",
        "metric",
        "neighborhood",
        "situ",
        "desiderata",
        "atanasova",
        "alvarez",
        "chu"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 249,
      "paragraph": "Metric III.32: Neighborhood Continuity\nNeighborhood Continuity evaluates whether explanantia remain similar for similar explananda. The underlying assumption is that small changes in the input should not lead to disproportionately large differences in the output, thereby increasing user trust through Continuity.\nThe choice of neighborhood depends on the domain and explanation use case. Some approaches define neighborhoods using fixed-radius input distances or k -nearest neighbors (Chu et al., 2018; Situ et al., 2021; Fouladgar et al., 2022; Tekkesinoglu & Pudas, 2024), while others\nThe main distinction between proposed metrics lies in how 'similar instances' are defined. Some classic metrics include Local Lipschitz Stability from Alvarez-Melis & Jaakkola (2018a;b), Sensitivity from Yeh et al. (2019), and RIS/RRS/ROS from Agarwal et al. (2022a).\nrestrict similarity to instances sharing the same predicted label (Honegger, 2018; Fan et al., 2020).",
      "keywords": [
        "metrics",
        "similarity",
        "metric",
        "continuity",
        "distances",
        "neighborhoods",
        "nearest",
        "lipschitz"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 250,
      "paragraph": "Metric III.32: Neighborhood Continuity\nExplanans similarity is typically calculated using distance or correlation-based metrics (see Subsection B.2.3) and may be normalized by input distance (Alvarez-Melis & Jaakkola, 2018a;b; Agarwal et al., 2022a), or based on distances in model activation space (Agarwal et al., 2022a; 2023).\nTo generate similar inputs , perturbation-based strategies are often used (see Subsection B.2.1). Perturbations may be bounded in magnitude [ a ] or derived from domain-specific semantics (Zhang et al., 2019a; Yin et al., 2021; Fouladgar et al., 2022). Some metrics require that perturbed inputs preserve the original prediction ˆ y x (Alvarez-Melis & Jaakkola, 2018a;b; Agarwal et al., 2022a), or maintain similar logits θ x ( ) (Agarwal et al., 2023).",
      "keywords": [
        "similarity",
        "metrics",
        "prediction",
        "perturbations",
        "perturbation",
        "similar",
        "distances",
        "distance"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 251,
      "paragraph": "Metric III.32: Neighborhood Continuity\nVariations : Fan et al. (2020) compare similarity for nearby vs. distant inputs , while Zhang et al. (2019a) restrict comparisons to unchanged features to reduce noise from perturbations. Although originally proposed for FAs, this concept is broadly transferable to other explanation types. For instance, Lakkaraju et al. (2020) evaluate WBSs by comparing surrogate models trained on perturbed data, using model-specific similarity measures such as coefficient mismatch for linear models.\na Alvarez-Melis & Jaakkola (2018a;b); Yeh et al. (2019); Bajaj et al. (2021); Agarwal et al. (2022a); Atanasova et al. (2022)",
      "keywords": [
        "similarity",
        "features",
        "perturbations",
        "comparing",
        "comparisons",
        "compare",
        "distant",
        "perturbed"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 253,
      "paragraph": "Metric III.33: Adversarial Input Resilience\n- · Explanans manipulation : The explanans is altered while the prediction remains fixed (Dombrowski et al., 2019; Boopathy et al., 2020; Kuppa & Le-Khac, 2020; Huang et al., 2023a).\n- · Prediction manipulation : The prediction changes while the explanans remains similar (Zhang et al., 2020; Huang et al., 2023a).\nEvaluation typically measures the distance between the adversarial output (explanans or prediction) and either its original or targeted counterpart. The input change is usually bounded. Performance is reported either as aggregated distance metrics (Dombrowski et al., 2019; Ghorbani et al., 2019; Zhang et al., 2020; Huang et al., 2023a) or as attack success rates based on predefined thresholds (Kuppa & Le-Khac, 2020; Zhang et al., 2020; Huang et al., 2023a). While the reported authors focus on FAs, the approach can be easily adapted to other explanation types by selecting appropriate similarity measures.",
      "keywords": [
        "adversarial",
        "prediction",
        "attack",
        "metrics",
        "resilience",
        "targeted",
        "explanans",
        "output"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 254,
      "paragraph": "B.3.IV Model Intervention\nFor some categories of metrics, it is necessary to change the underlying model, e.g., through retraining or weight randomization.",
      "keywords": [
        "retraining",
        "metrics",
        "intervention",
        "change",
        "randomization",
        "iv",
        "model",
        "categories"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 256,
      "paragraph": "Metric IV.34: Model Parameter Randomization Test\nMost commonly, the weights of a neural network are randomized either entirely, by layer, or iteratively in top-down or bottom-up order. The similarity between the original and randomized explanantia is then computed (Adebayo et al., 2018; Hedström et al., 2024). Similarity may be evaluated using SSIM and correlation for heatmaps (Adebayo et al., 2018; Binder et al., 2023; Bommer et al., 2024), or any suitable similarity metric (see Subsection B.2.3). Hedström et al. (2024) additionally average the results across noisy inputs (e.g., via input perturbations) to stabilize local explanations.\nA complementary strategy proposed by Kindermans et al. (2019) keeps the weights fixed but internal computations and outputs remain unchanged. Since the model behavior is invariant by\napplies a controlled shift to the input distribution, adapting the input-layer biases such that design, any change in the explanans indicates unwanted sensitivity.\nThis shift-invariance test",
      "keywords": [
        "neural",
        "randomization",
        "heatmaps",
        "randomized",
        "invariance",
        "invariant",
        "perturbations",
        "layer"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 257,
      "paragraph": "Metric IV.34: Model Parameter Randomization Test\nAlthough originally proposed for FAs, the approach is applicable to any explanation type for which suitable similarity metrics can be defined.\ncan be quantified using standard similarity metrics between pre- and post-shift explanantia.",
      "keywords": [
        "randomization",
        "metrics",
        "similarity",
        "parameter",
        "model",
        "shift",
        "explanantia",
        "explanation"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 259,
      "paragraph": "Metric IV.35: Data Randomization Test\nThis test compares explanantia generated by a model trained on the randomized data to those from a model trained on correctly labeled data. A strong explanation method should yield low similarity between the two, as the latter explanantia reflect meaningful decision features, while the former do not. Similarity can be computed using SSIM, correlation (Adebayo et al., 2018), rank-based metrics such as Kendall's Tau (Sanchez-Lengeling et al., 2020), or other suitable measures (see Subsection B.2.3).",
      "keywords": [
        "randomization",
        "metrics",
        "randomized",
        "metric",
        "correlation",
        "data",
        "test",
        "labeled"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 260,
      "paragraph": "Metric IV.36: Retrained Model Evaluation\n```\nDesiderata: Fidelity Explanation Type: FA References: (10) Guo et al. (2019); Hooker et al. (2019); Cheng et al. (2020); Han et al. (2020); Schiller et al. (2020); Hemamou et al. (2021); Shah et al. (2021); Li et al. (2022); Khalane et al. (2023); Raval et al. (2023)\n```\nA key limitation of perturbation-based evaluation methods lies in their potential to introduce distribution shifts though the alteration of features. To mitigate this, two related strategies retrain models on the perturbed datasets.\nThe second strategy leverages explanations for knowledge distillation. Here, various authors train a new model on a dataset reduced to only the features deemed important by the explanation [ a ] .",
      "keywords": [
        "retrained",
        "retrain",
        "datasets",
        "perturbed",
        "alteration",
        "dataset",
        "perturbation",
        "features"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 261,
      "paragraph": "Metric IV.36: Retrained Model Evaluation\nThe first approach is most famously known as Remove and Retrain (ROAR) by Hooker et al. (2019), with related variants proposed by Han et al. (2020) and Shah et al. (2021). It builds on the perturbation strategies of Metric 28, but ROAR removes features (e.g., the most important ones according to the explanans) from the training data and then retrains the model from scratch on the altered dataset. Performance degradation of this retrained model, compared to the original model, is then used to infer the quality of the explanation: a faithful explanans should identify features whose removal severely affects performance.\nEvaluation follows the general structure of Metric 28, measuring how much predictive performance is retained when only the relevant features are used.\nWhile both ROAR and distillation-based setups are valuable for benchmarking explanation methods in a more robust setting, they do not directly assess the original explanans for a specific instance. Instead, they evaluate the utility of the explanation method by measuring the average informativeness of the features it selects.",
      "keywords": [
        "retrained",
        "predictive",
        "retrains",
        "explanans",
        "features",
        "explanation",
        "informativeness",
        "infer"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 262,
      "paragraph": "Metric IV.36: Retrained Model Evaluation\na Guo et al. (2019); Cheng et al. (2020); Schiller et al. (2020); Hemamou et al. (2021); Li et al. (2022); Khalane et al. (2023); Raval et al. (2023)",
      "keywords": [
        "retrained",
        "model",
        "evaluation",
        "guo",
        "metric",
        "cheng",
        "schiller",
        "iv"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 263,
      "paragraph": "Metric IV.37: Influence Fidelity\nDesiderata: Fidelity\nExplanation Type:\nExE\nReferences: (2)\nKrishnan & Wu (2017); Guo et al. (2020)\nTo evaluate the fidelity of ExEs, the model is retrained on a dataset modified according to the explanation. The effect of these modifications on model predictions is used to assess the explanatory quality.\nIn an alternative approach, Krishnan & Wu (2017) retain the identified influential instances but randomly flips the labels of all remaining training examples before retraining. If the explanation captures all relevant information, the prediction should remain stable. Hence, lower prediction variance after retraining indicates a more complete and accurate explanans.\nGuo et al. (2020) remove the identified influential training points from the dataset, retrain the model, and measure the change in loss for the explanandum. If the removed instances were truly helpful, model performance should degrade. Conversely, if they were misleading, removal should lead to an improvement. Thus, a greater change in loss signals a more correct and complete set of influential instances.",
      "keywords": [
        "prediction",
        "predictions",
        "influence",
        "influential",
        "dataset",
        "explanatory",
        "retrained",
        "retraining"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 264,
      "paragraph": "Metric IV.38: Normalized Movement Rate\nDesiderata: Continuity\nExplanation Type:\nFA\nSalih et al. (2022; 2024b)\nReferences: (2)\nTo assess the robustness of FAs in the presence of collinear or redundant features, Salih et al. (2022) introduce a metric that evaluates the stability of feature rankings as the most important features are iteratively removed and the model is retrained. This procedure mirrors the setup of the Remove and Retrain paradigm (Metric 36), but shifts the focus from prediction performance to the behavior of the explanation itself.\nAfter each retraining step, the explanans is recomputed, and the ranks of the remaining features are compared to the previous ones. A large shift in ranking indicates that redundant or weakly relevant features have taken over the role of more informative ones, suggesting that the\nattribution method lacks robustness in the face of redundancy and may not reflect the true rationale behind the model's prediction.",
      "keywords": [
        "features",
        "prediction",
        "feature",
        "retrained",
        "movement",
        "robustness",
        "retraining",
        "performance"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 265,
      "paragraph": "Metric IV.38: Normalized Movement Rate\nTo address this issue, Salih et al. (2024b) propose the Modified Informative Position (MIP), which aims to stabilize the interpretation by providing a more resilient ranking structure across iterative retraining steps.",
      "keywords": [
        "ranking",
        "position",
        "retraining",
        "mip",
        "movement",
        "metric",
        "normalized",
        "iterative"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 267,
      "paragraph": "Metric IV.39: Adversarial Model Resilience\n- · Targeted , where the modified model is encouraged to produce explanantia similar to a predefined target (Heo et al., 2019; Viering et al., 2019).\n- · Untargeted , where the goal is to produce explanantia that differ substantially from the original ones (Heo et al., 2019; Pruthi et al., 2019; Dimanov et al., 2020).",
      "keywords": [
        "adversarial",
        "targeted",
        "untargeted",
        "resilience",
        "model",
        "target",
        "explanantia",
        "modified"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 268,
      "paragraph": "Metric IV.39: Adversarial Model Resilience\nTo preserve prediction behavior, constraints may be imposed on the model modification. Dimanov et al. (2020) bound the change in prediction scores , but other restrictions are thinkable, such as limiting the weight difference norm to ensure the model remains close to the original. The success of the manipulation is measured depending on the attack setup. For targeted attacks, its the similarity between the manipulated explanans and the predefined target (Viering et al., 2019). Untargeted attacks are evaluated using the dissimilarity between the manipulated and original explanans (Dimanov et al., 2020; Pruthi et al., 2019). All approaches were originally introduced for FA, but by applying suitable similarity measures\n(see Subsection B.2.3), they can be extended to other explanation types as well.",
      "keywords": [
        "adversarial",
        "manipulated",
        "prediction",
        "manipulation",
        "targeted",
        "model",
        "untargeted",
        "attacks"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 269,
      "paragraph": "B.3.V A Priori Constrained\nThe final group of metrics is those that have additional requirements to the desideratum, namely the necessity for specific annotations in the dataset or using a specific type of models.",
      "keywords": [
        "metrics",
        "constrained",
        "requirements",
        "priori",
        "annotations",
        "desideratum",
        "necessity",
        "dataset"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 270,
      "paragraph": "Metric V.40: GT Dataset Evaluation\nDesiderata: Plausibility, Fidelity\nExplanation Type:\nFA, CE, NLE\nReferences: (119)",
      "keywords": [
        "dataset",
        "metric",
        "evaluation",
        "fidelity",
        "gt",
        "desiderata",
        "40",
        "nle"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 271,
      "paragraph": "Metric V.40: GT Dataset Evaluation\nHuman-Annotated Dataset (69): Simonyan et al. (2013); Cao et al. (2015); Lapuschkin et al. (2016); Zhou et al. (2016); Das et al. (2017); Fong & Vedaldi (2017); Selvaraju et al. (2017); Bargal et al. (2018); Baumgartner et al. (2018); Camburu et al. (2018); Chen et al. (2018a); Chuang et al. (2018); Jha et al. (2018); Liu et al. (2018a); Park et al. (2018); Poerner et al. (2018); Wang et al. (2018a); Wu & Mooney (2018); Zhang et al. (2018a); Bastings et al. (2019); Chen et al. (2019d;c); DeYoung et al. (2019); Fong et al. (2019); Kanehira & Harada (2019); Kanehira et al. (2019);",
      "keywords": [
        "dataset",
        "annotated",
        "metric",
        "chen",
        "evaluation",
        "zhang",
        "zhou",
        "camburu"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 272,
      "paragraph": "Metric V.40: GT Dataset Evaluation\nMitsuhara et al. (2019); Puri et al. (2019); Rajani et al. (2019); Shu et al. (2019); Sydorova et al. (2019); Taghanaki et al. (2019); Trokielewicz et al. (2019); Verma & Ganguly (2019); Wang & Nvasconcelos (2019); Wickramanayake et al. (2019); Zeng et al. (2019); Bass et al. (2020); Cheng et al. (2020); Li et al. (2020a); Liu et al. (2020); Nam et al. (2020); Pan et al. (2020a); Rio-Torto et al. (2020); Schulz et al. (2020); Subramanian et al. (2020); Sun et al. (2020); Wang & Vasconcelos (2020); Xu et al. (2020a;b); Bany Muhammad & Yeasin (2021); Barnett et al. (2021); Bykov et al. (2021);",
      "keywords": [
        "dataset",
        "metric",
        "nvasconcelos",
        "40",
        "torto",
        "2020a",
        "xu",
        "sydorova"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 273,
      "paragraph": "Metric V.40: GT Dataset Evaluation\nJang & Lukasiewicz (2021); Joshi et al. (2021); Mathew et al. (2021); Nguyen et al. (2021); Wiegreffe & Marasović (2021); Asokan et al. (2022);",
      "keywords": [
        "dataset",
        "metric",
        "40",
        "joshi",
        "2021",
        "2022",
        "asokan",
        "wiegreffe"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 274,
      "paragraph": "Metric V.40: GT Dataset Evaluation\nCui et al. (2022); Du et al. (2022); Mücke & Pfahler (2022); Theiner et al. (2022); Nematzadeh et al. (2023); Rasmussen et al. (2023); Ribeiro et al. (2023); Tritscher et al. (2023); Atanasova (2024b); Saifullah et al. (2024) Synthetic Dataset (50): Cortez & Embrechts (2013); Chen et al. (2017); Oramas et al. (2017); Ross et al. (2017); Chen et al. (2018b); Kim et al. (2018); Mascharka et al. (2018); Yang et al. (2018b); Antwarg et al. (2019); Arras et al. (2019); Camburu et al. (2019); Ismail et al. (2019); Jia et al. (2019); Lin et al. (2019); Subramanya et al. (2019);",
      "keywords": [
        "dataset",
        "metric",
        "50",
        "synthetic",
        "40",
        "atanasova",
        "chen",
        "ribeiro"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 275,
      "paragraph": "Metric V.40: GT Dataset Evaluation\nTakeishi (2019); Yang & Kim (2019); Ying et al. (2019); Amiri et al. (2020); Ismail et al. (2020); Jia et al. (2020); Kohlbrenner et al. (2020); Lucieri et al. (2020); Luo et al. (2020); Nguyen & Martínez (2020); Tritscher et al. (2020); Wang et al. (2020a); Bohle et al. (2021); Faber et al. (2021); Kim et al. (2021); Lin et al. (2021); Liu et al. (2021a); Shah et al. (2021); Yalcin et al. (2021); Agarwal et al. (2022b); Amoukou et al. (2022); Arias-Duart et al. (2022); Arras et al. (2022); Fan et al. (2022); Khakzar et al. (2022); Rao et al. (2022); Tjoa & Guan",
      "keywords": [
        "dataset",
        "metric",
        "rao",
        "40",
        "yang",
        "takeishi",
        "2020a",
        "jia"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 277,
      "paragraph": "Metric V.40: GT Dataset Evaluation\nSynthetic Datasets are constructed such that the relationship between input features and labels is controlled and known. Ideally, these datasets admit only a single valid rationale. Although this assumption is not always met in practice [ a ] . The ground-truth explanantia may either be explicitly specified, e.g., by indicating the features responsible for prediction [ b ] , or derived from the data-generation process itself, for instance using gradients or Shapley values [ c ] . We present selected examples in Figure 8.\nThe former is feasible with synthetic datasets that encode precisely one explanatory rationale. The latter is more common in practice but inherently less reliable, as black-box models may learn spurious correlations as shown by Ribeiro et al. (2016) or identify valid rationales beyond those provided by humans (Mücke & Pfahler, 2022; Ya et al., 2023).\nTypical strategies for synthetic data include:",
      "keywords": [
        "datasets",
        "dataset",
        "data",
        "explanatory",
        "prediction",
        "rationale",
        "rationales",
        "gradients"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 278,
      "paragraph": "Metric V.40: GT Dataset Evaluation\n- · Creating inputs from structured primitives (e.g., objects or shapes), where the target labels are deterministic functions of those primitives [ d ] .\n- · Performing adversarial attacks restricted to known feature subsets, which then serve as the explanatory evidence for prediction changes (Subramanya et al., 2019).\n- · Inserting backdoor triggers into inputs, which act as the decisive explanans for altered predictions [ e ] .\n- · Generating labels as noise-free functions over randomly sampled input features, commonly in tabular domains [ f ] .\n- · Constructing mosaic images or input grids where the rationale corresponds to a localized sub-region of the input [ g ] .\nAdvantages of synthetic datasets include reduced distribution shift when applying perturbationbased metrics (Metric 28) (Ismail et al., 2019; 2020; Hesse et al., 2023), support for concept-level manipulations (Yang & Kim, 2019; Lin et al., 2021; Hesse et al., 2023), and the inclusion of meaningful counterfactuals or rationale variants (Yang & Kim, 2019).",
      "keywords": [
        "adversarial",
        "datasets",
        "dataset",
        "prediction",
        "metrics",
        "synthetic",
        "predictions",
        "generating"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 280,
      "paragraph": "Evaluation :\nment with the intended rationale, thereby justifying a comparison between the generated explanantia and the ground-truth annotations. This comparison can be performed using any suitable similarity metric (see Subsection B.2.3). While FA evaluations often rely on featurelevel scoring, CE and NLEs require domain-appropriate measures.\nFor visual and natural language domains , the well-known Pointing Game checks whether the most highly attributed features lie within predefined ground-truth regions such as bounding boxes or key tokens [ j ] . Closely related is Weakly Supervised Localization , where the explanans is compared directly to segmentation masks or bounding boxes, typically using Intersection-overUnion (IoU) [ k ] . In multi-label or multi-object scenarios, further alignment can be tested by verifying whether the explanation focuses on the input features associated with the predicted label (Du et al., 2019).",
      "keywords": [
        "annotations",
        "segmentation",
        "evaluations",
        "localization",
        "evaluation",
        "pointing",
        "label",
        "visual"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 281,
      "paragraph": "Evaluation :\nIn the context of FAs , several dedicated evaluation strategies were reported. Precision and recall can be computed over truly important features to assess how well the explanation captures relevant inputs (Bastings et al., 2019). Other techniques evaluate the ranking of important and unimportant features (Chen et al., 2017; 2018b; Antwarg et al., 2019; Camburu et al., 2019), or quantify the symmetric difference between the sets of selected and annotated features (Nguyen & Martínez, 2020). A particularly common metric involves summing the attribution values over known important features, often with normalization or weighting adjustments [ i ] .\nFor CEs , evaluation typically involves test datasets that explicitly annotate the presence or absence of each concept, enabling precise assessment of concept identification accuracy (Kim et al., 2018; Lucieri et al., 2020; Asokan et al., 2022).\nFinally, to ensure that the evaluation isolates explanation quality from predictive accuracy, Fan et al. (2022) recommend to limit evaluation to those samples for which the model's prediction is both correct and sufficiently confident.",
      "keywords": [
        "predictive",
        "accuracy",
        "evaluation",
        "annotated",
        "assessment",
        "annotate",
        "prediction",
        "assess"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 282,
      "paragraph": "Evaluation :\nFor NLEs , comparisons to reference justifications rely on general natural language processing measures such as BLEU or ROUGE [ l ] , as well as similarity measures specifically designed for NLEs (Park et al., 2018; Du et al., 2022)\na Kim et al. (2018); Arras et al. (2019); Yang & Kim (2019); Tritscher et al. (2020); Faber et al. (2021) b Chen et al. (2017); Oramas et al. (2017); Ross et al. (2017); Chen et al. (2018b); Yang et al. (2018b); Ying et al. (2019); Luo et al. (2020); Faber et al. (2021); Kim et al. (2021); Tjoa & Guan (2022); Wilming et al. (2022); Zhou et al. (2022)\nc Cortez & Embrechts (2013); Jia et al. (2019; 2020); Liu et al. (2021a); Amoukou et al. (2022)",
      "keywords": [
        "justifications",
        "nles",
        "comparisons",
        "similarity",
        "reference",
        "language",
        "evaluation",
        "rely"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 283,
      "paragraph": "Evaluation :\nd Oramas et al. (2017); Ross et al. (2017); Kim et al. (2018); Yang et al. (2018b); Ying et al. (2019); Lucieri et al. (2020); Luo et al. (2020); Faber et al. (2021); Kim et al. (2021); Yalcin et al. (2021); Khakzar et al. (2022); Tjoa & Guan (2022); Agarwal et al. (2023); Hesse et al. (2023); Miró-Nicolau et al. (2023) e Lin et al. (2019; 2021); Fan et al. (2022); Sun et al. (2023); Ya et al. (2023)",
      "keywords": [
        "ying",
        "yang",
        "oramas",
        "luo",
        "lin",
        "lucieri",
        "guan",
        "2023"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 284,
      "paragraph": "Evaluation :\nf Cortez & Embrechts (2013); Chen et al. (2017; 2018b); Ismail et al. (2019); Jia et al. (2019); Amiri et al. (2020); Ismail et al. (2020); Jia et al. (2020); Nguyen & Martínez (2020); Tritscher et al. (2020); Liu et al. (2021a); Agarwal et al. (2022b); Amoukou et al. (2022)\ng Bohle et al. (2021); Shah et al. (2021); Arias-Duart et al. (2022); Rao et al. (2022; 2024)\nh Selvaraju et al. (2017); Kanehira et al. (2019); Wang & Nvasconcelos (2019); Cheng et al. (2020); Saifullah et al. (2024)",
      "keywords": [
        "nvasconcelos",
        "jia",
        "chen",
        "wang",
        "liu",
        "cortez",
        "nguyen",
        "cheng"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 285,
      "paragraph": "Evaluation :\ni Lapuschkin et al. (2016); Yang & Kim (2019); Kohlbrenner et al. (2020); Nam et al. (2020); Rio-Torto et al. (2020); Wang et al. (2020a); Xu et al. (2020a); Bohle et al. (2021); Kim et al. (2021); Arias-Duart et al. (2022); Arras et al. (2022); Zhou et al. (2022)\nj Bargal et al. (2018); Poerner et al. (2018); Zhang et al. (2018a); Fong et al. (2019); Sydorova et al. (2019); Taghanaki et al. (2019); Takeishi (2019); Schulz et al. (2020); Barnett et al. (2021); Arras et al. (2022); Theiner et al. (2022)",
      "keywords": [
        "zhang",
        "xu",
        "yang",
        "zhou",
        "wang",
        "lapuschkin",
        "sydorova",
        "arras"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 286,
      "paragraph": "Evaluation :\nk Simonyan et al. (2013); Cao et al. (2015); Zhou et al. (2016); Fong & Vedaldi (2017); Selvaraju et al. (2017); Wickramanayake et al. (2019); Bany Muhammad & Yeasin (2021); Nguyen et al. (2021); Fan et al. (2022) l Camburu et al. (2018); Chuang et al. (2018); Liu et al. (2018a); Wu & Mooney (2018); Chen et al. (2019d); Rajani et al. (2019); Wickramanayake et al. (2019); Li et al. (2020a); Sun et al. (2020); Jang & Lukasiewicz (2021); Wiegreffe & Marasović (2021); Ribeiro et al. (2023); Atanasova (2024b)\n(a) The an8Flower dataset (Oramas et al., 2017). flowers, Right: Ground-Truth explanans of rele-",
      "keywords": [
        "an8flower",
        "flowers",
        "rele",
        "dataset",
        "explanans",
        "sun",
        "fong",
        "atanasova"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 287,
      "paragraph": "Evaluation :\nLeft: Input images of with differing petals. vant region.\n(b) The ShapeGGen dataset (Agarwal et al., 2023).\nThe graph's label depends on relevant structures or motifs (e.g. house-shape) that serve as ground-truth explanans.\n(c) The SCDB dataset (Lucieri et al., 2020).\nThe label is defined by the combination of small objects inside the are of the larger one and serve as ground-truth concepts.\nFigure 8: Examples of synthetic datasets containing ground-truth explanantia of relevant features.",
      "keywords": [
        "datasets",
        "dataset",
        "shape",
        "shapeggen",
        "structures",
        "synthetic",
        "graph",
        "petals"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 288,
      "paragraph": "Metric V.41: White Box Model Check\nDesiderata: Fidelity Explanation Type: FA, (ExE, NLE) References: (12) Ribeiro et al. (2016); Antwarg et al. (2019); Dhurandhar et al. (2019); Jia et al. (2019); Zhou et al. (2019); Crabbe et al. (2020); Jia et al. (2020); Guidotti (2021); Velmurugan et al. (2021a); Dai et al. (2022); Brandt et al. (2023); Carmichael & Scheirer (2023)",
      "keywords": [
        "metric",
        "fidelity",
        "box",
        "white",
        "model",
        "check",
        "desiderata",
        "dai"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 291,
      "paragraph": "C Excluded Metrics\nWe excluded a total of four metrics from our framework. While initially identified during our literature review, we later dismissed them after closer inspection. Some do not evaluate the explanation itself, but rather the model or downstream tasks, while others rely on assumptions that are overly narrow or reward properties we consider misleading. For transparency, we briefly describe each and illustrate our reasons for exclusion below.",
      "keywords": [
        "metrics",
        "tasks",
        "reward",
        "framework",
        "evaluate",
        "model",
        "reasons",
        "rely"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 295,
      "paragraph": "References\n- Yilun Zhou, Serena Booth, Marco Tulio Ribeiro, and Julie Shah. Do feature attribution methods correctly attribute features? In Proceedings of the AAAI Conference on Artificial Intelligence , volume 36, pp. 9623-9633, 2022.\n- Zihan Zhou, Mingxuan Sun, and Jianhua Chen. A model-agnostic approach for explaining the predictions on clustered data. In 2019 IEEE international conference on data mining (ICDM) , pp. 1528-1533. IEEE, 2019.\n- Jan Ruben Zilke, Eneldo Loza Mencía, and Frederik Janssen. Deepred-rule extraction from deep neural networks. In Discovery Science: 19th International Conference, DS 2016, Bari, Italy, October 19-21, 2016, Proceedings 19 , pp. 457-473. Springer, 2016.",
      "keywords": [
        "deepred",
        "neural",
        "predictions",
        "data",
        "attribution",
        "rule",
        "feature",
        "intelligence"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    },
    {
      "index": 296,
      "paragraph": "References\nLin Zou, Han Leong Goh, Charlene Jin Yee Liew, Jessica Lishan Quah, Gary Tianyu Gu, Jun Jie Chew, Mukundaram Prem Kumar, Christine Gia Lee Ang, and Andy Wee An Ta. Ensemble image explainable ai (xai) algorithm for severe community-acquired pneumonia and covid-19 respiratory infections. IEEE Transactions on Artificial Intelligence , 4(2):242-254, 2022.",
      "keywords": [
        "ai",
        "pneumonia",
        "xai",
        "ensemble",
        "respiratory",
        "covid",
        "algorithm",
        "infections"
      ],
      "match_type": null,
      "matched_terms": [],
      "embedding_model_name": "all-mpnet-base-v2",
      "score": null
    }
  ]
}