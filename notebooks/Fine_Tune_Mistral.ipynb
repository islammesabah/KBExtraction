{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6r2T3wQTVCYi"
      },
      "source": [
        "#### Set-up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bqxk5tEj7q43"
      },
      "outputs": [],
      "source": [
        "# Install Libraries\n",
        "%%capture\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "# All LangChain libraries for implementing logic chaining\n",
        "%pip install -U langchain\n",
        "%pip install -U langchain_community\n",
        "%pip install -U langchain-huggingface\n",
        "%pip install -U langchain_experimental\n",
        "%pip install -U langchain_openai\n",
        "\n",
        "%pip install -U unstructured\n",
        "%pip install -U sentence-transformers\n",
        "\n",
        "%pip install -U Neo4jGraph\n",
        "%pip install -U py2neo\n",
        "%pip install -U spacy\n",
        "%pip install -U rdflib-neo4j\n",
        "# Langchain\n",
        "from langchain_community.graphs import Neo4jGraph\n",
        "from langchain_community.vectorstores import Neo4jVector\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQAWithSourcesChain\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "from spacy import load, displacy\n",
        "import pandas as pd\n",
        "from google.colab import userdata\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_PJR8Tq388YF"
      },
      "outputs": [],
      "source": [
        "# Load from colab note\n",
        "NEO4J_USERNAME = \"neo4j\"\n",
        "NEO4J_URI = userdata.get('NEO4J_URI')\n",
        "NEO4J_PASSWORD = userdata.get('NEO4J_PASSWORD')\n",
        "HF_API_KEY = userdata.get('HF_API_KEY')\n",
        "\n",
        "# Set up connection to graph instance using LangChain\n",
        "kg = Neo4jGraph(\n",
        "    url=NEO4J_URI,\n",
        "    username=NEO4J_USERNAME,\n",
        "    password=NEO4J_PASSWORD\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ofhgwfbwKX-2"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "\n",
        "# define huggingface generation endpoint\n",
        "hf_llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\", # Model Name\n",
        "    task=\"text-generation\",                       # task as generating a text response\n",
        "    max_new_tokens=150,                           # maximum numbers of generated tokens\n",
        "    do_sample=False,                              # disables sampling\n",
        "    huggingfacehub_api_token=HF_API_KEY           # ðŸ¤— huggingface API token\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNvzeziikXI0"
      },
      "source": [
        "Caution: Run, only if you wish to delete the content of database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V90teZPa9hHu",
        "outputId": "7c6f880f-0f34-4d46-f75b-2a2f61843267"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'count(n)': 0}]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# clean the neo4j dataset\n",
        "## All nodes and relationships.\n",
        "kg.query(\"MATCH (n) DETACH DELETE n\")\n",
        "## All indexes and constraints.\n",
        "kg.query(\"CALL apoc.schema.assert({},{},true) YIELD label, key RETURN *\")\n",
        "\n",
        "# check if the dataset empty\n",
        "kg.query(\"MATCH (n) RETURN count(n)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ekpkRjo3KxM"
      },
      "source": [
        "#### Rule-Based Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "_CmgQ6je_Bjs"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "allowed_dependencies = {\n",
        "     'acomp','advmod','agent','amod','attr','aux','auxpass',\n",
        "     'case','cc','ccomp','compound','conj','det','dobj',\n",
        "     'nmod','nsubj','nsubjpass',\n",
        "     'pcomp','pobj','prep','poss','ROOT','xcomp'\n",
        "}\n",
        "\n",
        "def has_required_dependencies(doc, allowed_dependencies):\n",
        "    if not {token.dep_ for token in doc}.issubset(allowed_dependencies):\n",
        "        return False\n",
        "\n",
        "    return (\"is a\" in doc.text.lower() or \"is an\" in doc.text.lower()) or \\\n",
        "            (any(token.dep_ == 'ROOT' for token in doc) and \\\n",
        "            any(token.dep_ in {'nsubj', 'nsubjpass'} for token in doc) and \\\n",
        "            any(token.dep_ in {'dobj', 'pobj'} for token in doc))\n",
        "\n",
        "nlp = load(\"en_core_web_sm\")\n",
        "unhandled_sentences=set()\n",
        "all_graphs = []\n",
        "with open(\"DSA_knowledge.txt\", \"r\") as file:\n",
        "    sentences = file.read()\n",
        "\n",
        "for sentence in [s.strip().rstrip(string.punctuation) for s in sentences.strip().split('\\n') if s.strip()]:\n",
        "  doc = nlp(sentence)\n",
        "  # displacy.render(doc, style=\"dep\", jupyter=True, options={'distance': 90})\n",
        "  if not has_required_dependencies(doc, allowed_dependencies):\n",
        "      unhandled_sentences.add(sentence)\n",
        "      continue\n",
        "\n",
        "  try:\n",
        "    temp_graph = {\n",
        "        \"nodes\": {},  # {'nodes': {0: {'pos': 0, 'label': 'X', 'dep': 'nsubj'}, 4: {'pos': 4, 'label': 'Y', 'dep': 'pobj'}},\n",
        "        \"edges\": [],  # 'edges': [(0, 4, 'is subclass of')]}\n",
        "        \"sentence\": sentence\n",
        "    }\n",
        "\n",
        "    edge_mapping = {\n",
        "        'subject_nodes': {},  # {1: {0}} # multiple subject nodes possible\n",
        "        'object_nodes': {},   # {1: 4}\n",
        "        'edge_ids': set()     # {1}\n",
        "    }\n",
        "\n",
        "    temp_graph[\"nodes\"] = {token['id']: {\"pos\": token['id'], \"label\": doc.text, \"dep\": token['dep']}\n",
        "                          for token, doc in zip(doc.to_json()['tokens'], doc)}\n",
        "\n",
        "    temp_graph[\"edges\"] = [(token['head'], token['id'], token['dep'])\n",
        "                            for token in doc.to_json()['tokens'] if token['head'] != token['id']]\n",
        "\n",
        "    root_node = list(filter(lambda node: temp_graph[\"nodes\"][node]['dep'] == 'ROOT', temp_graph[\"nodes\"]))[0]\n",
        "    stopping = False\n",
        "    while not stopping:\n",
        "      for edge in sorted(temp_graph[\"edges\"], key=lambda x: abs(x[0] - x[1])):\n",
        "\n",
        "        source_pos, target_pos, meta = edge\n",
        "\n",
        "        if source_pos not in temp_graph[\"nodes\"] or target_pos not in temp_graph[\"nodes\"]:\n",
        "            continue\n",
        "        #print(edge)\n",
        "        source_metadata = temp_graph[\"nodes\"][source_pos]\n",
        "        target_metadata = temp_graph[\"nodes\"][target_pos]\n",
        "        try:\n",
        "            match (source_metadata, meta, target_metadata):\n",
        "                case {'label': s, **source}, 'compound' | 'amod' | 'aux' |'auxpass' | 'advmod', {'label': t, **target}:\n",
        "                    source_metadata['label'] = f\"{t} {s}\"\n",
        "                    temp_graph['edges'] = list(filter(lambda edge: not (edge[0] == source_pos and edge[1] == target_pos), temp_graph['edges']))\n",
        "                    del temp_graph['nodes'][target_pos]\n",
        "                    continue\n",
        "\n",
        "                case {'label': s, **source}, 'agent', {'label': t, **target}:\n",
        "                    source_metadata['label'] = f\"{s} {t}\"\n",
        "                    next_node = next((n for src, n, label in temp_graph[\"edges\"] if src == target_pos and label == 'pobj'), None)\n",
        "                    edge_mapping['edge_ids'].add(source_pos)\n",
        "                    edge_mapping['object_nodes'][source_pos] = next_node\n",
        "                    temp_graph['edges'].append((source_pos, next_node, 'pobj'))\n",
        "                    temp_graph['edges'] = list(filter(lambda edge: not (edge[0] == source_pos and edge[1] == target_pos), temp_graph['edges']))\n",
        "                    temp_graph['edges'] = list(filter(lambda edge: not (edge[0] == target_pos and edge[1] == next_node), temp_graph['edges']))\n",
        "                    del temp_graph['nodes'][target_pos]\n",
        "                    continue\n",
        "\n",
        "                case {'label': s, **source}, 'case' | 'cc', {'label': t, **target}:\n",
        "                    temp_graph['edges'] = list(filter(lambda edge: not (edge[0] == source_pos and edge[1] == target_pos), temp_graph['edges']))\n",
        "                    del temp_graph['nodes'][target_pos]\n",
        "                    continue\n",
        "\n",
        "                case {'label': s, **source}, 'det', {'label': t, **target}:\n",
        "                    temp_graph['nodes'][source_pos]['det'] = t\n",
        "                    temp_graph['edges'] = list(filter(lambda edge: not (edge[0] == source_pos and edge[1] == target_pos), temp_graph['edges']))\n",
        "                    del temp_graph['nodes'][target_pos]\n",
        "                    continue\n",
        "\n",
        "                case {'label': s, **source}, 'attr'|'acomp', {'label': 'subclass'|'attribute'|'dimension'|'kind'|'threat'|'result'|'type'|'equal'|'form', **target}: #is(head)--attr--subclass(tail)--prep--of(child)--pobj--Risk\n",
        "                    next_node = next((n for src, n, label in temp_graph[\"edges\"] if src == target_pos and label == 'prep'), None)\n",
        "                    obj_node = next((n for src, n, label in temp_graph[\"edges\"] if src == next_node and label == 'pobj'), None)\n",
        "                    source_metadata['label'] = f\"{s} {target_metadata['label']} {temp_graph['nodes'][next_node]['label']}\" #is-->issubclassof\n",
        "                    edge_mapping['edge_ids'].add(source_pos)\n",
        "                    edge_mapping['object_nodes'][source_pos] = obj_node\n",
        "                    temp_graph['edges'] = list(filter(lambda edge: not (edge[0] == source_pos and edge[1] == target_pos), temp_graph['edges'])) # remove edge: is--subclass\n",
        "                    temp_graph['edges'] = list(filter(lambda edge: not (edge[0] == target_pos and edge[1] == next_node), temp_graph['edges'])) # remove edge: subclass--of\n",
        "                    temp_graph['edges'] = list(filter(lambda edge: not (edge[0] == next_node and edge[1] == obj_node), temp_graph['edges'])) # remove edge: of--Y\n",
        "                    temp_graph['edges'].append((source_pos, obj_node, 'pobj')) #connect edge from 'is' node to obj node\n",
        "                    del temp_graph['nodes'][target_pos] # remove node: 'subclass'\n",
        "                    del temp_graph['nodes'][next_node]  # remove node: 'of'\n",
        "                    continue\n",
        "\n",
        "                case {'label': s, **source}, 'attr'|'acomp', {'label': t, **target}: #is-attr-Y\n",
        "                    edge_mapping['edge_ids'].add(source_pos)\n",
        "                    edge_mapping['object_nodes'][source_pos] = target_pos\n",
        "                    continue\n",
        "\n",
        "                case {'dep': 'ROOT', 'label': s, **source}, 'prep'|'xcomp', {'label': t, **target}: #attributes(ROOT)--prep--to #helps--xcomp--see--pobj--X\n",
        "                    source_metadata['label'] = f\"{s} {t}\"\n",
        "                    next_node = next((n for src, n, label in temp_graph[\"edges\"] if src == target_pos and label in {'pobj', 'dobj'}), None)\n",
        "                    if next_node:\n",
        "                      temp_graph['edges'].append((source_pos, next_node, 'pobj'))\n",
        "                      temp_graph['edges'] = list(filter(lambda edge: not (edge[0] == source_pos and edge[1] == target_pos), temp_graph['edges']))\n",
        "                      temp_graph['edges'] = list(filter(lambda edge: not (edge[0] == target_pos and edge[1] == next_node), temp_graph['edges']))\n",
        "                      del temp_graph['nodes'][target_pos]\n",
        "                    continue\n",
        "\n",
        "                case {'label': s, **source}, 'prep', {'label': t, **target}: #*-dobj-assessment--prep--of|*-attr-(a)dimension-prep-of\n",
        "                    if next((n for src, n, label in temp_graph[\"edges\"] if n == source_pos and label == 'attr'), None) is None:\n",
        "                      next_node = next((n for src, n, label in temp_graph[\"edges\"] if src == target_pos and label in {'pobj'}), None)\n",
        "                      if next_node: #Date-prep-of-pobj-birth\n",
        "                        source_metadata['label'] = f\"{s} {t} {temp_graph['nodes'][next_node]['label']}\"\n",
        "                        temp_graph['edges'] = list(filter(lambda edge: not (edge[0] == source_pos and edge[1] == target_pos), temp_graph['edges']))\n",
        "                        temp_graph['edges'] = list(filter(lambda edge: not (edge[0] == target_pos and edge[1] == next_node), temp_graph['edges']))\n",
        "                        del temp_graph['nodes'][target_pos]\n",
        "                        del temp_graph['nodes'][next_node]\n",
        "                      else:\n",
        "                        edge_mapping['edge_ids'].add(target_pos)\n",
        "                        edge_mapping['subject_nodes'].setdefault(target_pos, set()).add(source_pos)\n",
        "                        temp_graph['edges'] = list(filter(lambda edge: not (edge[0] == source_pos and edge[1] == target_pos), temp_graph['edges']))\n",
        "                    continue\n",
        "\n",
        "                case {'label': s, **source}, 'poss', {'label': t, **target}:\n",
        "                    next_node = next((n for src, n, label in temp_graph[\"edges\"] if src == source_pos and label == 'conj'), None)\n",
        "                    if next_node:\n",
        "                      temp_graph['edges'].append((temp_graph[\"nodes\"][next_node]['label'],\n",
        "                                                  temp_graph[\"nodes\"][target_pos]['label'],\n",
        "                                                  'of'))\n",
        "                    continue\n",
        "\n",
        "                case {'label': s, **source}, 'nmod', {'label': t, **target}:\n",
        "                    source_metadata['label'] = f\"{t} {s}\"\n",
        "                    incoming_node = next((src for src, n, label in temp_graph[\"edges\"] if target == source_pos and label == 'nsubj'), None)\n",
        "                    if 'conj' in target:\n",
        "                        target['conj']['nodeId'] = target['conj']['text'] + f\" {s}\"\n",
        "                        edge_mapping['subject_nodes'][incoming_node].add(target['conj']['nodeId'])\n",
        "                    temp_graph['edges'] = list(filter(lambda edge: not (edge[0] == source_pos and edge[1] == target_pos), temp_graph['edges']))\n",
        "                    del temp_graph['nodes'][target_pos]\n",
        "                    continue\n",
        "\n",
        "                case {'label': s, **source}, 'conj', {'label': t, **target}:\n",
        "                    temp_graph['edges'] = list(filter(lambda edge: not (edge[0] == source_pos and edge[1] == target_pos), temp_graph['edges']))\n",
        "                    temp_graph['nodes'][source_pos]['conj'] = {'text': t, 'nodeId': target_pos}\n",
        "                    continue\n",
        "\n",
        "                case {'label': s, **source}, 'pcomp', {'label': t, **target}: #in--pcomp--explaining--dobj--x\n",
        "                    temp_graph['edges'] = list(filter(lambda edge: not (edge[0] == source_pos and edge[1] == target_pos), temp_graph['edges']))\n",
        "                    next_node = next((n for src, n, label in temp_graph[\"edges\"] if src == target_pos and label in {'pobj', 'dobj'}), None)\n",
        "                    if next_node:\n",
        "                      temp_graph['nodes'][root_node]['label'] += f\" {t}\"  #if not work f\" {temp_graph['nodes'][root_node]['label']} {t}\n",
        "                      temp_graph['edges'].append((root_node, next_node, 'dobj'))\n",
        "                      temp_graph['edges'] = list(filter(lambda edge: not (edge[0] == target_pos and edge[1] == next_node), temp_graph['edges']))\n",
        "                      del temp_graph['nodes'][source_pos]\n",
        "                      del temp_graph['nodes'][target_pos]\n",
        "                    continue\n",
        "\n",
        "                case {'label': s, **source}, 'ccomp', {'label': t, **target}: #Design interface can help users understand AI decisions\n",
        "                    next_node = next((n for src, n, label in temp_graph[\"edges\"] if src == target_pos and label == 'nsubj'), None)\n",
        "                    if next_node:\n",
        "                      edge_mapping['object_nodes'][source_pos] = next_node\n",
        "                    continue\n",
        "\n",
        "                case {'label': s, **source}, 'nsubj' | 'nsubjpass', {'label': t, **target}:\n",
        "                    edge_mapping['edge_ids'].add(source_pos)\n",
        "                    edge_mapping['subject_nodes'].setdefault(source_pos, set()).add(target_pos)\n",
        "                    if 'conj' in target:\n",
        "                      edge_mapping['subject_nodes'][source_pos].add(target_metadata['nodeId'])\n",
        "                    continue\n",
        "\n",
        "                case {'label': s, **source}, 'dobj' | 'pobj', {'label': t, **target}:\n",
        "                    if next((src for src, n, label in temp_graph[\"edges\"] if src == source_pos and label == 'prep'), None):\n",
        "                      source_metadata['label'] = f\"{s} {t}\"\n",
        "                      temp_graph['edges'] = list(filter(lambda edge: not (edge[0] == source_pos and edge[1] == target_pos), temp_graph['edges']))\n",
        "                      del temp_graph['nodes'][target_pos]\n",
        "                    #assign object outside loop\n",
        "                    continue\n",
        "\n",
        "                case another:\n",
        "                    print(\"another:\", edge)\n",
        "                    unhandled_sentences.add(sentence)\n",
        "                    stopping = True\n",
        "                    continue\n",
        "\n",
        "        except Exception as e:\n",
        "                print(f\"Error occurred in sentence: {sentence} with edge: {edge}, error: {e}\")\n",
        "                unhandled_sentences.add(sentence) # throw error\n",
        "                stopping = True\n",
        "                continue\n",
        "      else:\n",
        "          break\n",
        "\n",
        "    # Update object nodes\n",
        "    edge_mapping['object_nodes'].update({\n",
        "        edge_id: next((tail for head, tail, meta in temp_graph['edges']\n",
        "                      if meta in {'dobj', 'pobj'}), None)\n",
        "        for edge_id in edge_mapping['edge_ids']\n",
        "        if edge_id not in edge_mapping['object_nodes']\n",
        "    })\n",
        "\n",
        "    for edge_id, obj_node in edge_mapping['object_nodes'].items():\n",
        "        if obj_node is None:\n",
        "            print(f\"Missing object node for edge ID: {edge_id}\")\n",
        "\n",
        "    # create final mapping\n",
        "    for edge_id in edge_mapping['edge_ids']:\n",
        "        subject_nodes = edge_mapping['subject_nodes'][edge_id]\n",
        "        object_node = edge_mapping['object_nodes'][edge_id]\n",
        "        edge_node = temp_graph['nodes'][edge_id]\n",
        "        for subject_node in subject_nodes:\n",
        "            #temp_graph['edges'].append((subject_node, object_node, edge_node['label']))\n",
        "            temp_graph['edges'].append((temp_graph[\"nodes\"][subject_node]['label'],\n",
        "                                        temp_graph[\"nodes\"][object_node]['label'],\n",
        "                                        edge_node['label']))\n",
        "            temp_graph['edges'] = list(filter(lambda edge: not (edge[0] == edge_id and edge[1] == subject_node), temp_graph['edges']))\n",
        "\n",
        "        temp_graph['edges'] = list(filter(lambda edge: not (edge[0] == edge_id and edge[1] == object_node), temp_graph['edges']))\n",
        "        del temp_graph['nodes'][edge_id]\n",
        "\n",
        "    temp_graph['edges'] = list(set(temp_graph['edges'])-set([edge for edge in temp_graph['edges'] if edge[2] in allowed_dependencies]))\n",
        "\n",
        "    # all_graphs.append(temp_graph) # we don't need nodes to be pat of json, as edges have them\n",
        "    all_graphs.append({\n",
        "      \"edges\": temp_graph[\"edges\"],\n",
        "      \"sentence\": temp_graph[\"sentence\"]\n",
        "    })\n",
        "\n",
        "    for source, target, edge in temp_graph['edges']:\n",
        "        kg.query(\n",
        "          \"\"\"\n",
        "          CALL apoc.merge.node([$source_label], {label:$source_label}) YIELD node AS s\n",
        "          CALL apoc.merge.node([$target_label], {label:$target_label}) YIELD node AS t\n",
        "          CALL apoc.merge.relationship(s, $edge_label, {sentence: $sentence}, {}, t, {}) YIELD rel\n",
        "          RETURN s,t,rel\n",
        "          \"\"\"\n",
        "          , params={\n",
        "          'source_label': temp_graph['nodes'][source]['label'].replace(\" \", \"_\"),\n",
        "          'target_label': temp_graph['nodes'][target]['label'].replace(\" \", \"_\"),\n",
        "          'edge_label': edge.replace(\" \", \"_\"),\n",
        "          'sentence': sentence\n",
        "        })\n",
        "\n",
        "  except Exception as e:\n",
        "    print(f\"Failed to process sentence: {sentence}, error: {e}\")\n",
        "    unhandled_sentences.add(sentence)\n",
        "\n",
        "\n",
        "with open(\"unhandled_sentences.txt\", \"w\") as file:\n",
        "  for unhandled in unhandled_sentences:\n",
        "    file.write(unhandled + \"\\n\")\n",
        "\n",
        "# Training data - for fine tuning the HF model:\n",
        "with open('graph_data.json', 'w') as json_file:\n",
        "    json.dump(all_graphs, json_file, indent=4, ensure_ascii=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOfJVtH9MrhI"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "# load JSON file\n",
        "def load_json(path):\n",
        "  with open(path, 'r') as file:\n",
        "    return json.load(file)\n",
        "\n",
        "import textwrap\n",
        "# Prints the text with lines wrapped to a maximum width of 80 characters\n",
        "def clean_print(text):\n",
        "    return print(textwrap.fill(text, width=80))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-34SEPp_lGq6"
      },
      "source": [
        "##### Query graph database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGofGtCqZjrG"
      },
      "outputs": [],
      "source": [
        "CYPHER_GENERATION_TEMPLATE = \"\"\"Task:\n",
        "Generate Cypher statement to query a Neo4j graph database.\n",
        "\n",
        "Instructions:\n",
        "* Only use the provided relationship types, node labels, and properties in the schema.\n",
        "* Do not use any other relationship types, properties, or node labels that are not provided.\n",
        "* Always follow the correct relationship direction.\n",
        "* Ensure that the query follows the correct Cypher syntax.\n",
        "\n",
        "Schema:\n",
        "{schema}\n",
        "\n",
        "Examples:\n",
        "Here are a few examples of generated Cypher statements for particular questions:\n",
        "\n",
        "# What is Bias?\n",
        "    MATCH (s:Bias)-[r]-(t)\n",
        "    RETURN s,r,t\n",
        "\n",
        "# What might introduce Bias?\n",
        "    MATCH (s)-[r:might_introduce]->(t:Bias)\n",
        "    RETURN s,r,t\n",
        "\n",
        "The question is:\n",
        "{question}\n",
        "\n",
        "The generated Cypher statement:\"\"\"\n",
        "\n",
        "# Define schema\n",
        "schema = \"\"\"Node: Risk\n",
        "Properties: label\n",
        "Relationships: (Bias)-[:is]->(Risk)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bfEiuatJTof"
      },
      "outputs": [],
      "source": [
        "# build the query prompt template\n",
        "from langchain.prompts.prompt import PromptTemplate\n",
        "CYPHER_GENERATION_PROMPT = PromptTemplate(\n",
        "    input_variables=[\"schema\", \"question\"],\n",
        "    template=CYPHER_GENERATION_TEMPLATE\n",
        ")\n",
        "\n",
        "# initilize the chain\n",
        "from langchain.chains import GraphCypherQAChain\n",
        "cypherChain = GraphCypherQAChain.from_llm(\n",
        "    graph=kg,\n",
        "    llm=hf_llm,\n",
        "    #cypher_llm=hf_llm,                    # see intermediate steps\n",
        "    cypher_prompt=CYPHER_GENERATION_PROMPT,   # cypher generation prompt\n",
        "    verbose=True,\n",
        "    allow_dangerous_requests=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuWDDHqrQ4fe",
        "outputId": "97482505-0d79-4506-dd26-5ce89324789a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
            "Generated Cypher:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "MATCH (s:Requirement)-[r]-(t)\n",
            "RETURN s,r,t\n",
            "\u001b[0m\n",
            "Full Context:\n",
            "\u001b[32;1m\u001b[1;3m[{'s': {'label': 'Requirement'}, 'r': ({'label': 'Fairness'}, 'is', {'label': 'Requirement'}), 't': {'label': 'Fairness'}}, {'s': {'label': 'Requirement'}, 'r': ({'label': 'Explainability'}, 'is', {'label': 'Requirement'}), 't': {'label': 'Explainability'}}]\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Question: What is Requirement?\n",
            "Response:  Fairness and Explainability are Requirements.\n"
          ]
        }
      ],
      "source": [
        "# test the chain\n",
        "question = \"What is Requirement?\"\n",
        "res = cypherChain.run(question)\n",
        "clean_print('Question: '+question)\n",
        "clean_print('Response: '+res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5VozjCoN8sy",
        "outputId": "be06af75-a51d-40ef-ef88-8490bdd4b642"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
            "Generated Cypher:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "MATCH (s:Bias)-[r]-(t)\n",
            "RETURN s,r,t\n",
            "\u001b[0m\n",
            "Full Context:\n",
            "\u001b[32;1m\u001b[1;3m[{'s': {'label': 'Bias'}, 'r': ({'label': 'Bias'}, 'is', {'label': 'Risk'}), 't': {'label': 'Risk'}}, {'s': {'label': 'Bias'}, 'r': ({'label': 'Algorithmic_Bias'}, 'is_subclass_of', {'label': 'Bias'}), 't': {'label': 'Algorithmic_Bias'}}, {'s': {'label': 'Bias'}, 'r': ({'label': 'Historical_Bias'}, 'is_subclass_of', {'label': 'Bias'}), 't': {'label': 'Historical_Bias'}}, {'s': {'label': 'Bias'}, 'r': ({'label': 'TrainTestSplit'}, 'might_introduce', {'label': 'Bias'}), 't': {'label': 'TrainTestSplit'}}, {'s': {'label': 'Bias'}, 'r': ({'label': 'Bias'}, 'is_threat_to', {'label': 'Fairness'}), 't': {'label': 'Fairness'}}]\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Question: What is Bias?\n",
            "Response:  Bias is a type of risk. It can be algorithmic bias, historical bias,\n",
            "or it can be introduced by the train-test split. Bias is a threat to fairness.\n"
          ]
        }
      ],
      "source": [
        "question = \"What is Bias?\"\n",
        "res = cypherChain.run(question)\n",
        "clean_print('Question: '+question)\n",
        "clean_print('Response: '+res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 710
        },
        "collapsed": true,
        "id": "xzwg5HqOrbn5",
        "outputId": "8be72ede-5de9-4d47-9e7b-bc4b4fdf64e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
            "Generated Cypher:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Note: This Cypher query traverses the graph from the TrainTestSplit node, checks if it is connected to a Risk node, and then finds all nodes connected by the is_subclass_of relationship, excluding the original TrainTestSplit node. It returns those found nodes.\n",
            "\n",
            "For a more specific example, let's assume we have a graph with the following nodes and relationships:\n",
            "\n",
            "```\n",
            "(a:TrainTestSplit {name: 'TrainSet'})\n",
            "(b:TrainTestSplit {name: 'TestSet'})\n",
            "(c:Risk {name: 'HighRisk'})\n",
            "(d:TrainTestSplit {name: 'Train\u001b[0m\n"
          ]
        },
        {
          "ename": "CypherSyntaxError",
          "evalue": "{code: Neo.ClientError.Statement.SyntaxError} {message: Invalid input 'Note': expected 'FOREACH', 'ALTER', 'ORDER BY', 'CALL', 'USING PERIODIC COMMIT', 'CREATE', 'LOAD CSV', 'START DATABASE', 'STOP DATABASE', 'DEALLOCATE', 'DELETE', 'DENY', 'DETACH', 'DROP', 'DRYRUN', 'FINISH', 'GRANT', 'INSERT', 'LIMIT', 'MATCH', 'MERGE', 'NODETACH', 'OFFSET', 'OPTIONAL', 'REALLOCATE', 'REMOVE', 'RENAME', 'RETURN', 'REVOKE', 'ENABLE SERVER', 'SET', 'SHOW', 'SKIP', 'TERMINATE', 'UNWIND', 'USE' or 'WITH' (line 2, column 1 (offset: 1))\n\"Note: This Cypher query traverses the graph from the TrainTestSplit node, checks if it is connected to a Risk node, and then finds all nodes connected by the is_subclass_of relationship, excluding the original TrainTestSplit node. It returns those found nodes.\"\n ^}",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCypherSyntaxError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-74c11b499928>\u001b[0m in \u001b[0;36m<cell line: 37>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# Run the query and get the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcypherChain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"query\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnode_name\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# Display the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py\u001b[0m in \u001b[0;36mwarning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0mwarned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0memit_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mawarning_emitting_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    604\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"`run` supports only one positional argument.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m             return self(args[0], callbacks=callbacks, tags=tags, metadata=metadata)[\n\u001b[0m\u001b[1;32m    607\u001b[0m                 \u001b[0m_output_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m             ]\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py\u001b[0m in \u001b[0;36mwarning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0mwarned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0memit_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mawarning_emitting_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    387\u001b[0m         }\n\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m         return self.invoke(\n\u001b[0m\u001b[1;32m    390\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m             \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRunnableConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             outputs = (\n\u001b[0;32m--> 160\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/chains/graph_qa/cypher.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;31m# Generated Cypher be null if query corrector identifies invalid schema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgenerated_cypher\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_cypher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/graphs/neo4j_graph.py\u001b[0m in \u001b[0;36mquery\u001b[0;34m(self, query, params)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m             data, _, _ = self._driver.execute_query(\n\u001b[0m\u001b[1;32m    432\u001b[0m                 \u001b[0mQuery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m                 \u001b[0mdatabase_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_database\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/neo4j/_sync/driver.py\u001b[0m in \u001b[0;36mexecute_query\u001b[0;34m(self, query_, parameters_, routing_, database_, impersonated_user_, bookmark_manager_, auth_, result_transformer_, **kwargs)\u001b[0m\n\u001b[1;32m    969\u001b[0m                 )\n\u001b[1;32m    970\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pipelined_begin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 971\u001b[0;31m                 return session._run_transaction(\n\u001b[0m\u001b[1;32m    972\u001b[0m                     \u001b[0maccess_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m                     \u001b[0mTelemetryAPI\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDRIVER\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/neo4j/_sync/work/session.py\u001b[0m in \u001b[0;36m_run_transaction\u001b[0;34m(self, access_mode, api, transaction_function, args, kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m                 \u001b[0mtx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transaction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransaction_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCancelledError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m                     \u001b[0;31m# if cancellation callback has not been called yet:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/neo4j/_work/query.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mwrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/neo4j/_sync/driver.py\u001b[0m in \u001b[0;36m_work\u001b[0;34m(tx, query, parameters, transformer)\u001b[0m\n\u001b[1;32m   1305\u001b[0m     \u001b[0mtransformer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mResult\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_T\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m ) -> _T:\n\u001b[0;32m-> 1307\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1308\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/neo4j/_sync/work/transaction.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, query, parameters, **kwparameters)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tx_ready_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/neo4j/_sync/work/result.py\u001b[0m in \u001b[0;36m_tx_ready_run\u001b[0;34m(self, query, parameters)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;31m# BEGIN {extra}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;31m# RUN \"query\" {parameters} {extra}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     def _run(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/neo4j/_sync/work/result.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, query, parameters, db, imp_user, access_mode, bookmarks, notifications_min_severity, notifications_disabled_classifications)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_pull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/neo4j/_sync/work/result.py\u001b[0m in \u001b[0;36m_attach\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exhausted\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attached\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/neo4j/_sync/io/_common.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                     \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mNeo4jError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mServiceUnavailable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSessionExpired\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                     \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miscoroutinefunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__on_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/neo4j/_sync/io/_bolt.py\u001b[0m in \u001b[0;36mfetch_message\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    975\u001b[0m             \u001b[0mhydration_hooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhydration_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m         )\n\u001b[0;32m--> 977\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midle_since\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/neo4j/_sync/io/_bolt5.py\u001b[0m in \u001b[0;36m_process_message\u001b[0;34m(self, tag, fields)\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_server_state_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbolt_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFAILED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_failure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary_metadata\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mServiceUnavailable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatabaseUnavailable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/neo4j/_sync/io/_common.py\u001b[0m in \u001b[0;36mon_failure\u001b[0;34m(self, metadata)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0mhandler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandlers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"on_summary\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0mUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNeo4jError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhydrate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_ignored\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCypherSyntaxError\u001b[0m: {code: Neo.ClientError.Statement.SyntaxError} {message: Invalid input 'Note': expected 'FOREACH', 'ALTER', 'ORDER BY', 'CALL', 'USING PERIODIC COMMIT', 'CREATE', 'LOAD CSV', 'START DATABASE', 'STOP DATABASE', 'DEALLOCATE', 'DELETE', 'DENY', 'DETACH', 'DROP', 'DRYRUN', 'FINISH', 'GRANT', 'INSERT', 'LIMIT', 'MATCH', 'MERGE', 'NODETACH', 'OFFSET', 'OPTIONAL', 'REALLOCATE', 'REMOVE', 'RENAME', 'RETURN', 'REVOKE', 'ENABLE SERVER', 'SET', 'SHOW', 'SKIP', 'TERMINATE', 'UNWIND', 'USE' or 'WITH' (line 2, column 1 (offset: 1))\n\"Note: This Cypher query traverses the graph from the TrainTestSplit node, checks if it is connected to a Risk node, and then finds all nodes connected by the is_subclass_of relationship, excluding the original TrainTestSplit node. It returns those found nodes.\"\n ^}"
          ]
        }
      ],
      "source": [
        "CYPHER_GENERATION_TEMPLATE = \"\"\"Task:\n",
        "Generate Cypher statement to query a Neo4j graph database.\n",
        "\n",
        "Instructions:\n",
        "* Use only the provided node label: {query}.\n",
        "* Traverse from this node and check if it is connected to a node labeled 'Risk'.\n",
        "* Find all nodes connected by the 'is_subclass_of' relationship and return them, excluding the original node.\n",
        "\n",
        "Cypher Query:\n",
        "MATCH (tts:{query})-[*]-(risk:Risk)\n",
        "WHERE risk IS NOT NULL\n",
        "WITH tts\n",
        "MATCH (tts)-[:is_subclass_of]->(parent)\n",
        "MATCH (source)-[:is_subclass_of]->(parent)\n",
        "WHERE source <> tts\n",
        "RETURN source\n",
        "\"\"\"\n",
        "\n",
        "from langchain.prompts.prompt import PromptTemplate\n",
        "\n",
        "CYPHER_GENERATION_PROMPT = PromptTemplate(\n",
        "    input_variables=[\"query\"],\n",
        "    template=CYPHER_GENERATION_TEMPLATE\n",
        ")\n",
        "\n",
        "node_name = \"TrainTestSplit\"\n",
        "\n",
        "cypherChain = GraphCypherQAChain.from_llm(\n",
        "    graph=kg,\n",
        "    llm=hf_llm,\n",
        "    cypher_prompt=CYPHER_GENERATION_PROMPT,\n",
        "    verbose=True,\n",
        "    allow_dangerous_requests=True\n",
        ")\n",
        "\n",
        "# Run the query and get the response\n",
        "res = cypherChain.run({\"query\": node_name})\n",
        "\n",
        "# Display the response\n",
        "clean_print(f'Question: {node_name}')\n",
        "clean_print(f'Response: {res}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-KtvrZ7b8jI"
      },
      "source": [
        "#### Fine tune Mistral-7B Lora"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "wslKAWxK30mM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: 4.32.0 not found\n"
          ]
        }
      ],
      "source": [
        "# Install Libraries\n",
        "!pip install -q streamlit                                                                             # For Deploying apps\n",
        "!pip install -q transformers>=4.32.0 datasets evaluate                                                # Comes from HuggingFace\n",
        "!pip install -q -U peft                                                                               # Parameter-efficient Fine-tuning\n",
        "!pip install -q -U accelerate                                                                         # Loading models across GPUs/CPU/disk\n",
        "!pip install -q trl==0.7.1\n",
        "!pip install -q sentencepiece\n",
        "!pip install -q sentence-transformers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPUwUOpGUs7x"
      },
      "source": [
        "##### Loading the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QD8ROSsy4JRm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# load the environment variables\n",
        "load_dotenv(override=True)\n",
        "HF_API_TOKEN = os.getenv(\"HF_API_TOKEN\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_name_or_path = \"mistralai/Mistral-7B-Instruct-v0.3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPHMqhcr4Qsf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:11<00:00,  3.88s/it]\n",
            "Some parameters are on the meta device because they were offloaded to the disk.\n"
          ]
        }
      ],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name_or_path,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    revision=\"main\",\n",
        "    token=HF_API_TOKEN)\n",
        "#tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /opt/miniconda3/envs/trust/lib/python3.9/site-packages (4.47.1)\n",
            "Requirement already satisfied: sentencepiece in /opt/miniconda3/envs/trust/lib/python3.9/site-packages (0.2.0)\n",
            "Requirement already satisfied: filelock in /opt/miniconda3/envs/trust/lib/python3.9/site-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /opt/miniconda3/envs/trust/lib/python3.9/site-packages (from transformers) (0.27.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /opt/miniconda3/envs/trust/lib/python3.9/site-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/miniconda3/envs/trust/lib/python3.9/site-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /opt/miniconda3/envs/trust/lib/python3.9/site-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /opt/miniconda3/envs/trust/lib/python3.9/site-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /opt/miniconda3/envs/trust/lib/python3.9/site-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/miniconda3/envs/trust/lib/python3.9/site-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /opt/miniconda3/envs/trust/lib/python3.9/site-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /opt/miniconda3/envs/trust/lib/python3.9/site-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /opt/miniconda3/envs/trust/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/miniconda3/envs/trust/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/envs/trust/lib/python3.9/site-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/envs/trust/lib/python3.9/site-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/envs/trust/lib/python3.9/site-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/envs/trust/lib/python3.9/site-packages (from requests->transformers) (2024.12.14)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install --upgrade transformers sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Cannot instantiate this tokenizer from a slow version. If it's based on sentencepiece, make sure you have sentencepiece installed.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/miniconda3/envs/trust/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:921\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    917\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    918\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    919\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist or is not currently imported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    920\u001b[0m         )\n\u001b[0;32m--> 921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;66;03m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n\u001b[1;32m    925\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, EncoderDecoderConfig):\n",
            "File \u001b[0;32m/opt/miniconda3/envs/trust/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2032\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2029\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2030\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2032\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2033\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2034\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2035\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2036\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2037\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2038\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2039\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2040\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2041\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2042\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2043\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2044\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/miniconda3/envs/trust/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2272\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2270\u001b[0m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[1;32m   2271\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2272\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2273\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m import_protobuf_decode_error():\n\u001b[1;32m   2274\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   2275\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load tokenizer model from SPM, loading from TikToken will be attempted instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2276\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(Google protobuf error: Tried to load SPM model with non-SPM vocab file).\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2277\u001b[0m     )\n",
            "File \u001b[0;32m/opt/miniconda3/envs/trust/lib/python3.9/site-packages/transformers/models/llama/tokenization_llama_fast.py:157\u001b[0m, in \u001b[0;36mLlamaTokenizerFast.__init__\u001b[0;34m(self, vocab_file, tokenizer_file, clean_up_tokenization_spaces, unk_token, bos_token, eos_token, add_bos_token, add_eos_token, use_default_system_prompt, legacy, add_prefix_space, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m add_prefix_space \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_slow\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 157\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43munk_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munk_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43meos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_bos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_bos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_eos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_eos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_default_system_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_default_system_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_prefix_space\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_prefix_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlegacy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlegacy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_bos_token \u001b[38;5;241m=\u001b[39m add_bos_token\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_eos_token \u001b[38;5;241m=\u001b[39m add_eos_token\n",
            "File \u001b[0;32m/opt/miniconda3/envs/trust/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py:107\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m added_tokens_decoder \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madded_tokens_decoder\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m from_slow \u001b[38;5;129;01mand\u001b[39;00m slow_tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mslow_tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot instantiate this tokenizer from a slow version. If it\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms based on sentencepiece, make sure you \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    109\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave sentencepiece installed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    110\u001b[0m     )\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_object \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(tokenizer_object)\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot instantiate this tokenizer from a slow version. If it's based on sentencepiece, make sure you have sentencepiece installed."
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6KOn8Uk4ujI"
      },
      "outputs": [],
      "source": [
        "import torch                                # Deep Learning Framework\n",
        "import time                                 # Measure inference time\n",
        "import pandas as pd                         # For table dataset structure\n",
        "import numpy as np                          # Numerical operations on the CPU\n",
        "from datasets import load_dataset           # Loading the dataseet\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ppMfiwPUhoQ"
      },
      "source": [
        "##### Loading the Quantized Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "ca189e831a2b4694a276f9324504c0e5",
            "8420489150824a1b9165babf8feff934",
            "6c4c236462574357ab7421efe325a9a9",
            "215698777d0848639f8d4f6e66908b81",
            "9f1201b9b7814a97a13a45fcb2e6380d",
            "0b598bbbae4045b6b5dab9567ef6175b",
            "41c087371c554945a4bd24a2964b17a5",
            "9ed3ea3a6a4e42389757f2052f498c8e",
            "ae94f9fd759c47d3b8db970abcfd1a41",
            "7b1fb9f04270484585e6599da952e0c7",
            "86c56f26f32a40269e80fb0c9fc9a93e"
          ]
        },
        "id": "ZbNZDyLw4zf4",
        "outputId": "eb96c878-4723-489c-982b-768436947c4e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ca189e831a2b4694a276f9324504c0e5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "# 1. Setup the quantization configuarion\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,                     # Q = 4 bits\n",
        "    bnb_4bit_use_double_quant=True,        # double quantization, quantizing the quantization constants for saving an additional 0.4 bits per parameter\n",
        "    bnb_4bit_quant_type=\"nf4\",             # 4-bit NormalFloat Quantization (optimal for normal weights; enforces w âˆˆ [-1,1])\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16  # Dequantize to 16-bits before computations (as in the paper)\n",
        ")\n",
        "# 2. Pass it while using the model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name_or_path,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lk7HCW6cUlNq"
      },
      "source": [
        "##### Loading the Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqTjVfbI9owl"
      },
      "outputs": [],
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWb29G-B9ryA"
      },
      "outputs": [],
      "source": [
        "prompt = f\"\"\"\n",
        "How are you, Llama! Tell me about yourself.\n",
        "\"\"\"\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors='pt')\n",
        "output_tokens = model.generate(inputs[\"input_ids\"], max_new_tokens=100,)[0]     # batch of tokens with one sequence\n",
        "output = tokenizer.decode(output_tokens, skip_special_tokens=True)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115,
          "referenced_widgets": [
            "3eebf3da54f94374846d40b266f5227d",
            "a85287f4b6b749c088305ff41dcb6913",
            "402151eec95b4fbc921508a7b626ca69",
            "933a97ade9554484beaf8027feec2e22",
            "fae0ded9eeda4735983b8610870f64b6",
            "def83d3d7aa44eeabcbba9eaca53564f",
            "e7b010d375ef4cbb9ea7b3c87a5a8a73",
            "ac0c06f242004b1ebbf1438cab35914b",
            "6abe4380b31d4073b62a1d4a23b5a8d2",
            "e317bd384f6340829bd0562930b88374",
            "7f452df6c8524dd9b273d5d02fea0ac0",
            "0909b187346e471a8aea7c85de606c18",
            "9916efc5089a47f1ade900d381446176",
            "8ba9c2d95bdf4d6f93588016eef3e56f",
            "91bc2f38d06847528b5770b97b24d624",
            "b14ce7ca8d3a48728d314cafffc14ecb",
            "b7429902ba7d4016a68f83bb9d5d4b67",
            "60f2ec7ee45946b1aae83f3c5294fce4",
            "a2973c4ceb064f999426a896d24e861e",
            "1245ba36c9074a29b832cd069bcf7cea",
            "027740da05424e3fb7a8bf127b7f67e6",
            "b518f7fa98ae49e68e851b1e8d658bd8"
          ]
        },
        "id": "FzedM_Y5BTMX",
        "outputId": "0d61f702-b7b8-416e-adf7-51b4fe5ff726"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3eebf3da54f94374846d40b266f5227d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/206 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0909b187346e471a8aea7c85de606c18",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/52 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training dataset size: 206\n",
            "Validation dataset size: 52\n"
          ]
        }
      ],
      "source": [
        "from datasets import Dataset\n",
        "import json\n",
        "# Load your JSON file\n",
        "with open('graph_data.json', 'r') as f:\n",
        "    graph_data = json.load(f)\n",
        "\n",
        "# Prepare the dataset (input/output pairs) from your JSON data\n",
        "# Assuming the JSON file has the 'sentence' and 'edges' structure\n",
        "\n",
        "train_data_dict = {\"input\": [], \"output\": []}  # Initialize an empty dictionary with the desired columns\n",
        "for graph in graph_data:\n",
        "    train_data_dict[\"input\"].append(graph.get(\"sentence\", \"\"))\n",
        "    train_data_dict[\"output\"].append(f\"Edges: {graph['edges']}\")\n",
        "\n",
        "# Convert the dictionary into a Hugging Face Dataset\n",
        "dataset = Dataset.from_dict(train_data_dict)\n",
        "\n",
        "train_dataset, val_dataset = dataset.train_test_split(test_size=0.2).values()\n",
        "\n",
        "# Preprocessing function for dataset\n",
        "def preprocess_function(examples):\n",
        "    inputs = examples[\"input\"]\n",
        "    outputs = examples[\"output\"]\n",
        "\n",
        "    model_inputs = tokenizer(inputs, truncation=True, padding=True, max_length=256)\n",
        "    labels = tokenizer(outputs, truncation=True, padding=True, max_length=256)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# Preprocess the dataset\n",
        "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
        "val_dataset = val_dataset.map(preprocess_function, batched=True)\n",
        "print(f\"Training dataset size: {len(train_dataset)}\")\n",
        "print(f\"Validation dataset size: {len(val_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XeeJycYQEcS0"
      },
      "outputs": [],
      "source": [
        "print(train_data[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "WTmvMJ_bCIDD"
      },
      "outputs": [],
      "source": [
        "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
        "# Prepare the model for LoRA fine-tuning\n",
        "lora_config = LoraConfig(\n",
        "    r=16,  # rank of the low-rank matrices\n",
        "    lora_alpha=64,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"  # LoRA fine-tuning for causal language modeling task\n",
        ")\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "model = get_peft_model(model, lora_config)\n",
        "#print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmlivAumUMcJ"
      },
      "source": [
        "##### Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqXtJtGsK5jL"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_arguments = TrainingArguments(\n",
        "    fp16=True,                           # Training computations in 16 bits\n",
        "    # batch-related\n",
        "    per_device_train_batch_size=2,       # Batch Size\n",
        "    gradient_accumulation_steps=4,       # Batch Size (Mathematically)\n",
        "    # optimizer-related\n",
        "    optim=\"paged_adamw_32bit\",           # Variant of AdamW designed to be more efficient on 32-bit GPUs\n",
        "    learning_rate=1e-4,                  # Learning Rate\n",
        "    warmup_ratio=0.05,                   # After 5% of the data, learning rate has linearly  from 0 to 1e-4\n",
        "    lr_scheduler_type=\"cosine\",          # Adjust learning rate sinusoidally\n",
        "    max_grad_norm=0.3,                   # Clip gradients if less than 0.3 (prevent gradient explosion)\n",
        "    # epochs and saving\n",
        "    num_train_epochs=2,                  # Number of Epochs\n",
        "    save_strategy=\"epoch\",               # Save after each epoch\n",
        "    output_dir=\"epoch-finetuned\",        # Where to save the model\n",
        "    # validation\n",
        "    evaluation_strategy=\"steps\",         # For the next argument\n",
        "    eval_steps=0.2,                      # Evaluate after 20% of training steps\n",
        "    # logging-related\n",
        "    logging_steps=1,                     # Number of update steps between two logs\n",
        "    group_by_length=True,                # Minimize padding by grouping sentences of similar length\n",
        "    seed=42,                             # For consistent results\n",
        ")\n",
        "model.gradient_checkpointing_enable()    # Store less activations and recompute later\n",
        "model.config.use_cache = False           # Disable using attention output cache. Should be enabled in inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "UwllVzi6Mo-D",
        "outputId": "9323f0ba-807c-472b-cbdb-26053ac85a1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: trl in /usr/local/lib/python3.10/dist-packages (0.7.1)\n",
            "Collecting trl\n",
            "  Downloading trl-0.11.4-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from trl) (2.5.0+cu121)\n",
            "Requirement already satisfied: transformers>=4.40.0 in /usr/local/lib/python3.10/dist-packages (from trl) (4.44.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from trl) (1.0.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from trl) (3.0.2)\n",
            "Collecting tyro>=0.5.11 (from trl)\n",
            "  Downloading tyro-0.8.14-py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.2 in /usr/local/lib/python3.10/dist-packages (from trl) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (2024.6.1)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.4.0->trl) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.40.0->trl) (0.24.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.40.0->trl) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.40.0->trl) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.40.0->trl) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.40.0->trl) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.40.0->trl) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.40.0->trl) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.40.0->trl) (4.66.5)\n",
            "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (0.16)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (13.9.3)\n",
            "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl)\n",
            "  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->trl) (5.9.5)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (16.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (3.10.10)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (1.16.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.40.0->trl) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.40.0->trl) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.40.0->trl) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.40.0->trl) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.18.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.4.0->trl) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl) (2024.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->trl) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets->trl) (0.2.0)\n",
            "Downloading trl-0.11.4-py3-none-any.whl (316 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m316.6/316.6 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tyro-0.8.14-py3-none-any.whl (109 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m109.8/109.8 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: shtab, tyro, trl\n",
            "  Attempting uninstall: trl\n",
            "    Found existing installation: trl 0.7.1\n",
            "    Uninstalling trl-0.7.1:\n",
            "      Successfully uninstalled trl-0.7.1\n",
            "Successfully installed shtab-1.7.1 trl-0.11.4 tyro-0.8.14\n"
          ]
        }
      ],
      "source": [
        "pip install --upgrade trl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9RvhU8jtLKv4"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "    # tokenizer and model\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    peft_config=lora_config,\n",
        "    max_seq_length=1024,\n",
        "    args=training_arguments,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJ2YGGleUFsC"
      },
      "source": [
        "##### Save Trained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkFvzmRlMrtj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "peft_model_path =os.path.expanduser(\"~/Downloads/fine-tuned-mistral\")\n",
        "os.makedirs(peft_model_path, exist_ok=True)\n",
        "trainer.model.save_pretrained(peft_model_path)\n",
        "tokenizer.save_pretrained(peft_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "562a2edaa8ba461e8617515e7af06814",
            "99e5ae044bf34a1c9092d8dc6a9a6461",
            "e732ecd79ad04c54aba93e967ef13ec0",
            "157614047bdc4587b789a5d418bcd381",
            "ff27741944af4f29aae9e1dc1a421d96",
            "e43baad4de354b14bf583ab54e312d50",
            "02f7c30e892c4bceb203c11aff42e067",
            "b6e3e96935224d368e53ccf1dbfa4049",
            "f7053e1ae0fa4422a468a1d42ef44149",
            "5e35161adb8b4408900e98c8336cf82d",
            "30af5fcd2f524a1b9fba3606795a0d46"
          ]
        },
        "id": "5qocwGrLSiEq",
        "outputId": "0e390316-3526-424a-dbb2-110dd27db497"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "562a2edaa8ba461e8617515e7af06814",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from peft import AutoPeftModelForCausalLM\n",
        "from transformers import AutoTokenizer\n",
        "tuned_model = AutoPeftModelForCausalLM.from_pretrained(peft_model_path, low_cpu_mem_usage=True, torch_dtype=torch.float16, load_in_4bit=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(peft_model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRw0kKFDUAS2"
      },
      "source": [
        "##### Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6jK6uV-OyEB"
      },
      "outputs": [],
      "source": [
        "# Example single sentence for the model to process\n",
        "sentence = \"X is subclass of Y.\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Extract the nodes and relationships from the following sentence.\n",
        "\n",
        "### Input:\n",
        "\"{sentence}\"\n",
        "\n",
        "### Nodes and Relationships:\n",
        "\"\"\"\n",
        "\n",
        "# Encode the prompt\n",
        "input_ids = tokenizer(prompt, return_tensors='pt', truncation=True).input_ids.cuda()\n",
        "outputs = tuned_model.generate(input_ids=input_ids, max_new_tokens=150)\n",
        "output = tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]\n",
        "\n",
        "# Print the generated nodes and relationships\n",
        "print(f'TRAINED MODEL GENERATED RESULT :\\n{output}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "axRxWCdLSb_e"
      },
      "outputs": [],
      "source": [
        "# Example single sentence for the model to process\n",
        "sentence = \"Re-weighting mitigates Algorithmic Bias\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Extract the nodes and relationships from the following sentence.\n",
        "\n",
        "### Input:\n",
        "\"{sentence}\"\n",
        "\n",
        "### Nodes and Relationships:\n",
        "\"\"\"\n",
        "\n",
        "# Encode the prompt\n",
        "input_ids = tokenizer(prompt, return_tensors='pt', truncation=True).input_ids.cuda()\n",
        "outputs = tuned_model.generate(input_ids=input_ids, max_new_tokens=150)\n",
        "output = tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]\n",
        "\n",
        "# Print the generated nodes and relationships\n",
        "print(f'TRAINED MODEL GENERATED RESULT :\\n{output}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crBRXLDgTP5x"
      },
      "outputs": [],
      "source": [
        "# Example single sentence for the model to process\n",
        "sentence = \"Societal and environmental well-being Includes sustainability\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Extract the nodes and relationships from the following sentence.\n",
        "\n",
        "### Input:\n",
        "\"{sentence}\"\n",
        "\n",
        "### Nodes and Relationships:\n",
        "\"\"\"\n",
        "\n",
        "# Encode the prompt\n",
        "input_ids = tokenizer(prompt, return_tensors='pt', truncation=True).input_ids.cuda()\n",
        "outputs = tuned_model.generate(input_ids=input_ids, max_new_tokens=150)\n",
        "output = tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]\n",
        "\n",
        "# Print the generated nodes and relationships\n",
        "print(f'TRAINED MODEL GENERATED RESULT :\\n{output}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNOobaBwTfCk"
      },
      "outputs": [],
      "source": [
        "# Example single sentence for the model to process\n",
        "sentence = \"AI systemsâ€™ resource usage and energy consumption need to be monitored\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Extract the nodes and relationships from the following sentence.\n",
        "\n",
        "### Input:\n",
        "\"{sentence}\"\n",
        "\n",
        "### Nodes and Relationships:\n",
        "\"\"\"\n",
        "\n",
        "# Encode the prompt\n",
        "input_ids = tokenizer(prompt, return_tensors='pt', truncation=True).input_ids.cuda()\n",
        "outputs = tuned_model.generate(input_ids=input_ids, max_new_tokens=150)\n",
        "output = tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]\n",
        "\n",
        "# Print the generated nodes and relationships\n",
        "print(f'TRAINED MODEL GENERATED RESULT :\\n{output}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSkxxyukTsaV"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "6r2T3wQTVCYi",
        "8ekpkRjo3KxM",
        "-34SEPp_lGq6",
        "6-KtvrZ7b8jI",
        "gPUwUOpGUs7x",
        "6ppMfiwPUhoQ",
        "lk7HCW6cUlNq",
        "cmlivAumUMcJ",
        "eJ2YGGleUFsC",
        "SRw0kKFDUAS2"
      ],
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "trust",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "027740da05424e3fb7a8bf127b7f67e6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02f7c30e892c4bceb203c11aff42e067": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0909b187346e471a8aea7c85de606c18": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9916efc5089a47f1ade900d381446176",
              "IPY_MODEL_8ba9c2d95bdf4d6f93588016eef3e56f",
              "IPY_MODEL_91bc2f38d06847528b5770b97b24d624"
            ],
            "layout": "IPY_MODEL_b14ce7ca8d3a48728d314cafffc14ecb"
          }
        },
        "0b598bbbae4045b6b5dab9567ef6175b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1245ba36c9074a29b832cd069bcf7cea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "157614047bdc4587b789a5d418bcd381": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e35161adb8b4408900e98c8336cf82d",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_30af5fcd2f524a1b9fba3606795a0d46",
            "value": "â€‡3/3â€‡[00:09&lt;00:00,â€‡â€‡3.24s/it]"
          }
        },
        "215698777d0848639f8d4f6e66908b81": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b1fb9f04270484585e6599da952e0c7",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_86c56f26f32a40269e80fb0c9fc9a93e",
            "value": "â€‡3/3â€‡[00:10&lt;00:00,â€‡â€‡3.54s/it]"
          }
        },
        "30af5fcd2f524a1b9fba3606795a0d46": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3eebf3da54f94374846d40b266f5227d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a85287f4b6b749c088305ff41dcb6913",
              "IPY_MODEL_402151eec95b4fbc921508a7b626ca69",
              "IPY_MODEL_933a97ade9554484beaf8027feec2e22"
            ],
            "layout": "IPY_MODEL_fae0ded9eeda4735983b8610870f64b6"
          }
        },
        "402151eec95b4fbc921508a7b626ca69": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac0c06f242004b1ebbf1438cab35914b",
            "max": 206,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6abe4380b31d4073b62a1d4a23b5a8d2",
            "value": 206
          }
        },
        "41c087371c554945a4bd24a2964b17a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "562a2edaa8ba461e8617515e7af06814": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_99e5ae044bf34a1c9092d8dc6a9a6461",
              "IPY_MODEL_e732ecd79ad04c54aba93e967ef13ec0",
              "IPY_MODEL_157614047bdc4587b789a5d418bcd381"
            ],
            "layout": "IPY_MODEL_ff27741944af4f29aae9e1dc1a421d96"
          }
        },
        "5e35161adb8b4408900e98c8336cf82d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60f2ec7ee45946b1aae83f3c5294fce4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6abe4380b31d4073b62a1d4a23b5a8d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6c4c236462574357ab7421efe325a9a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9ed3ea3a6a4e42389757f2052f498c8e",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ae94f9fd759c47d3b8db970abcfd1a41",
            "value": 3
          }
        },
        "7b1fb9f04270484585e6599da952e0c7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f452df6c8524dd9b273d5d02fea0ac0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8420489150824a1b9165babf8feff934": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b598bbbae4045b6b5dab9567ef6175b",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_41c087371c554945a4bd24a2964b17a5",
            "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
          }
        },
        "86c56f26f32a40269e80fb0c9fc9a93e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8ba9c2d95bdf4d6f93588016eef3e56f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a2973c4ceb064f999426a896d24e861e",
            "max": 52,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1245ba36c9074a29b832cd069bcf7cea",
            "value": 52
          }
        },
        "91bc2f38d06847528b5770b97b24d624": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_027740da05424e3fb7a8bf127b7f67e6",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_b518f7fa98ae49e68e851b1e8d658bd8",
            "value": "â€‡52/52â€‡[00:00&lt;00:00,â€‡2046.27â€‡examples/s]"
          }
        },
        "933a97ade9554484beaf8027feec2e22": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e317bd384f6340829bd0562930b88374",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_7f452df6c8524dd9b273d5d02fea0ac0",
            "value": "â€‡206/206â€‡[00:00&lt;00:00,â€‡3578.21â€‡examples/s]"
          }
        },
        "9916efc5089a47f1ade900d381446176": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b7429902ba7d4016a68f83bb9d5d4b67",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_60f2ec7ee45946b1aae83f3c5294fce4",
            "value": "Map:â€‡100%"
          }
        },
        "99e5ae044bf34a1c9092d8dc6a9a6461": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e43baad4de354b14bf583ab54e312d50",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_02f7c30e892c4bceb203c11aff42e067",
            "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
          }
        },
        "9ed3ea3a6a4e42389757f2052f498c8e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f1201b9b7814a97a13a45fcb2e6380d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2973c4ceb064f999426a896d24e861e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a85287f4b6b749c088305ff41dcb6913": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_def83d3d7aa44eeabcbba9eaca53564f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e7b010d375ef4cbb9ea7b3c87a5a8a73",
            "value": "Map:â€‡100%"
          }
        },
        "ac0c06f242004b1ebbf1438cab35914b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae94f9fd759c47d3b8db970abcfd1a41": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b14ce7ca8d3a48728d314cafffc14ecb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b518f7fa98ae49e68e851b1e8d658bd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b6e3e96935224d368e53ccf1dbfa4049": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7429902ba7d4016a68f83bb9d5d4b67": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca189e831a2b4694a276f9324504c0e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8420489150824a1b9165babf8feff934",
              "IPY_MODEL_6c4c236462574357ab7421efe325a9a9",
              "IPY_MODEL_215698777d0848639f8d4f6e66908b81"
            ],
            "layout": "IPY_MODEL_9f1201b9b7814a97a13a45fcb2e6380d"
          }
        },
        "def83d3d7aa44eeabcbba9eaca53564f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e317bd384f6340829bd0562930b88374": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e43baad4de354b14bf583ab54e312d50": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e732ecd79ad04c54aba93e967ef13ec0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6e3e96935224d368e53ccf1dbfa4049",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f7053e1ae0fa4422a468a1d42ef44149",
            "value": 3
          }
        },
        "e7b010d375ef4cbb9ea7b3c87a5a8a73": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f7053e1ae0fa4422a468a1d42ef44149": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fae0ded9eeda4735983b8610870f64b6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff27741944af4f29aae9e1dc1a421d96": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
