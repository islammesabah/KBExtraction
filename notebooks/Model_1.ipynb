{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bqxk5tEj7q43"
      },
      "outputs": [],
      "source": [
        "# Install Libraries\n",
        "%%capture\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "# All LangChain libraries for implementing logic chaining\n",
        "%pip install -U langchain\n",
        "%pip install -U langchain_community\n",
        "%pip install -U langchain-huggingface\n",
        "%pip install -U langchain_experimental\n",
        "%pip install -U langchain_openai\n",
        "\n",
        "%pip install -U unstructured\n",
        "%pip install -U sentence-transformers\n",
        "\n",
        "%pip install -U Neo4jGraph\n",
        "%pip install -U py2neo\n",
        "%pip install -U spacy\n",
        "%pip install -U rdflib-neo4j\n",
        "# Langchain\n",
        "from langchain_community.graphs import Neo4jGraph\n",
        "from langchain_community.vectorstores import Neo4jVector\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQAWithSourcesChain\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "from spacy import load, displacy\n",
        "import pandas as pd\n",
        "from google.colab import userdata\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_PJR8Tq388YF"
      },
      "outputs": [],
      "source": [
        "# Load from colab note\n",
        "NEO4J_USERNAME = \"neo4j\"\n",
        "NEO4J_URI = userdata.get('NEO4J_URI')\n",
        "NEO4J_PASSWORD = userdata.get('NEO4J_PASSWORD')\n",
        "HF_API_KEY = userdata.get('HF_API_KEY')\n",
        "\n",
        "# Set up connection to graph instance using LangChain\n",
        "kg = Neo4jGraph(\n",
        "    url=NEO4J_URI,\n",
        "    username=NEO4J_USERNAME,\n",
        "    password=NEO4J_PASSWORD\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ofhgwfbwKX-2"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "\n",
        "# define huggingface generation endpoint\n",
        "hf_llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\", # Model Name\n",
        "    task=\"text-generation\",                       # task as generating a text response\n",
        "    max_new_tokens=150,                           # maximum numbers of generated tokens\n",
        "    do_sample=False,                              # disables sampling\n",
        "    huggingfacehub_api_token=HF_API_KEY           # ðŸ¤— huggingface API token\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Caution: Run, only if you wish to delete the content of database"
      ],
      "metadata": {
        "id": "CiayEI1IHGc2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V90teZPa9hHu",
        "outputId": "a502bfd5-c4b8-4228-f1cc-9edae09f444c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'count(n)': 0}]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# clean the neo4j dataset\n",
        "## All nodes and relationships.\n",
        "kg.query(\"MATCH (n) DETACH DELETE n\")\n",
        "## All indexes and constraints.\n",
        "kg.query(\"CALL apoc.schema.assert({},{},true) YIELD label, key RETURN *\")\n",
        "\n",
        "# check if the dataset empty\n",
        "kg.query(\"MATCH (n) RETURN count(n)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CmgQ6je_Bjs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "collapsed": true,
        "outputId": "bfdd5181-240e-44f3-d226-ca439bb5cf39"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'load' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c77bf9f20938>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m             any(token.dep_ in {'dobj', 'pobj'} for token in doc))\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"en_core_web_sm\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0munhandled_sentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mall_graphs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'load' is not defined"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "allowed_dependencies = {\n",
        "     'acomp','advmod','agent','amod','attr','aux','auxpass',\n",
        "     'case','cc','ccomp','compound','conj','det','dobj',\n",
        "     'nmod','nsubj','nsubjpass',\n",
        "     'pcomp','pobj','prep','poss','ROOT','xcomp'\n",
        "}\n",
        "\n",
        "def has_required_dependencies(doc, allowed_dependencies):\n",
        "    if not {token.dep_ for token in doc}.issubset(allowed_dependencies):\n",
        "        return False\n",
        "\n",
        "    return (\"is a\" in doc.text.lower() or \"is an\" in doc.text.lower()) or \\\n",
        "            (any(token.dep_ == 'ROOT' for token in doc) and \\\n",
        "            any(token.dep_ in {'nsubj', 'nsubjpass'} for token in doc) and \\\n",
        "            any(token.dep_ in {'dobj', 'pobj'} for token in doc))\n",
        "\n",
        "nlp = load(\"en_core_web_sm\")\n",
        "unhandled_sentences=set()\n",
        "all_graphs = []\n",
        "with open(\"DSA_knowledge.txt\", \"r\") as file:\n",
        "    sentences = file.read()\n",
        "\n",
        "for sentence in [s.strip().rstrip(string.punctuation) for s in sentences.strip().split('\\n') if s.strip()]:\n",
        "  doc = nlp(sentence)\n",
        "  # displacy.render(doc, style=\"dep\", jupyter=True, options={'distance': 90})\n",
        "  if not has_required_dependencies(doc, allowed_dependencies):\n",
        "      unhandled_sentences.add(sentence)\n",
        "      continue\n",
        "\n",
        "  try:\n",
        "    temp_graph = {\n",
        "        \"nodes\": {},  # {'nodes': {0: {'pos': 0, 'label': 'X', 'dep': 'nsubj'}, 4: {'pos': 4, 'label': 'Y', 'dep': 'pobj'}},\n",
        "        \"edges\": [],  # 'edges': [(0, 4, 'is subclass of')]}\n",
        "        \"sentence\": sentence\n",
        "    }\n",
        "\n",
        "    edge_mapping = {\n",
        "        'subject_nodes': {},  # {1: {0}} # multiple subject nodes possible\n",
        "        'object_nodes': {},   # {1: 4}\n",
        "        'edge_ids': set()     # {1}\n",
        "    }\n",
        "\n",
        "    temp_graph[\"nodes\"] = {token['id']: {\"pos\": token['id'], \"label\": doc.text, \"dep\": token['dep']}\n",
        "                          for token, doc in zip(doc.to_json()['tokens'], doc)}\n",
        "\n",
        "    temp_graph[\"edges\"] = [(token['head'], token['id'], token['dep'])\n",
        "                            for token in doc.to_json()['tokens'] if token['head'] != token['id']]\n",
        "\n",
        "    root_node = list(filter(lambda node: temp_graph[\"nodes\"][node]['dep'] == 'ROOT', temp_graph[\"nodes\"]))[0]\n",
        "    stopping = False\n",
        "    while not stopping:\n",
        "      for edge in sorted(temp_graph[\"edges\"], key=lambda x: abs(x[0] - x[1])):\n",
        "\n",
        "        source_pos, target_pos, meta = edge\n",
        "\n",
        "        if source_pos not in temp_graph[\"nodes\"] or target_pos not in temp_graph[\"nodes\"]:\n",
        "            continue\n",
        "        #print(edge)\n",
        "        source_metadata = temp_graph[\"nodes\"][source_pos]\n",
        "        target_metadata = temp_graph[\"nodes\"][target_pos]\n",
        "        try:\n",
        "            match (source_metadata, meta, target_metadata):\n",
        "                case {'label': s, **source}, 'compound' | 'amod' | 'aux' |'auxpass' | 'advmod', {'label': t, **target}:\n",
        "                    source_metadata['label'] = f\"{t} {s}\"\n",
        "                    temp_graph['edges'] = list(filter(lambda edge: not (edge[0] == source_pos and edge[1] == target_pos), temp_graph['edges']))\n",
        "                    del temp_graph['nodes'][target_pos]\n",
        "                    continue\n",
        "\n",
        "                case {'label': s, **source}, 'agent', {'label': t, **target}:\n",
        "                    source_metadata['label'] = f\"{s} {t}\"\n",
        "                    next_node = next((n for src, n, label in temp_graph[\"edges\"] if src == target_pos and label == 'pobj'), None)\n",
        "                    edge_mapping['edge_ids'].add(source_pos)\n",
        "                    edge_mapping['object_nodes'][source_pos] = next_node\n",
        "                    temp_graph['edges'].append((source_pos, next_node, 'pobj'))\n",
        "                    temp_graph['edges'] = list(filter(lambda edge: not (edge[0] == source_pos and edge[1] == target_pos), temp_graph['edges']))\n",
        "                    temp_graph['edges'] = list(filter(lambda edge: not (edge[0] == target_pos and edge[1] == next_node), temp_graph['edges']))\n",
        "                    del temp_graph['nodes'][target_pos]\n",
        "                    continue\n",
        "\n",
        "                case {'label': s, **source}, 'case' | 'cc', {'label': t, **target}:\n",
        "                    temp_graph['edges'] = list(filter(lambda edge: not (edge[0] == source_pos and edge[1] == target_pos), temp_graph['edges']))\n",
        "                    del temp_graph['nodes'][target_pos]\n",
        "                    continue\n",
        "\n",
        "                case {'label': s, **source}, 'det', {'label': t, **target}:\n",
        "                    temp_graph['nodes'][source_pos]['det'] = t\n",
        "                    temp_graph['edges'] = list(filter(lambda edge: not (edge[0] == source_pos and edge[1] == target_pos), temp_graph['edges']))\n",
        "                    del temp_graph['nodes'][target_pos]\n",
        "                    continue\n",
        "\n",
        "                case {'label': s, **source}, 'attr'|'acomp', {'label': 'subclass'|'attribute'|'dimension'|'kind'|'threat'|'result'|'type'|'equal'|'form', **target}: #is(head)--attr--subclass(tail)--prep--of(child)--pobj--Risk\n",
        "                    next_node = next((n for src, n, label in temp_graph[\"edges\"] if src == target_pos and label == 'prep'), None)\n",
        "                    obj_node = next((n for src, n, label in temp_graph[\"edges\"] if src == next_node and label == 'pobj'), None)\n",
        "                    source_metadata['label'] = f\"{s} {target_metadata['label']} {temp_graph['nodes'][next_node]['label']}\" #is-->issubclassof\n",
        "                    edge_mapping['edge_ids'].add(source_pos)\n",
        "                    edge_mapping['object_nodes'][source_pos] = obj_node\n",
        "                    temp_graph['edges'] = list(filter(lambda edge: not (edge[0] == source_pos and edge[1] == target_pos), temp_graph['edges'])) # remove edge: is--subclass\n",
        "                    temp_graph['edges'] = list(filter(lambda edge: not (edge[0] == target_pos and edge[1] == next_node), temp_graph['edges'])) # remove edge: subclass--of\n",
        "                    temp_graph['edges'] = list(filter(lambda edge: not (edge[0] == next_node and edge[1] == obj_node), temp_graph['edges'])) # remove edge: of--Y\n",
        "                    temp_graph['edges'].append((source_pos, obj_node, 'pobj')) #connect edge from 'is' node to obj node\n",
        "                    del temp_graph['nodes'][target_pos] # remove node: 'subclass'\n",
        "                    del temp_graph['nodes'][next_node]  # remove node: 'of'\n",
        "                    continue\n",
        "\n",
        "                case {'label': s, **source}, 'attr'|'acomp', {'label': t, **target}: #is-attr-Y\n",
        "                    edge_mapping['edge_ids'].add(source_pos)\n",
        "                    edge_mapping['object_nodes'][source_pos] = target_pos\n",
        "                    continue\n",
        "\n",
        "                case {'dep': 'ROOT', 'label': s, **source}, 'prep'|'xcomp', {'label': t, **target}: #attributes(ROOT)--prep--to #helps--xcomp--see--pobj--X\n",
        "                    source_metadata['label'] = f\"{s} {t}\"\n",
        "                    next_node = next((n for src, n, label in temp_graph[\"edges\"] if src == target_pos and label in {'pobj', 'dobj'}), None)\n",
        "                    if next_node:\n",
        "                      temp_graph['edges'].append((source_pos, next_node, 'pobj'))\n",
        "                      temp_graph['edges'] = list(filter(lambda edge: not (edge[0] == source_pos and edge[1] == target_pos), temp_graph['edges']))\n",
        "                      temp_graph['edges'] = list(filter(lambda edge: not (edge[0] == target_pos and edge[1] == next_node), temp_graph['edges']))\n",
        "                      del temp_graph['nodes'][target_pos]\n",
        "                    continue\n",
        "\n",
        "                case {'label': s, **source}, 'prep', {'label': t, **target}: #*-dobj-assessment--prep--of|*-attr-(a)dimension-prep-of\n",
        "                    if next((n for src, n, label in temp_graph[\"edges\"] if n == source_pos and label == 'attr'), None) is None:\n",
        "                      next_node = next((n for src, n, label in temp_graph[\"edges\"] if src == target_pos and label in {'pobj'}), None)\n",
        "                      if next_node: #Date-prep-of-pobj-birth\n",
        "                        source_metadata['label'] = f\"{s} {t} {temp_graph['nodes'][next_node]['label']}\"\n",
        "                        temp_graph['edges'] = list(filter(lambda edge: not (edge[0] == source_pos and edge[1] == target_pos), temp_graph['edges']))\n",
        "                        temp_graph['edges'] = list(filter(lambda edge: not (edge[0] == target_pos and edge[1] == next_node), temp_graph['edges']))\n",
        "                        del temp_graph['nodes'][target_pos]\n",
        "                        del temp_graph['nodes'][next_node]\n",
        "                      else:\n",
        "                        edge_mapping['edge_ids'].add(target_pos)\n",
        "                        edge_mapping['subject_nodes'].setdefault(target_pos, set()).add(source_pos)\n",
        "                        temp_graph['edges'] = list(filter(lambda edge: not (edge[0] == source_pos and edge[1] == target_pos), temp_graph['edges']))\n",
        "                    continue\n",
        "\n",
        "                case {'label': s, **source}, 'poss', {'label': t, **target}:\n",
        "                    next_node = next((n for src, n, label in temp_graph[\"edges\"] if src == source_pos and label == 'conj'), None)\n",
        "                    if next_node:\n",
        "                      temp_graph['edges'].append((temp_graph[\"nodes\"][next_node]['label'],\n",
        "                                                  temp_graph[\"nodes\"][target_pos]['label'],\n",
        "                                                  'of'))\n",
        "                    continue\n",
        "\n",
        "                case {'label': s, **source}, 'nmod', {'label': t, **target}:\n",
        "                    source_metadata['label'] = f\"{t} {s}\"\n",
        "                    incoming_node = next((src for src, n, label in temp_graph[\"edges\"] if target == source_pos and label == 'nsubj'), None)\n",
        "                    if 'conj' in target:\n",
        "                        target['conj']['nodeId'] = target['conj']['text'] + f\" {s}\"\n",
        "                        edge_mapping['subject_nodes'][incoming_node].add(target['conj']['nodeId'])\n",
        "                    temp_graph['edges'] = list(filter(lambda edge: not (edge[0] == source_pos and edge[1] == target_pos), temp_graph['edges']))\n",
        "                    del temp_graph['nodes'][target_pos]\n",
        "                    continue\n",
        "\n",
        "                case {'label': s, **source}, 'conj', {'label': t, **target}:\n",
        "                    temp_graph['edges'] = list(filter(lambda edge: not (edge[0] == source_pos and edge[1] == target_pos), temp_graph['edges']))\n",
        "                    temp_graph['nodes'][source_pos]['conj'] = {'text': t, 'nodeId': target_pos}\n",
        "                    continue\n",
        "\n",
        "                case {'label': s, **source}, 'pcomp', {'label': t, **target}: #in--pcomp--explaining--dobj--x\n",
        "                    temp_graph['edges'] = list(filter(lambda edge: not (edge[0] == source_pos and edge[1] == target_pos), temp_graph['edges']))\n",
        "                    next_node = next((n for src, n, label in temp_graph[\"edges\"] if src == target_pos and label in {'pobj', 'dobj'}), None)\n",
        "                    if next_node:\n",
        "                      temp_graph['nodes'][root_node]['label'] += f\" {t}\"  #if not work f\" {temp_graph['nodes'][root_node]['label']} {t}\n",
        "                      temp_graph['edges'].append((root_node, next_node, 'dobj'))\n",
        "                      temp_graph['edges'] = list(filter(lambda edge: not (edge[0] == target_pos and edge[1] == next_node), temp_graph['edges']))\n",
        "                      del temp_graph['nodes'][source_pos]\n",
        "                      del temp_graph['nodes'][target_pos]\n",
        "                    continue\n",
        "\n",
        "                case {'label': s, **source}, 'ccomp', {'label': t, **target}: #Design interface can help users understand AI decisions\n",
        "                    next_node = next((n for src, n, label in temp_graph[\"edges\"] if src == target_pos and label == 'nsubj'), None)\n",
        "                    if next_node:\n",
        "                      edge_mapping['object_nodes'][source_pos] = next_node\n",
        "                    continue\n",
        "\n",
        "                case {'label': s, **source}, 'nsubj' | 'nsubjpass', {'label': t, **target}:\n",
        "                    edge_mapping['edge_ids'].add(source_pos)\n",
        "                    edge_mapping['subject_nodes'].setdefault(source_pos, set()).add(target_pos)\n",
        "                    if 'conj' in target:\n",
        "                      edge_mapping['subject_nodes'][source_pos].add(target_metadata['nodeId'])\n",
        "                    continue\n",
        "\n",
        "                case {'label': s, **source}, 'dobj' | 'pobj', {'label': t, **target}:\n",
        "                    if next((src for src, n, label in temp_graph[\"edges\"] if src == source_pos and label == 'prep'), None):\n",
        "                      source_metadata['label'] = f\"{s} {t}\"\n",
        "                      temp_graph['edges'] = list(filter(lambda edge: not (edge[0] == source_pos and edge[1] == target_pos), temp_graph['edges']))\n",
        "                      del temp_graph['nodes'][target_pos]\n",
        "                    #assign object outside loop\n",
        "                    continue\n",
        "\n",
        "                case another:\n",
        "                    print(\"another:\", edge)\n",
        "                    unhandled_sentences.add(sentence)\n",
        "                    stopping = True\n",
        "                    continue\n",
        "\n",
        "        except Exception as e:\n",
        "                print(f\"Error occurred in sentence: {sentence} with edge: {edge}, error: {e}\")\n",
        "                unhandled_sentences.add(sentence) # throw error\n",
        "                stopping = True\n",
        "                continue\n",
        "      else:\n",
        "          break\n",
        "\n",
        "    # Update object nodes\n",
        "    edge_mapping['object_nodes'].update({\n",
        "        edge_id: next((tail for head, tail, meta in temp_graph['edges']\n",
        "                      if meta in {'dobj', 'pobj'}), None)\n",
        "        for edge_id in edge_mapping['edge_ids']\n",
        "        if edge_id not in edge_mapping['object_nodes']\n",
        "    })\n",
        "\n",
        "    for edge_id, obj_node in edge_mapping['object_nodes'].items():\n",
        "        if obj_node is None:\n",
        "            print(f\"Missing object node for edge ID: {edge_id}\")\n",
        "\n",
        "    # create final mapping\n",
        "    for edge_id in edge_mapping['edge_ids']:\n",
        "        subject_nodes = edge_mapping['subject_nodes'][edge_id]\n",
        "        object_node = edge_mapping['object_nodes'][edge_id]\n",
        "        edge_node = temp_graph['nodes'][edge_id]\n",
        "        for subject_node in subject_nodes:\n",
        "            #temp_graph['edges'].append((subject_node, object_node, edge_node['label']))\n",
        "            temp_graph['edges'].append((temp_graph[\"nodes\"][subject_node]['label'],\n",
        "                                        temp_graph[\"nodes\"][object_node]['label'],\n",
        "                                        edge_node['label']))\n",
        "            temp_graph['edges'] = list(filter(lambda edge: not (edge[0] == edge_id and edge[1] == subject_node), temp_graph['edges']))\n",
        "\n",
        "        temp_graph['edges'] = list(filter(lambda edge: not (edge[0] == edge_id and edge[1] == object_node), temp_graph['edges']))\n",
        "        del temp_graph['nodes'][edge_id]\n",
        "\n",
        "    temp_graph['edges'] = list(set(temp_graph['edges'])-set([edge for edge in temp_graph['edges'] if edge[2] in allowed_dependencies]))\n",
        "\n",
        "    #all_graphs.append(temp_graph)\n",
        "    all_graphs.append({\n",
        "      \"edges\": temp_graph[\"edges\"],\n",
        "      \"sentence\": temp_graph[\"sentence\"]\n",
        "    })\n",
        "    for source, target, edge in temp_graph['edges']:\n",
        "        kg.query(\n",
        "          \"\"\"\n",
        "          CALL apoc.merge.node([$source_label], {label:$source_label}) YIELD node AS s\n",
        "          CALL apoc.merge.node([$target_label], {label:$target_label}) YIELD node AS t\n",
        "          CALL apoc.merge.relationship(s, $edge_label, {sentence: $sentence}, {}, t, {}) YIELD rel\n",
        "          RETURN s,t,rel\n",
        "          \"\"\"\n",
        "          , params={\n",
        "          'source_label': temp_graph['nodes'][source]['label'].replace(\" \", \"_\"),\n",
        "          'target_label': temp_graph['nodes'][target]['label'].replace(\" \", \"_\"),\n",
        "          'edge_label': edge.lower().replace(\" \", \"_\"),\n",
        "          'sentence': sentence\n",
        "        })\n",
        "\n",
        "  except Exception as e:\n",
        "    print(f\"Failed to process sentence: {sentence}, error: {e}\")\n",
        "    unhandled_sentences.add(sentence)\n",
        "\n",
        "\n",
        "with open(\"unhandled_sentences.txt\", \"w\") as file:\n",
        "  for unhandled in unhandled_sentences:\n",
        "    file.write(unhandled + \"\\n\")\n",
        "\n",
        "# Training data - for fine tuning the HF model:\n",
        "# Structure of json file is to be updated(current json edge has position of connected node, which we will change to node name)\n",
        "with open('graph_data.json', 'w') as json_file:\n",
        "    json.dump(all_graphs, json_file, indent=4, ensure_ascii=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "# load JSON file\n",
        "def load_json(path):\n",
        "  with open(path, 'r') as file:\n",
        "    return json.load(file)\n",
        "\n",
        "import textwrap\n",
        "# Prints the text with lines wrapped to a maximum width of 80 characters\n",
        "def clean_print(text):\n",
        "    return print(textwrap.fill(text, width=80))"
      ],
      "metadata": {
        "id": "nW3SQ6YekDti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGofGtCqZjrG"
      },
      "outputs": [],
      "source": [
        "\n",
        "CYPHER_GENERATION_TEMPLATE = \"\"\"Task:\n",
        "Generate Cypher statement to query a Neo4j graph database.\n",
        "\n",
        "Instructions:\n",
        "* Only use the provided relationship types, node labels, and properties in the schema.\n",
        "* Do not use any other relationship types, properties, or node labels that are not provided.\n",
        "* Always follow the correct relationship direction.\n",
        "* Ensure that the query follows the correct Cypher syntax.\n",
        "\n",
        "Schema:\n",
        "{schema}\n",
        "\n",
        "Examples:\n",
        "Here are a few examples of generated Cypher statements for particular questions:\n",
        "\n",
        "# What is Bias?\n",
        "    MATCH (s:Bias)-[r]-(t)\n",
        "    RETURN s,r,t\n",
        "\n",
        "# What might introduce Bias?\n",
        "    MATCH (s)-[r:might_introduce]->(t:Bias)\n",
        "    RETURN s,r,t\n",
        "\n",
        "The question is:\n",
        "{question}\n",
        "\n",
        "The generated Cypher statement:\"\"\"\n",
        "\n",
        "# Define schema\n",
        "schema = \"\"\"Node: Risk\n",
        "Properties: label\n",
        "Relationships: (Bias)-[:is]->(Risk)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bfEiuatJTof"
      },
      "outputs": [],
      "source": [
        "# build the query prompt template\n",
        "from langchain.prompts.prompt import PromptTemplate\n",
        "CYPHER_GENERATION_PROMPT = PromptTemplate(\n",
        "    input_variables=[\"schema\", \"question\"],\n",
        "    template=CYPHER_GENERATION_TEMPLATE\n",
        ")\n",
        "\n",
        "# initilize the chain\n",
        "from langchain.chains import GraphCypherQAChain\n",
        "cypherChain = GraphCypherQAChain.from_llm(\n",
        "    graph=kg,\n",
        "    llm=hf_llm,\n",
        "    #cypher_llm=hf_llm,                    # see intermediate steps\n",
        "    cypher_prompt=CYPHER_GENERATION_PROMPT,   # cypher generation prompt\n",
        "    verbose=True,\n",
        "    allow_dangerous_requests=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuWDDHqrQ4fe",
        "outputId": "97482505-0d79-4506-dd26-5ce89324789a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
            "Generated Cypher:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "MATCH (s:Requirement)-[r]-(t)\n",
            "RETURN s,r,t\n",
            "\u001b[0m\n",
            "Full Context:\n",
            "\u001b[32;1m\u001b[1;3m[{'s': {'label': 'Requirement'}, 'r': ({'label': 'Fairness'}, 'is', {'label': 'Requirement'}), 't': {'label': 'Fairness'}}, {'s': {'label': 'Requirement'}, 'r': ({'label': 'Explainability'}, 'is', {'label': 'Requirement'}), 't': {'label': 'Explainability'}}]\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Question: What is Requirement?\n",
            "Response:  Fairness and Explainability are Requirements.\n"
          ]
        }
      ],
      "source": [
        "# test the chain\n",
        "question = \"What is Requirement?\"\n",
        "res = cypherChain.run(question)\n",
        "clean_print('Question: '+question)\n",
        "clean_print('Response: '+res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5VozjCoN8sy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be06af75-a51d-40ef-ef88-8490bdd4b642"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
            "Generated Cypher:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "MATCH (s:Bias)-[r]-(t)\n",
            "RETURN s,r,t\n",
            "\u001b[0m\n",
            "Full Context:\n",
            "\u001b[32;1m\u001b[1;3m[{'s': {'label': 'Bias'}, 'r': ({'label': 'Bias'}, 'is', {'label': 'Risk'}), 't': {'label': 'Risk'}}, {'s': {'label': 'Bias'}, 'r': ({'label': 'Algorithmic_Bias'}, 'is_subclass_of', {'label': 'Bias'}), 't': {'label': 'Algorithmic_Bias'}}, {'s': {'label': 'Bias'}, 'r': ({'label': 'Historical_Bias'}, 'is_subclass_of', {'label': 'Bias'}), 't': {'label': 'Historical_Bias'}}, {'s': {'label': 'Bias'}, 'r': ({'label': 'TrainTestSplit'}, 'might_introduce', {'label': 'Bias'}), 't': {'label': 'TrainTestSplit'}}, {'s': {'label': 'Bias'}, 'r': ({'label': 'Bias'}, 'is_threat_to', {'label': 'Fairness'}), 't': {'label': 'Fairness'}}]\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Question: What is Bias?\n",
            "Response:  Bias is a type of risk. It can be algorithmic bias, historical bias,\n",
            "or it can be introduced by the train-test split. Bias is a threat to fairness.\n"
          ]
        }
      ],
      "source": [
        "question = \"What is Bias?\"\n",
        "res = cypherChain.run(question)\n",
        "clean_print('Question: '+question)\n",
        "clean_print('Response: '+res)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}