{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74bSNuM04Stm"
      },
      "source": [
        "Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain_huggingface\n",
        "!pip install -q langchain-community\n",
        "!pip install -q \"unstructured[local-inference]\"\n",
        "!pip install -q pymupdf"
      ],
      "metadata": {
        "id": "NgK7vCWO5XRE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b58402d0-4a88-493c-b2f3-bef9fc59b711"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/2.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.7/2.4 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m409.5/409.5 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /packages/50/7d/8ff52a37beb75874b733ae3197345479b53a112ba504b8d8e4ea8f48467e/rapidfuzz-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /simple/backoff/\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m48.4/48.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m112.5/112.5 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m586.9/586.9 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m486.9/486.9 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m274.9/274.9 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.9/62.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m159.9/159.9 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.2/59.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Connect to the models"
      ],
      "metadata": {
        "id": "T3ZWlkYi0NwD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "from langchain.document_loaders import UnstructuredPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema import BaseOutputParser\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "from typing import List\n",
        "from openai import OpenAI\n",
        "import random\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "xaBOR28L7oSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WaEwbwSi4Eio"
      },
      "outputs": [],
      "source": [
        "# get your HF TOKEN from https://huggingface.co/settings/tokens/new?tokenType=read\n",
        "HF_API_KEY = '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'\n",
        "\n",
        "# for now mistral model works but we will need to update\n",
        "# define huggingface generation endpoint\n",
        "hf_llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\", # Model Name\n",
        "    task=\"text-generation\",                       # task as generating a text response\n",
        "    max_new_tokens=150,                           # maximum numbers of generated tokens\n",
        "    do_sample=False,                              # disables sampling\n",
        "    huggingfacehub_api_token=HF_API_KEY           # ğŸ¤— huggingface API token\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# use LLaMA model internal server\n",
        "#set the base url to the local server\n",
        "# client = OpenAI(base_url=\"WILL PROVIDE THE URL\",\n",
        "                # api_key=\"\") #setting an api key is required from the openai framework, but the server itself does not use it\n",
        "\n",
        "#get a list of models hosted on the local server\n",
        "# print(client.models.list())\n",
        "\n",
        "# Call the ChatCompletion API\n",
        "# completion = client.chat.completions.create(\n",
        "#       model=\"meta-llama-3.1-70b-instruct-fp8\",\n",
        "#       messages=[\n",
        "#           {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "#           {\"role\": \"user\", \"content\": \"Hello, are you there?\"}\n",
        "#           ])\n",
        "\n",
        "#print the response\n",
        "\n",
        "# print(completion.choices[0].message.content)"
      ],
      "metadata": {
        "id": "1loEPS9B0ovI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the data"
      ],
      "metadata": {
        "id": "Eunw-Iw42ZaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_data_file = 'sentence_data.txt'\n",
        "pdf_path = '20241015_MISSION_KI_Glossar_v1.0 en.pdf'"
      ],
      "metadata": {
        "id": "-scWCtcv2ZJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Few shot examples"
      ],
      "metadata": {
        "id": "sCsoI1kV17cn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_examples = [\n",
        "    {\n",
        "        \"sentence\":\"\"\"Transparency\n",
        "Property of an â†’KI system that is explainable and comprehensible. In the context of this quality\n",
        "standard, \"transparency\" also includes documentation of the properties of the â†’KI system.\"\"\",\n",
        "        \"output\":\"\"\"1-Transparency is a property of KI system. 2-Transparency is explainable. 3-Transparency is comprehensible. 4-Transparency includes documentation of properties of KI system.\"\"\"\n",
        "    },\n",
        "    {\n",
        "        \"sentence\":\"\"\"opacity\n",
        "opaqueness:\n",
        "Property of a system that appropriate information about the system is unavailable to relevant stakeholders.\"\"\",\n",
        "        \"output\":\"\"\"1-Opacity also called Opaqueness. 2-Opacity is a Property of a system. 3-Property of a system has characteristic Information unavailable to stakeholders.\"\"\"\n",
        "    },\n",
        "    {\n",
        "        \"sentence\":\"\"\"This requirement is closely linked with the principle of explicability and encompasses transparency of elements relevant to an AI system: the data, the system and the business models.\"\"\",\n",
        "        \"output\":\"\"\"1-Requirement is linked with principle of explicability. 2-Requirement encompasses Transparency of elements. 3-Transparency is relevant to AI system. 4-Elements include Data. 5-Elements include System. 6-Elements include Business models.\"\"\"\n",
        "    },\n",
        "]\n",
        "\n",
        "chunks_examples = [\n",
        "    {\n",
        "        \"chunk\":\"\"\"behavior and/or functioning of this â†’AI system in principle and during operation and, if necessary, to\n",
        "terminate it.\n",
        "Monitoring\n",
        "Procedure in which deviations between observable actual states and the desired target states are detected\n",
        "during the operation of an â†’KI system.\n",
        "Non-discrimination\n",
        "Characteristic of an open process carried out by an â†’KI system if, in the course of this process, several\n",
        "human individuals are treated in comparison with each other and this process is carried out in an open\n",
        "process.\n",
        "is legally free from the mistreatment of a human individual on the basis of a legally protected\n",
        "characteristic.\n",
        "User information\"\"\",\n",
        "        \"output\":\"\"\"1-AI system has property functioning in principle and during operation. 2-Monitoring is deviations between observable actual states and desired target states. 3-Deviations occur during operation of an KI system. 4-Monitoring is a procedure for detecting deviations during operation (Optional, better to have). 5-Non-discrimination is a characteristic of an open process by an KI system. 6-Open process treats several human individuals in comparison with each other. 7-Non-discrimination ensures no mistreatment of a human individual. 8-Mistreatment is based on legally protected characteristics.\"\"\"\n",
        "    },\n",
        "    {\n",
        "        \"chunk\":\"\"\"characteristic.\n",
        "User information\n",
        "Characteristics of an â†’AI system with regard to the quality of information, interaction and operation by a\n",
        "user, including knowledge of the involvement of AI, barriers, and the quality of the user experience.\n",
        "freedom and with a view to preventing nudging.\n",
        "Robustness\n",
        "Ability of an â†’AI system to maintain its regular and usual behavior and functioning in the best possible\n",
        "way even in the event of non-malicious, adverse, disruptive or faulty inputs or external influences.\n",
        "to keep.\n",
        "Traceability\n",
        "Property of an â†’KI system with regard to the ability to record the consecutive sequence of all decisions\"\"\",\n",
        "        \"output\":\"\"\"1-AI system has characteristic User information. 2-User information relates to quality of information, interaction and operation by a user. 3-AI system includes Knowledge of AI involvement. 4-AI system includes barriers. 5-AI system includes quality of the user experience. 6-AI system has ability Robustness. 7-Robustness allows regular and usual behavior in adverse conditions. 8-AI system maintains behavior even under faulty inputs or external influences. 9-AI system has property Traceability. 10-Traceability relates to recording consecutive sequence of all decisions.\"\"\"\n",
        "    },\n",
        "    {\n",
        "        \"chunk\":\"\"\"that enter or have entered an â†’KI system along the entire life cycle.\n",
        "Transparency\n",
        "Property of an â†’KI system that is explainable and comprehensible. In the context of this quality\n",
        "standard, \"transparency\" also includes documentation of the properties of the â†’KI system.\"\"\",\n",
        "        \"output\":\"\"\"1-Transparency is a property of KI system. 2-Transparency is explainable. 3-Transparency is comprehensible. 4-Transparency includes documentation of properties of the KI system.\"\"\"\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "5yiH3gzE1-Aa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtMP2UrSDRHs"
      },
      "source": [
        "Prompt Engineering for Sentence Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# please keep a record for all prompts you try\n",
        "query = \"\"\"<s>[INST]you are a helpful assistant. You will help me extract requirements for an AI system based on the sentences provided to you.\n",
        "\n",
        "Your answer must be structured as a JSON string. Do not answer in any other way, e.g. as normal text. Here is the structure:\n",
        "{{\"requirement\": [Requirment you extracted goes here]}}\n",
        "\n",
        "if you do not adhere to this structure everything will fail.\n",
        "Make sure to never use other information except the text provided to you. Only use the reference text given to you for extracting the requirements.\n",
        "Do not hallucinate. This is very important.\n",
        "Try to be consice and to the point, because you are going to extract only requirements.\n",
        "Don't write ``` at the beginning or end of your answer.\n",
        "\n",
        "Here are some examples:\n",
        "[/INST]\n",
        "{examples}\n",
        "</s>\n",
        "[INST]sentence:{user_query}[/INST]\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(query)\n",
        "\n",
        "class NewLineSeparatedOutputParser(BaseOutputParser):\n",
        "    def parse(self, text: str) -> List[str]:\n",
        "        return text.strip().split('\\n')\n",
        "\n",
        "simple_rag_chain = (\n",
        " prompt                                   # build the prompt\n",
        " | hf_llm                                 # llm for generation\n",
        "#  | NewLineSeparatedOutputParser()         # collect the response into list\n",
        ")"
      ],
      "metadata": {
        "id": "Wirp8mGHoUv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_examples(sentence_examples):\n",
        "    examples = \"\"\n",
        "    for ex in sentences_examples:\n",
        "        examples += f\"\"\"\n",
        "sentence: \"{ex[\"sentence\"]}\"\n",
        "{{\n",
        "\"requirement\" : \"{ex[\"output\"]}\"\n",
        "}}\n",
        "\"\"\"\n",
        "    return examples\n",
        "\n",
        "with open(\"./sentence_data.txt\") as f:\n",
        "    text = f.read()\n",
        "    new_sentences = list(filter(lambda x: x.strip(), text.split(\"===================\")))\n",
        "\n",
        "outputs = []\n",
        "for sentence in tqdm(new_sentences):\n",
        "    outputs.append(simple_rag_chain.invoke({\"examples\":create_examples(sentences_examples), \"user_query\":f'\"{sentence}\"'}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylZ-lLoC4dc5",
        "outputId": "298397fc-2d7e-4699-a446-f25b3d67d2c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:25<00:00,  1.62s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_result = pd.DataFrame({\"source_sentence\":new_sentences, \"llm_output\":outputs})\n",
        "df_result.to_csv(\"./sentence_predictions.csv\")\n",
        "df_result.iloc[0]"
      ],
      "metadata": {
        "id": "gYXNvvtgA9J_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "outputId": "9b44696d-77fa-4208-dfc0-fe3e5a2aa6dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "source_sentence    \\nQuality dimension\\nA quality dimension denot...\n",
              "llm_output         {\\n\"requirement\" : \"1-Quality dimension denote...\n",
              "Name: 0, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>source_sentence</th>\n",
              "      <td>\\nQuality dimension\\nA quality dimension denot...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>llm_output</th>\n",
              "      <td>{\\n\"requirement\" : \"1-Quality dimension denote...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt Engineering for Chunk Data"
      ],
      "metadata": {
        "id": "XZOdewoPwQKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for chunks load the pdf file and do the chunking and select rondom thirty chunks\n",
        "loader = UnstructuredPDFLoader(pdf_path)\n",
        "data = loader.load()\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=70)\n",
        "documents = text_splitter.split_documents(data)\n",
        "random.seed(42)\n",
        "num_chunks_to_select = 15  # Number of random chunks you want\n",
        "random_chunks = random.sample(documents, num_chunks_to_select)"
      ],
      "metadata": {
        "id": "Ud0fBJZnwMqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# please keep a record for all prompts you try\n",
        "query = \"\"\"<s>[INST]you are a helpful assistant. You will help me extract requirements for an AI system based on the short text provided to you.\n",
        "\n",
        "Your answer must be structured as a JSON string. Do not answer in any other way, e.g. as normal text. Here is the structure:\n",
        "{{\"requirement\": [Requirment you extracted goes here]}}\n",
        "\n",
        "if you do not adhere to this structure everything will fail.\n",
        "Make sure to never use other information except the text provided to you. Only use the reference text given to you for extracting the requirements.\n",
        "Do not hallucinate. This is very important.\n",
        "Try to be consice and to the point, because you are going to extract only requirements.\n",
        "Don't write ``` at the beginning or end of your answer.\n",
        "\n",
        "Only answer with one json like mentioned, don't output multiple json structured strings.\n",
        "If the text doesn't provide specific AI system requirements for any reason, just leave the \"requirement\" data in the json output empty string. Refrain from any further explanation. The empty string is completely enough.\n",
        "Don't write long requirements. If it is more than a sentence, just break it up and create a new requirement from it.\n",
        "If multiple instances/aspects of an idea is present in the text, split them up and make distinct requirements for each.\n",
        "\n",
        "Here are some examples:\n",
        "[/INST]\n",
        "{examples}\n",
        "</s>\n",
        "[INST]text:{user_query}[/INST]\n",
        "\"\"\"\n",
        "\n",
        "# NOTES:\n",
        "    # Examples are impacting the llm's output in a bad way. It is extracting general terms from specific examples.\n",
        "    # sometimes it outputs more than one json\n",
        "    # sometimes there is some explanation after the json\n",
        "    # some requirements are long, it's better to break them up\n",
        "    # sometimes, llm writes every point in a seperate json\n",
        "\n",
        "# Possible prompt improvements:\n",
        "    # Don't just repeat the text given to you, extract only important requirements from it.\n",
        "    # Say you are seeing only chunk of the text\n",
        "\n",
        "prompt = PromptTemplate.from_template(query)\n",
        "\n",
        "class NewLineSeparatedOutputParser(BaseOutputParser):\n",
        "    def parse(self, text: str) -> List[str]:\n",
        "        return text.strip().split('\\n')\n",
        "\n",
        "hf_llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\", # Model Name\n",
        "    task=\"text-generation\",                       # task as generating a text response\n",
        "    max_new_tokens=200,                           # maximum numbers of generated tokens\n",
        "    do_sample=False,                              # disables sampling\n",
        "    huggingfacehub_api_token=HF_API_KEY           # ğŸ¤— huggingface API token\n",
        ")\n",
        "\n",
        "simple_rag_chain = (\n",
        " prompt                                   # build the prompt\n",
        " | hf_llm                                 # llm for generation\n",
        "#  | NewLineSeparatedOutputParser()         # collect the response into list\n",
        ")"
      ],
      "metadata": {
        "id": "7rmgqGrx0eY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_examples(chunk_examples):\n",
        "    examples = \"\"\n",
        "    for ex in chunk_examples:\n",
        "        examples += f\"\"\"\n",
        "text: \"{ex[\"chunk\"]}\"\n",
        "{{\n",
        "\"requirement\" : \"{ex[\"output\"]}\"\n",
        "}}\n",
        "    \"\"\"\n",
        "    return examples\n",
        "\n",
        "\n",
        "outputs = []\n",
        "for chunk in tqdm(random_chunks):\n",
        "    outputs.append(simple_rag_chain.invoke({\"examples\":create_examples(chunks_examples), \"user_query\":f'\"{chunk.page_content}\"'}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50f146bf-b848-402b-f3ea-4c626ebc30f7",
        "id": "RFJO5hna0eY6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:33<00:00,  2.23s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for i, o in enumerate(outputs):\n",
        "#     try:\n",
        "#         json.loads(o)\n",
        "#     except json.JSONDecodeError as e:\n",
        "#         print(i)\n",
        "#         raise e\n",
        "\n",
        "df_result = pd.DataFrame({\"source_chunk\":[ch.page_content for ch in random_chunks], \"llm_output\":outputs})\n",
        "df_result.to_csv(\"./chunk_predictions.csv\")\n",
        "df_result.iloc[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "TfdNLNRO3Bvf",
        "outputId": "3a988535-79ce-4f4b-9da9-f2161a250da2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "source_chunk    AI component An \"AI component\" comprises an im...\n",
              "llm_output      {\\n\"requirement\" : \"1-AI component includes an...\n",
              "Name: 0, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>source_chunk</th>\n",
              "      <td>AI component An \"AI component\" comprises an im...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>llm_output</th>\n",
              "      <td>{\\n\"requirement\" : \"1-AI component includes an...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U3OrigqC0Sn2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}