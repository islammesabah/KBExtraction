{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##### Environment Setup"
      ],
      "metadata": {
        "id": "74bSNuM04Stm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "id": "gMPIkr27ZGCj"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "%pip install -U langchain\n",
        "%pip install -U langchain-community\n",
        "%pip install -U langchain-huggingface\n",
        "%pip install -U langchain_experimental\n",
        "%pip install -U langchain_openai\n",
        "%pip install -U langchain-chroma\n",
        "\n",
        "%pip install -U chromadb\n",
        "\n",
        "%pip install -U unstructured\n",
        "%pip install -U sentence-transformers\n",
        "%pip install -U nltk\n",
        "%pip install -U spacy\n",
        "%pip install -U --upgrade pymupdf\n",
        "%pip install -U transformers torch\n",
        "\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "# Warning control\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "# Once you have obtained the Hugging Face API token with read permission, assign it to `HF_API_KEY`.\n",
        "HF_API_KEY = userdata.get('HF_API_KEY')\n",
        "\n",
        "# For Neo4j, update the URL and password using the credentials file you downloaded.\n",
        "NEO4J_USERNAME = \"neo4j\"\n",
        "NEO4J_URI = userdata.get('NEO4J_URI')\n",
        "NEO4J_PASSWORD = userdata.get('NEO4J_PASSWORD')"
      ],
      "metadata": {
        "id": "eqprzWE23rzX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface.embeddings import HuggingFaceEndpointEmbeddings\n",
        "\n",
        "# define huggingface embedding endpoint\n",
        "hf_embeddings = HuggingFaceEndpointEmbeddings(\n",
        "    #model= \"BAAI/bge-base-en-v1.5\",             # model embedding name\n",
        "    model=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    task=\"feature-extraction\",                  # generate embeddings (dense vectors)\n",
        "    huggingfacehub_api_token=HF_API_KEY         # ðŸ¤— huggingface API token\n",
        ")\n",
        "\n",
        "# get output vector length\n",
        "vect_embed = hf_embeddings.embed_query(\"Hello world!\")\n",
        "EMBEDDING_VECTOR_LENGTH = len(vect_embed)\n",
        "print(\"Embedding vector type: \", type(vect_embed))\n",
        "print(\"Embedding vector length: \", EMBEDDING_VECTOR_LENGTH)"
      ],
      "metadata": {
        "id": "O9pMZO7l38jc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5793943e-6f77-4b34-f860-59c58c951931"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding vector type:  <class 'list'>\n",
            "Embedding vector length:  384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "\n",
        "# define huggingface generation endpoint\n",
        "hf_llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\", # Model Name\n",
        "    task=\"text-generation\",                       # task as generating a text response\n",
        "    max_new_tokens=150,                           # maximum numbers of generated tokens\n",
        "    do_sample=False,                              # disables sampling\n",
        "    huggingfacehub_api_token=HF_API_KEY           # ðŸ¤— huggingface API token\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WaEwbwSi4Eio",
        "outputId": "aa5dca0b-ad9b-4cdb-cb41-e82a58bd6edd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Indexing"
      ],
      "metadata": {
        "id": "rz6gIqVs4ZPU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "import re\n",
        "import spacy\n",
        "from langchain.schema import Document\n",
        "import logging\n",
        "import os\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
        "\n",
        "# Initialize SpaCy's English model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def extract_text_with_metadata(pdf_path, page_limit=None):\n",
        "    try:\n",
        "        with fitz.open(pdf_path) as file:\n",
        "            logging.info(\"PDF opened successfully.\")\n",
        "\n",
        "            filename = os.path.basename(pdf_path)\n",
        "            title, _ = os.path.splitext(filename)\n",
        "\n",
        "            #return \"\\n\".join(page.get_text() for page in file)\n",
        "            pages_text = []\n",
        "            for page_num in range(len(file) if page_limit is None else min(len(file), page_limit)):\n",
        "                page = file.load_page(page_num)\n",
        "                text = page.get_text(\"text\").strip()\n",
        "                pages_text.append({\n",
        "                    'page_number': page_num + 1,  # 1-indexed\n",
        "                    'text': text\n",
        "                })\n",
        "            return title, pages_text\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error extracting text from PDF: {e}\")\n",
        "        return \"Untitled Document\", []\n",
        "\n",
        "def clean_extracted_text(raw_text):\n",
        "    cleaned_pages = []\n",
        "\n",
        "    for page in raw_text:\n",
        "        text = page['text']\n",
        "        page_number = page['page_number']\n",
        "        lines = text.split('\\n')\n",
        "        cleaned_lines = []\n",
        "\n",
        "        for line in lines:\n",
        "            # Skip lines with DOIs\n",
        "            if re.search(r'doi:\\s*\\d+\\.\\d+/\\S+', line, re.IGNORECASE):\n",
        "                continue\n",
        "            # Skip lines starting with numbers followed by ':' or '.'\n",
        "            if re.match(r'^\\d+[:.]', line):\n",
        "                continue\n",
        "            # Skip lines containing email addresses\n",
        "            if re.search(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', line):\n",
        "                continue\n",
        "            # Skip lines containing specific keywords\n",
        "            if re.search(r'(Received|revised|accepted|Keywords|Abstract)', line, re.IGNORECASE):\n",
        "                continue\n",
        "            # Skip lines that are standalone numbers\n",
        "            if re.match(r'^\\d+$', line.strip()):\n",
        "                continue\n",
        "            # Optionally skip very short lines\n",
        "            # if len(line.strip()) < 20:\n",
        "                # continue\n",
        "            # Fix hyphenated line breaks (e.g., \"retrieval-\\n augmented\")\n",
        "            line = re.sub(r'(\\w+)-\\s*\\n\\s*(\\w+)', r'\\1\\2', line)\n",
        "            # Replace multiple spaces with a single space\n",
        "            line = re.sub(r'\\s+', ' ', line)\n",
        "            cleaned_lines.append(line.strip())\n",
        "\n",
        "        # Preserve paragraph breaks by joining with double newline\n",
        "        cleaned_text = '\\n'.join(cleaned_lines) # can give \\n\\n\n",
        "        cleaned_pages.append({\n",
        "            'page_number': page_number,\n",
        "            'cleaned_text': cleaned_text\n",
        "        })\n",
        "\n",
        "    return cleaned_pages\n",
        "\n",
        "def tokenize_sentences(cleaned_pages):\n",
        "    tokenized_pages = []\n",
        "\n",
        "    for page in cleaned_pages:\n",
        "        text = page['cleaned_text']\n",
        "        page_number = page['page_number']\n",
        "        doc = nlp(text)\n",
        "        sentences = [sent.text.strip() for sent in doc.sents if len(sent.text.strip()) > 20]\n",
        "        tokenized_pages.append({\n",
        "            'page_number': page_number,\n",
        "            'sentences': sentences\n",
        "        })\n",
        "    return tokenized_pages\n",
        "\n",
        "def structure_sentences(tokenized_pages, title):\n",
        "    documents = []\n",
        "    for page in tokenized_pages:\n",
        "        page_number = page['page_number']\n",
        "        for sentence in page['sentences']:\n",
        "            doc = Document(\n",
        "                page_content=sentence,\n",
        "                metadata={\n",
        "                    'page_number': page_number,\n",
        "                    'source': title\n",
        "                }\n",
        "            )\n",
        "            documents.append(doc)\n",
        "    return documents\n",
        "\n",
        "def create_chunks(documents):\n",
        "    # Initialize RecursiveCharacterTextSplitter with the desired parameters\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=2000,                    # Define chunk size\n",
        "        chunk_overlap=200,                  # Define chunk overlap\n",
        "        add_start_index=True                # Optionally add start index to metadata\n",
        "    )\n",
        "\n",
        "    # Split documents into chunks\n",
        "    chunks = text_splitter.split_documents(documents)\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "sgPFGFS0Z1XU",
        "collapsed": true
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = '20241015_MISSION_KI_Glossar_v1.0 en.pdf'\n",
        "\n",
        "title, pages_raw_text = extract_text_with_metadata(pdf_path,5) #2nd argument is for page limit(optional)\n",
        "# if not pages_raw_text:\n",
        "#     print(\"No text extracted from the PDF.\")\n",
        "#     return\n",
        "\n",
        "cleaned_pages = clean_extracted_text(pages_raw_text)\n",
        "\n",
        "# Tokenize into sentences\n",
        "tokenized_pages = tokenize_sentences(cleaned_pages)\n",
        "\n",
        "# Structure sentences using LangChain's Document with metadata\n",
        "documents = structure_sentences(tokenized_pages, title)\n",
        "\n",
        "chunks = create_chunks(documents)\n",
        "print(f\"Total #chunks: {len(chunks)}\")\n",
        "\n",
        "print(\"\\nFirst 5 and Last 3 Chunks with Metadata:\")\n",
        "for idx, chunk in enumerate(chunks[:5] + chunks[-3:], 1):\n",
        "    print(f\"{idx}: {chunk.page_content}\")\n",
        "    print(f\"   Metadata: Title='{chunk.metadata['source']}', Page Number={chunk.metadata.get('page_number', 'N/A')}, Start Index={chunk.metadata.get('start_index', 'N/A')}\\n\")\n"
      ],
      "metadata": {
        "id": "yLmMzZ4cmCPk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f8a120b-849a-4ae0-d3b5-d35c93222db4"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total #chunks: 38\n",
            "\n",
            "First 5 and Last 3 Chunks with Metadata:\n",
            "1: October 15, 2024\n",
            "MISSION KI\n",
            "Glossary v1.0\n",
            "c /o acatech\n",
            "German Academy of Science and Engineering\n",
            "Office: Karolinenplatz 4,\n",
            "80333 Munich Germany\n",
            "www.acatech.de\n",
            "   Metadata: Title='20241015_MISSION_KI_Glossar_v1.0 en', Page Number=1, Start Index=0\n",
            "\n",
            "2: 1 / 11\n",
            "TABLE OF CONTENTS\n",
            "TABLE OF CONTENTS\n",
            "00\n",
            "   Metadata: Title='20241015_MISSION_KI_Glossar_v1.0 en', Page Number=2, Start Index=0\n",
            "\n",
            "3: Preliminary remarks on v1.0_______________________________________________________1\n",
            "01 Generic terms __________________________________________________________________1\n",
            "02 Individual quality dimensions ______________________________________________________2\n",
            "03 Horizontal concepts______________________________________________________________4\n",
            "04 Further terms __________________________________________________________________5\n",
            "   Metadata: Title='20241015_MISSION_KI_Glossar_v1.0 en', Page Number=2, Start Index=0\n",
            "\n",
            "4: Glossary\n",
            "v1.0\n",
            "1 / 11\n",
            "Glossary v1.0\n",
            "00\n",
            "   Metadata: Title='20241015_MISSION_KI_Glossar_v1.0 en', Page Number=3, Start Index=0\n",
            "\n",
            "5: Preliminary remark on v1.0\n",
            "This glossary reflects the current, provisionally final state of work in the sense of a v1.0.\n",
            "   Metadata: Title='20241015_MISSION_KI_Glossar_v1.0 en', Page Number=3, Start Index=0\n",
            "\n",
            "6: Traceability\n",
            "Property of an â†’KI system with regard to the ability to record the consecutive sequence of all decisions\n",
            "that enter or have entered an â†’KI system along the entire life cycle.\n",
            "   Metadata: Title='20241015_MISSION_KI_Glossar_v1.0 en', Page Number=5, Start Index=0\n",
            "\n",
            "7: Transparency\n",
            "Property of an â†’KI system that is explainable and comprehensible.\n",
            "   Metadata: Title='20241015_MISSION_KI_Glossar_v1.0 en', Page Number=5, Start Index=0\n",
            "\n",
            "8: In the context of this quality\n",
            "standard, \"transparency\" also includes documentation of the properties of the â†’KI system.\n",
            "   Metadata: Title='20241015_MISSION_KI_Glossar_v1.0 en', Page Number=5, Start Index=0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Embed"
      ],
      "metadata": {
        "id": "rTHf067U2_a8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from langchain_chroma import Chroma                    # This is the database we will use to store the embeddings\n",
        "\n",
        "vectorstore = None\n",
        "Chroma().delete_collection()\n",
        "for split in tqdm(chunks, colour=\"green\"):\n",
        "    if vectorstore:\n",
        "      vectorstore.add_documents([split])                # Add new split to the vectorstore\n",
        "    else:\n",
        "      vectorstore = Chroma.from_documents(              # Generate the vectorstore with first split\n",
        "                documents=[split],\n",
        "                embedding=hf_embeddings,\n",
        "                collection_metadata={\"hnsw:space\": \"cosine\"}     # by default L2 distance measured\n",
        "                )"
      ],
      "metadata": {
        "id": "OLyW9og61JFW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce608c2d-ab8a-4048-aa00-9e5942e11b5b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u001b[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 38/38 [00:16<00:00,  2.27it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Retrieval"
      ],
      "metadata": {
        "id": "1vsXdsCm6qix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectorstore.as_retriever(\n",
        "      search_type=\"similarity_score_threshold\",           # similarity function\n",
        "      search_kwargs={\"score_threshold\": 0.5}                     # number of retrieved relevant documents # \"k\": 3\n",
        "    )"
      ],
      "metadata": {
        "id": "BmciuxQg1UYF"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#user_query = \"An attempt to determine certain characteristics, performance or comparable characteristics.\"\n",
        "#user_query=\"Property of an â†’AI system,\"\n",
        "user_query=\"Human supervision and intervention refer to the ability of a skilled individual to monitor and modify the behavior or functioning of an AI system within its application context, including making necessary adjustments or halting its operations as needed.\"\n",
        "query_embedding = hf_embeddings.embed_query(user_query)"
      ],
      "metadata": {
        "id": "Hrx6q3WLKyOx"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "similarity_search_by_vector_with_relevance_scores():\n",
        " It calculates the distance rather than similarity.\n",
        " Default distance is L2, which is updated to cosine\n"
      ],
      "metadata": {
        "id": "vhuGI6pa5IXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# using query\n",
        "# results = retriever.get_relevant_documents(user_query)\n",
        "# using user query embedding\n",
        "results = vectorstore.similarity_search_by_vector_with_relevance_scores(\n",
        "    query_embedding\n",
        "\n",
        ")\n",
        "\n",
        "# Convert cosine distance to cosine similarity\n",
        "docs_with_similarity = [(doc, 1 - score) for doc, score in results]\n",
        "\n",
        "# Display the relevant documents\n",
        "print(\"Relevant Chunks for the Query (Using Embedding):\")\n",
        "for doc, score in docs_with_similarity:\n",
        "  print(doc.page_content)\n",
        "  print(doc.metadata)\n",
        "  print(f\"Similarity Score: {score} \\n\")"
      ],
      "metadata": {
        "id": "W3DMVQMWJi5k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7510a645-3187-4093-c620-4829fcb4a0cc"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Relevant Chunks for the Query (Using Embedding):\n",
            "Human Oversight and Control\n",
            "Property of an â†’AI system, including its embedding in the application context, with regard to the\n",
            "possibility for a - technically competent - human individual to adequately observe and change the\n",
            "behavior and/or functioning of this â†’AI system in principle and during operation and, if necessary, to\n",
            "terminate it.\n",
            "{'page_number': 5, 'source': '20241015_MISSION_KI_Glossar_v1.0 en', 'start_index': 0}\n",
            "Similarity Score: 0.5840766429901123 \n",
            "\n",
            "Explainability\n",
            "Characteristics of an â†’KI system with regard to the basic comprehensibility and comprehensibility of\n",
            "functionality, behavior and output for human specialists, but also for affected persons\n",
            "and users.\n",
            "{'page_number': 4, 'source': '20241015_MISSION_KI_Glossar_v1.0 en', 'start_index': 0}\n",
            "Similarity Score: 0.3151412010192871 \n",
            "\n",
            "User information\n",
            "Characteristics of an â†’AI system with regard to the quality of information, interaction and operation by a\n",
            "user, including knowledge of the involvement of AI, barriers, and the quality of the user experience.\n",
            "{'page_number': 5, 'source': '20241015_MISSION_KI_Glossar_v1.0 en', 'start_index': 0}\n",
            "Similarity Score: 0.27638453245162964 \n",
            "\n",
            "Interpretability\n",
            "Property of an â†’ AI model that its model parameters, weights or other (mathematical) properties are as\n",
            "directly comprehensible as possible and directly understandable for specialist personnel.\n",
            "{'page_number': 4, 'source': '20241015_MISSION_KI_Glossar_v1.0 en', 'start_index': 0}\n",
            "Similarity Score: 0.2614293098449707 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### using context in prompt"
      ],
      "metadata": {
        "id": "jtMP2UrSDRHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# prepare prompt\n",
        "template = \"\"\"Use the following 3 pieces of context to answer the question at the end.\n",
        "Use three sentences maximum and keep the answer as concise as possible.\n",
        "Say I don't know when you need to.\n",
        "{context}\n",
        "Question: {question}\n",
        "Helpful Answer:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "# Function to format chunks into context\n",
        "def format_docs(top_k_chunks):\n",
        "    return \"\\n\\n\".join(chunk.page_content for chunk in top_k_chunks)\n",
        "\n",
        "additional_context = \"Requirement refers to concepts such as Fairness and Explainability.\"\n",
        "\n",
        "def add_context(retrived_docs):\n",
        "    additional_context = \"Requirement refers to concepts such as Fairness and Explainability.\"\n",
        "\n",
        "    return additional_context + '\\n\\n---------\\n\\n' + retrived_docs\n",
        "\n"
      ],
      "metadata": {
        "id": "tgl6Ly0wTqFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.schema import StrOutputParser\n",
        "\n",
        "simple_rag_chain = (\n",
        " {  \"context\": add_context | retriever | format_docs | add_context,\n",
        "    \"question\":RunnablePassthrough()}\n",
        " | prompt                                 # build the prompt\n",
        " | hf_llm                                 # llm for generation\n",
        " | StrOutputParser()                      # collect the response text\n",
        ")\n",
        "\n",
        "# Display response\n",
        "print(\"Generated Response:\")\n",
        "print(simple_rag_chain.invoke(user_query))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZ4_Y9iiuGNd",
        "outputId": "f96c4e8c-a3c1-4320-aee6-2a6394322b66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_core.vectorstores.base:No relevant docs were retrieved using the relevance score threshold 0.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Response:\n",
            " Requirement refers to essential conditions or features needed to meet certain standards in a project or system, such as Fairness and Explainability.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# prepare prompt\n",
        "template = \"\"\"Use the following 3 pieces of context to answer the question at the end.\n",
        "Use three sentences maximum and keep the answer as concise as possible.\n",
        "Say I don't know when you need to.\n",
        "{context}\n",
        "Question: {question}\n",
        "new_context: {additional_context}\n",
        "Helpful Answer:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "# Function to format chunks into context\n",
        "def format_docs(top_k_chunks):\n",
        "    return \"\\n\\n\".join(chunk.page_content for chunk in top_k_chunks)\n",
        "\n",
        "additional_context = \"Requirement refers to concepts such as Fairness and Explainability.\"\n",
        "\n",
        "def add_context(additional_context):\n",
        "    return \"Requirement refers to concepts such as Fairness and Explainability.\"\n",
        "\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.schema import StrOutputParser\n",
        "\n",
        "simple_rag_chain = (\n",
        " {  \"context\": retriever | format_docs,\n",
        "    \"question\":RunnablePassthrough()}\n",
        " | prompt                                 # build the prompt\n",
        " | hf_llm                                 # llm for generation\n",
        " | StrOutputParser()                      # collect the response text\n",
        ")\n",
        "\n",
        "# Display response\n",
        "print(\"Generated Response:\")\n",
        "print(simple_rag_chain.invoke(user_query, additional_context))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "cEeldQWzhGqh",
        "outputId": "4f3ac46e-c223-4da2-d708-a76f43663242"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Response:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'str' object has no attribute 'items'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-93-83206c24920e>\u001b[0m in \u001b[0;36m<cell line: 36>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# Display response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Generated Response:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimple_rag_chain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_query\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madditional_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3001\u001b[0m         \u001b[0;31m# setup callbacks and context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3002\u001b[0;31m         \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig_with_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3003\u001b[0m         \u001b[0mcallback_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_callback_manager_for_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3004\u001b[0m         \u001b[0;31m# start the root run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/config.py\u001b[0m in \u001b[0;36mensure_config\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    181\u001b[0m                 {\n\u001b[1;32m    182\u001b[0m                     \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mCOPIABLE_KEYS\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mv\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mCONFIG_KEYS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m                 },\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'items'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Misc"
      ],
      "metadata": {
        "id": "78rDuXmSuPpx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# try 'all-Mpnet-base-v2' for better accuracy\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Example sentences from documents\n",
        "document_sentences = [\n",
        "    \"Can you recommend a good Italian place to eat?\",\n",
        "    \"What is the nearest Chinese restaurant?\",\n",
        "    \"I'm looking for a place that serves authentic Italian cuisine.\",\n",
        "    \"Where can I find good Italian food around here?\",\n",
        "    \"Is there any Italian restaurant open nearby?\",\n",
        "    \"I need directions to the nearest coffee shop.\",\n",
        "    \"Tell me about the best pizza spots in town.\",\n",
        "    \"Are there any Italian restaurants that offer delivery?\",\n",
        "    \"What is the best Japanese sushi place in this area?\",\n",
        "    \"I am interested in learning Italian cuisine recipes.\"\n",
        "]\n",
        "\n",
        "user_query = \"I want to find the best Italian restaurant nearby.\"\n",
        "\n",
        "# Generate embeddings\n",
        "document_embeddings = model.encode(document_sentences, convert_to_tensor=True)\n",
        "query_embedding = model.encode(user_query, convert_to_tensor=True)\n",
        "\n",
        "# Compute cosine similarity\n",
        "cosine_scores = util.pytorch_cos_sim(query_embedding, document_embeddings)\n",
        "\n",
        "# Get the most relevant sentences (sorting in descending order of similarity)\n",
        "relevant_indices = cosine_scores.argsort(descending=True).tolist()[0]\n",
        "print(\"Top relevant sentences:\")\n",
        "for idx in relevant_indices[:5]:  # Adjust number of top results as needed\n",
        "    print(f\"Sentence: {document_sentences[idx]}, Score: {cosine_scores[0][idx].item():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buOyyV7huGTy",
        "outputId": "e79c55b1-9491-4b51-cbce-56257938d0d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top relevant sentences:\n",
            "Sentence: Can you recommend a good Italian place to eat?, Score: 0.8437\n",
            "Sentence: Is there any Italian restaurant open nearby?, Score: 0.8398\n",
            "Sentence: I'm looking for a place that serves authentic Italian cuisine., Score: 0.8295\n",
            "Sentence: Where can I find good Italian food around here?, Score: 0.7989\n",
            "Sentence: Are there any Italian restaurants that offer delivery?, Score: 0.7651\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Initialize the embeddings model from LangChain\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Example sentences from documents\n",
        "document_sentences = [\n",
        "    \"Can you recommend a good Italian place to eat?\",\n",
        "    \"What is the nearest Chinese restaurant?\",\n",
        "    \"I'm looking for a place that serves authentic Italian cuisine.\",\n",
        "    \"Where can I find good Italian food around here?\",\n",
        "    \"Is there any Italian restaurant open nearby?\",\n",
        "    \"I need directions to the nearest coffee shop.\",\n",
        "    \"Tell me about the best pizza spots in town.\",\n",
        "    \"Are there any Italian restaurants that offer delivery?\",\n",
        "    \"What is the best Japanese sushi place in this area?\",\n",
        "    \"I am interested in learning Italian cuisine recipes.\"\n",
        "]\n",
        "\n",
        "# User query\n",
        "user_query = \"I want to find the best Italian restaurant nearby.\"\n",
        "\n",
        "# Generate embeddings for the query and documents\n",
        "query_embedding = np.array(embeddings.embed_query(user_query)).reshape(1, -1)  # Convert to 2D array for similarity calculation\n",
        "doc_embeddings = np.array(embeddings.embed_documents(document_sentences))\n",
        "\n",
        "# Compute cosine similarity\n",
        "cosine_scores = cosine_similarity(query_embedding, doc_embeddings)\n",
        "\n",
        "# Get the most relevant sentences (sorting in descending order of similarity)\n",
        "relevant_indices = cosine_scores.argsort(axis=1)[0][::-1]  # Sort indices by descending scores\n",
        "print(\"Top relevant sentences:\")\n",
        "for idx in relevant_indices[:5]:  # Adjust number of top results as needed\n",
        "    print(f\"Sentence: {document_sentences[idx]}, Score: {cosine_scores[0][idx]:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ls-7Mdr7xXsz",
        "outputId": "26187abd-fee6-4656-bb81-ba148dc3725a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top relevant sentences:\n",
            "Sentence: Can you recommend a good Italian place to eat?, Score: 0.8437\n",
            "Sentence: Is there any Italian restaurant open nearby?, Score: 0.8398\n",
            "Sentence: I'm looking for a place that serves authentic Italian cuisine., Score: 0.8295\n",
            "Sentence: Where can I find good Italian food around here?, Score: 0.7989\n",
            "Sentence: Are there any Italian restaurants that offer delivery?, Score: 0.7651\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UZcV1-zp1ZPe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}