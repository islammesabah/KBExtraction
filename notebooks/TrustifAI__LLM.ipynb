{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74bSNuM04Stm"
      },
      "source": [
        "##### Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "gMPIkr27ZGCj"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "%pip install -U langchain\n",
        "%pip install -U langchain-community\n",
        "%pip install -U langchain-huggingface\n",
        "%pip install -U langchain_experimental\n",
        "%pip install -U langchain_openai\n",
        "%pip install -U langchain-chroma\n",
        "\n",
        "%pip install -U chromadb\n",
        "\n",
        "%pip install -U unstructured\n",
        "%pip install -U sentence-transformers\n",
        "%pip install -U nltk\n",
        "%pip install -U spacy\n",
        "%pip install -U --upgrade pymupdf\n",
        "%pip install -U transformers torch\n",
        "\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "# Warning control\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqprzWE23rzX"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "# Once you have obtained the Hugging Face API token with read permission, assign it to `HF_API_KEY`.\n",
        "HF_API_KEY = userdata.get('HF_API_KEY')\n",
        "\n",
        "# For Neo4j, update the URL and password using the credentials file you downloaded.\n",
        "#NEO4J_USERNAME = \"neo4j\"\n",
        "#NEO4J_URI = userdata.get('NEO4J_URI')\n",
        "#NEO4J_PASSWORD = userdata.get('NEO4J_PASSWORD')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UDf6OPescnX"
      },
      "source": [
        "based on the MTEB leaderboard https://huggingface.co/spaces/mteb/leaderboard we will chose dunzhang/stella_en_1.5B_v5 model as it one of the leaders and small to fit on colab for now"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9pMZO7l38jc",
        "outputId": "1a4dbc4b-2637-4954-98d4-abbd834a303a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execution time: 158.12264919281006 seconds\n",
            "Embedding vector type:  <class 'list'>\n",
            "Embedding vector length:  1024\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "from langchain_huggingface.embeddings import HuggingFaceEndpointEmbeddings\n",
        "\n",
        "# define huggingface embedding endpoint\n",
        "hf_embeddings = HuggingFaceEndpointEmbeddings(\n",
        "    model='dunzhang/stella_en_1.5B_v5',\n",
        "    #model= \"BAAI/bge-base-en-v1.5\",             # model embedding name\n",
        "    #model=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    task=\"feature-extraction\",                  # generate embeddings (dense vectors)\n",
        "    huggingfacehub_api_token=HF_API_KEY         # ðŸ¤— huggingface API token\n",
        ")\n",
        "\n",
        "# get output vector length\n",
        "start_time = time.time()\n",
        "vect_embed = hf_embeddings.embed_query(\"Hello world!\")\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "print(\"Execution time:\", execution_time, \"seconds\")\n",
        "EMBEDDING_VECTOR_LENGTH = len(vect_embed)\n",
        "print(\"Embedding vector type: \", type(vect_embed))\n",
        "print(\"Embedding vector length: \", EMBEDDING_VECTOR_LENGTH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WaEwbwSi4Eio"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "\n",
        "# for now mistral model works but we will need to update\n",
        "# define huggingface generation endpoint\n",
        "hf_llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\", # Model Name\n",
        "    task=\"text-generation\",                       # task as generating a text response\n",
        "    max_new_tokens=150,                           # maximum numbers of generated tokens\n",
        "    do_sample=False,                              # disables sampling\n",
        "    huggingfacehub_api_token=HF_API_KEY           # ðŸ¤— huggingface API token\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rz6gIqVs4ZPU"
      },
      "source": [
        "##### Indexing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "sgPFGFS0Z1XU"
      },
      "outputs": [],
      "source": [
        "import fitz  # PyMuPDF\n",
        "import re\n",
        "import spacy\n",
        "from langchain.schema import Document\n",
        "import logging\n",
        "import os\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
        "\n",
        "# Initialize SpaCy's English model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def extract_text_with_metadata(pdf_path, page_limit=None):\n",
        "    try:\n",
        "        with fitz.open(pdf_path) as file:\n",
        "            logging.info(\"PDF opened successfully.\")\n",
        "\n",
        "            filename = os.path.basename(pdf_path)\n",
        "            title, _ = os.path.splitext(filename)\n",
        "\n",
        "            #return \"\\n\".join(page.get_text() for page in file)\n",
        "            pages_text = []\n",
        "            for page_num in range(len(file) if page_limit is None else min(len(file), page_limit)):\n",
        "                page = file.load_page(page_num)\n",
        "                text = page.get_text(\"text\").strip()\n",
        "                pages_text.append({\n",
        "                    'page_number': page_num + 1,  # 1-indexed\n",
        "                    'text': text\n",
        "                })\n",
        "            return title, pages_text\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error extracting text from PDF: {e}\")\n",
        "        return \"Untitled Document\", []\n",
        "\n",
        "def clean_extracted_text(raw_text):\n",
        "    cleaned_pages = []\n",
        "\n",
        "    for page in raw_text:\n",
        "        text = page['text']\n",
        "        page_number = page['page_number']\n",
        "        lines = text.split('\\n')\n",
        "        cleaned_lines = []\n",
        "\n",
        "        for line in lines:\n",
        "            # Skip lines with DOIs\n",
        "            if re.search(r'doi:\\s*\\d+\\.\\d+/\\S+', line, re.IGNORECASE):\n",
        "                continue\n",
        "            # Skip lines starting with numbers followed by ':' or '.'\n",
        "            if re.match(r'^\\d+[:.]', line):\n",
        "                continue\n",
        "            # Skip lines containing email addresses\n",
        "            if re.search(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', line):\n",
        "                continue\n",
        "            # Skip lines containing specific keywords\n",
        "            if re.search(r'(Received|revised|accepted|Keywords|Abstract)', line, re.IGNORECASE):\n",
        "                continue\n",
        "            # Skip lines that are standalone numbers\n",
        "            if re.match(r'^\\d+$', line.strip()):\n",
        "                continue\n",
        "            # Optionally skip very short lines\n",
        "            # if len(line.strip()) < 20:\n",
        "                # continue\n",
        "            # Fix hyphenated line breaks (e.g., \"retrieval-\\n augmented\")\n",
        "            line = re.sub(r'(\\w+)-\\s*\\n\\s*(\\w+)', r'\\1\\2', line)\n",
        "            # Replace multiple spaces with a single space\n",
        "            line = re.sub(r'\\s+', ' ', line)\n",
        "            cleaned_lines.append(line.strip())\n",
        "\n",
        "        # Preserve paragraph breaks by joining with double newline\n",
        "        cleaned_text = '\\n'.join(cleaned_lines) # can give \\n\\n\n",
        "        cleaned_pages.append({\n",
        "            'page_number': page_number,\n",
        "            'cleaned_text': cleaned_text\n",
        "        })\n",
        "\n",
        "    return cleaned_pages\n",
        "\n",
        "def tokenize_sentences(cleaned_pages):\n",
        "    tokenized_pages = []\n",
        "\n",
        "    for page in cleaned_pages:\n",
        "        text = page['cleaned_text']\n",
        "        page_number = page['page_number']\n",
        "        doc = nlp(text)\n",
        "        sentences = [sent.text.strip() for sent in doc.sents if len(sent.text.strip()) > 20]\n",
        "        tokenized_pages.append({\n",
        "            'page_number': page_number,\n",
        "            'sentences': sentences\n",
        "        })\n",
        "    return tokenized_pages\n",
        "\n",
        "def structure_sentences(tokenized_pages, title):\n",
        "    documents = []\n",
        "    for page in tokenized_pages:\n",
        "        page_number = page['page_number']\n",
        "        for sentence in page['sentences']:\n",
        "            doc = Document(\n",
        "                page_content=sentence,\n",
        "                metadata={\n",
        "                    'page_number': page_number,\n",
        "                    'source': title\n",
        "                }\n",
        "            )\n",
        "            documents.append(doc)\n",
        "    return documents\n",
        "\n",
        "# Why we need this function after we already chunking on sentences\n",
        "def create_chunks(documents):\n",
        "    # Initialize RecursiveCharacterTextSplitter with the desired parameters\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=2000,                    # Define chunk size\n",
        "        chunk_overlap=200,                  # Define chunk overlap\n",
        "        add_start_index=True                # Optionally add start index to metadata\n",
        "    )\n",
        "\n",
        "    # Split documents into chunks\n",
        "    chunks = text_splitter.split_documents(documents)\n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLmMzZ4cmCPk"
      },
      "outputs": [],
      "source": [
        "pdf_path = '20241015_MISSION_KI_Glossar_v1.0 en.pdf'\n",
        "\n",
        "title, pages_raw_text = extract_text_with_metadata(pdf_path,10) #2nd argument is for page limit(optional)\n",
        "# if not pages_raw_text:\n",
        "#     print(\"No text extracted from the PDF.\")\n",
        "#     return\n",
        "\n",
        "cleaned_pages = clean_extracted_text(pages_raw_text)\n",
        "\n",
        "# Tokenize into sentences\n",
        "tokenized_pages = tokenize_sentences(cleaned_pages)\n",
        "\n",
        "# Structure sentences using LangChain's Document with metadata\n",
        "chunks = structure_sentences(tokenized_pages, title)\n",
        "\n",
        "# we need to remove repetitive text like this\n",
        "## 1 / 11\n",
        "## TABLE OF CONTENTS\n",
        "## TABLE OF CONTENTS\n",
        "## 00\n",
        "\n",
        "# print(f\"Total #documents: {len(documents)}\")\n",
        "\n",
        "# no need for this step i guess same length\n",
        "# either we chunk on sentences or characters\n",
        "# chunks = create_chunks(documents)\n",
        "print(f\"Total #chunks: {len(chunks)}\")\n",
        "\n",
        "# print(\"\\nFirst 5 and Last 3 Chunks with Metadata:\")\n",
        "# for idx, chunk in enumerate(chunks, 1): #chunks[:5] + chunks[-3:\n",
        "#     print(f\"{idx}: \")\n",
        "#     print('===================')\n",
        "#     print(chunk.page_content)\n",
        "#     print('===================')\n",
        "#     print(f\"   Metadata: Title='{chunk.metadata['source']}', Page Number={chunk.metadata.get('page_number', 'N/A')}, Start Index={chunk.metadata.get('start_index', 'N/A')}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the output file name\n",
        "output_file = \"sentnece_chunks_record.txt\"\n",
        "\n",
        "# Open the file in write mode\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
        "    for idx, chunk in enumerate(chunks, 1):  # chunks[:5] + chunks[-3:]\n",
        "        file.write(f\"{idx}:\\n\")\n",
        "        file.write('===================\\n')\n",
        "        file.write(chunk.page_content + \"\\n\")\n",
        "        file.write('===================\\n\\n')\n",
        "\n",
        "print(f\"Chunks have been successfully saved to {output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHTGGevqhB7y",
        "outputId": "dd7cf933-e3e3-4585-e1ec-17f792bf7258"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunks have been successfully saved to sentnece_chunks_record.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDxl0WmQ8b74"
      },
      "source": [
        "It would be beneficial if we could get statistics on the length of sentences in the data for now"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTHf067U2_a8"
      },
      "source": [
        "##### Embed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLyW9og61JFW",
        "outputId": "d73d4215-5e1f-40a6-fe30-bdb434ec0c1d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|\u001b[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 38/38 [03:24<00:00,  5.39s/it]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "from langchain_chroma import Chroma                    # This is the database we will use to store the embeddings\n",
        "\n",
        "vectorstore = None\n",
        "Chroma().delete_collection()\n",
        "for split in tqdm(chunks, colour=\"green\"):\n",
        "    if vectorstore:\n",
        "      vectorstore.add_documents([split])                # Add new split to the vectorstore\n",
        "    else:\n",
        "      vectorstore = Chroma.from_documents(              # Generate the vectorstore with first split\n",
        "                documents=[split],\n",
        "                embedding=hf_embeddings,\n",
        "                collection_metadata={\"hnsw:space\": \"cosine\"}     # by default L2 distance measured\n",
        "                )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vsXdsCm6qix"
      },
      "source": [
        "##### Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BmciuxQg1UYF"
      },
      "outputs": [],
      "source": [
        "# as we need to make sure that we will not use the full context of the model\n",
        "# we use k but we can use high numbers as sentence is small\n",
        "retriever = vectorstore.as_retriever(\n",
        "      search_type=\"similarity_score_threshold\",           # similarity function\n",
        "      search_kwargs={\"score_threshold\": 0.5,\n",
        "                     'k':10}                     # number of retrieved relevant documents\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Hrx6q3WLKyOx"
      },
      "outputs": [],
      "source": [
        "#user_query = \"An attempt to determine certain characteristics, performance or comparable characteristics.\"\n",
        "#user_query=\"Property of an â†’AI system,\"\n",
        "user_query=\"Human supervision and intervention refer to the ability of a skilled individual to monitor and modify the behavior or functioning of an AI system within its application context, including making necessary adjustments or halting its operations as needed.\"\n",
        "query_embedding = hf_embeddings.embed_query(user_query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhuGI6pa5IXF"
      },
      "source": [
        "similarity_search_by_vector_with_relevance_scores():\n",
        " It calculates the distance rather than similarity.\n",
        " Default distance is L2, which is updated to cosine\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3DMVQMWJi5k",
        "outputId": "7510a645-3187-4093-c620-4829fcb4a0cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Relevant Chunks for the Query (Using Embedding):\n",
            "Human Oversight and Control\n",
            "Property of an â†’AI system, including its embedding in the application context, with regard to the\n",
            "possibility for a - technically competent - human individual to adequately observe and change the\n",
            "behavior and/or functioning of this â†’AI system in principle and during operation and, if necessary, to\n",
            "terminate it.\n",
            "{'page_number': 5, 'source': '20241015_MISSION_KI_Glossar_v1.0 en', 'start_index': 0}\n",
            "Similarity Score: 0.5840766429901123 \n",
            "\n",
            "Explainability\n",
            "Characteristics of an â†’KI system with regard to the basic comprehensibility and comprehensibility of\n",
            "functionality, behavior and output for human specialists, but also for affected persons\n",
            "and users.\n",
            "{'page_number': 4, 'source': '20241015_MISSION_KI_Glossar_v1.0 en', 'start_index': 0}\n",
            "Similarity Score: 0.3151412010192871 \n",
            "\n",
            "User information\n",
            "Characteristics of an â†’AI system with regard to the quality of information, interaction and operation by a\n",
            "user, including knowledge of the involvement of AI, barriers, and the quality of the user experience.\n",
            "{'page_number': 5, 'source': '20241015_MISSION_KI_Glossar_v1.0 en', 'start_index': 0}\n",
            "Similarity Score: 0.27638453245162964 \n",
            "\n",
            "Interpretability\n",
            "Property of an â†’ AI model that its model parameters, weights or other (mathematical) properties are as\n",
            "directly comprehensible as possible and directly understandable for specialist personnel.\n",
            "{'page_number': 4, 'source': '20241015_MISSION_KI_Glossar_v1.0 en', 'start_index': 0}\n",
            "Similarity Score: 0.2614293098449707 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# using query\n",
        "# results = retriever.get_relevant_documents(user_query)\n",
        "# using user query embedding\n",
        "results = vectorstore.similarity_search_by_vector_with_relevance_scores(\n",
        "    query_embedding\n",
        "\n",
        ")\n",
        "\n",
        "# Convert cosine distance to cosine similarity\n",
        "docs_with_similarity = [(doc, 1 - score) for doc, score in results]\n",
        "\n",
        "# Display the relevant documents\n",
        "print(\"Relevant Chunks for the Query (Using Embedding):\")\n",
        "for doc, score in docs_with_similarity:\n",
        "  print(doc.page_content)\n",
        "  print(doc.metadata)\n",
        "  print(f\"Similarity Score: {score} \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtMP2UrSDRHs"
      },
      "source": [
        "#### using context in prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Context is not only for generation of efficient result, moreover priority is during retrieval stage.\n",
        "Now as additional context we will have set of Node names as string, not LLM sentence as given in following code block.\n",
        "Do you think the edge details(is_subclass_of, implements, property_of) also make sense to add to the context?"
      ],
      "metadata": {
        "id": "nVw2r3eT__gm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgl6Ly0wTqFi"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# prepare prompt\n",
        "template = \"\"\"Use the following 3 pieces of context to answer the question at the end.\n",
        "Use three sentences maximum and keep the answer as concise as possible.\n",
        "Say I don't know when you need to.\n",
        "{context}\n",
        "Question: {question}\n",
        "Helpful Answer:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "# Function to format chunks into context\n",
        "def format_docs(top_k_chunks):\n",
        "    return \"\\n\\n\".join(chunk.page_content for chunk in top_k_chunks)\n",
        "\n",
        "additional_context = \"Requirement refers to concepts such as Fairness and Explainability.\"\n",
        "\n",
        "def add_context(retrived_docs):\n",
        "    additional_context = \"Requirement refers to concepts such as Fairness and Explainability.\"\n",
        "\n",
        "    return additional_context + '\\n\\n---------\\n\\n' + retrived_docs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZ4_Y9iiuGNd",
        "outputId": "f96c4e8c-a3c1-4320-aee6-2a6394322b66"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langchain_core.vectorstores.base:No relevant docs were retrieved using the relevance score threshold 0.5\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Response:\n",
            " Requirement refers to essential conditions or features needed to meet certain standards in a project or system, such as Fairness and Explainability.\n"
          ]
        }
      ],
      "source": [
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.schema import StrOutputParser\n",
        "\n",
        "simple_rag_chain = (\n",
        " {  \"context\": add_context | retriever | format_docs | add_context,\n",
        "    \"question\":RunnablePassthrough()}\n",
        " | prompt                                 # build the prompt\n",
        " | hf_llm                                 # llm for generation\n",
        " | StrOutputParser()                      # collect the response text\n",
        ")\n",
        "\n",
        "# Display response\n",
        "print(\"Generated Response:\")\n",
        "print(simple_rag_chain.invoke(user_query))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "cEeldQWzhGqh",
        "outputId": "4f3ac46e-c223-4da2-d708-a76f43663242"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Response:\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'str' object has no attribute 'items'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-93-83206c24920e>\u001b[0m in \u001b[0;36m<cell line: 36>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# Display response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Generated Response:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimple_rag_chain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_query\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madditional_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3001\u001b[0m         \u001b[0;31m# setup callbacks and context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3002\u001b[0;31m         \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig_with_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3003\u001b[0m         \u001b[0mcallback_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_callback_manager_for_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3004\u001b[0m         \u001b[0;31m# start the root run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/config.py\u001b[0m in \u001b[0;36mensure_config\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    181\u001b[0m                 {\n\u001b[1;32m    182\u001b[0m                     \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mCOPIABLE_KEYS\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mv\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mCONFIG_KEYS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m                 },\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'items'"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# prepare prompt\n",
        "template = \"\"\"Use the following 3 pieces of context to answer the question at the end.\n",
        "Use three sentences maximum and keep the answer as concise as possible.\n",
        "Say I don't know when you need to.\n",
        "{context}\n",
        "Question: {question}\n",
        "new_context: {additional_context}\n",
        "Helpful Answer:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "# Function to format chunks into context\n",
        "def format_docs(top_k_chunks):\n",
        "    return \"\\n\\n\".join(chunk.page_content for chunk in top_k_chunks)\n",
        "\n",
        "additional_context = \"Requirement refers to concepts such as Fairness and Explainability.\"\n",
        "\n",
        "def add_context(additional_context):\n",
        "    return \"Requirement refers to concepts such as Fairness and Explainability.\"\n",
        "\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.schema import StrOutputParser\n",
        "\n",
        "simple_rag_chain = (\n",
        " {  \"context\": retriever | format_docs,\n",
        "    \"question\":RunnablePassthrough()}\n",
        " | prompt                                 # build the prompt\n",
        " | hf_llm                                 # llm for generation\n",
        " | StrOutputParser()                      # collect the response text\n",
        ")\n",
        "\n",
        "# Display response\n",
        "print(\"Generated Response:\")\n",
        "print(simple_rag_chain.invoke(user_query, additional_context))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZcV1-zp1ZPe"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}