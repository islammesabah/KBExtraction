{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Set-up"
      ],
      "metadata": {
        "id": "6r2T3wQTVCYi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bqxk5tEj7q43"
      },
      "outputs": [],
      "source": [
        "# Install Libraries\n",
        "%%capture\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "# All LangChain libraries for implementing logic chaining\n",
        "%pip install -U langchain\n",
        "%pip install -U langchain_community\n",
        "%pip install -U langchain-huggingface\n",
        "%pip install -U langchain_experimental\n",
        "%pip install -U langchain_openai\n",
        "\n",
        "%pip install -U unstructured\n",
        "%pip install -U sentence-transformers\n",
        "\n",
        "%pip install -U Neo4jGraph\n",
        "%pip install -U py2neo\n",
        "%pip install -U spacy\n",
        "%pip install -U rdflib-neo4j\n",
        "# Langchain\n",
        "from langchain_community.graphs import Neo4jGraph\n",
        "from langchain_community.vectorstores import Neo4jVector\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQAWithSourcesChain\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "from spacy import load, displacy\n",
        "import pandas as pd\n",
        "from google.colab import userdata\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_PJR8Tq388YF"
      },
      "outputs": [],
      "source": [
        "# Load from colab note\n",
        "NEO4J_USERNAME = \"neo4j\"\n",
        "NEO4J_URI = userdata.get('NEO4J_URI')\n",
        "NEO4J_PASSWORD = userdata.get('NEO4J_PASSWORD')\n",
        "HF_API_KEY = userdata.get('HF_API_KEY')\n",
        "\n",
        "# Set up connection to graph instance using LangChain\n",
        "kg = Neo4jGraph(\n",
        "    url=NEO4J_URI,\n",
        "    username=NEO4J_USERNAME,\n",
        "    password=NEO4J_PASSWORD\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ofhgwfbwKX-2"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "\n",
        "# define huggingface generation endpoint\n",
        "hf_llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\", # Model Name\n",
        "    task=\"text-generation\",                       # task as generating a text response\n",
        "    max_new_tokens=150,                           # maximum numbers of generated tokens\n",
        "    do_sample=False,                              # disables sampling\n",
        "    huggingfacehub_api_token=HF_API_KEY           # ðŸ¤— huggingface API token\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Caution: Run, only if you wish to delete the content of database"
      ],
      "metadata": {
        "id": "WNvzeziikXI0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V90teZPa9hHu",
        "outputId": "7c6f880f-0f34-4d46-f75b-2a2f61843267"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'count(n)': 0}]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# clean the neo4j dataset\n",
        "## All nodes and relationships.\n",
        "kg.query(\"MATCH (n) DETACH DELETE n\")\n",
        "## All indexes and constraints.\n",
        "kg.query(\"CALL apoc.schema.assert({},{},true) YIELD label, key RETURN *\")\n",
        "\n",
        "# check if the dataset empty\n",
        "kg.query(\"MATCH (n) RETURN count(n)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Rule-Based Model"
      ],
      "metadata": {
        "id": "8ekpkRjo3KxM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CmgQ6je_Bjs",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "allowed_dependencies = {\n",
        "     'acomp','advmod','agent','amod','attr','aux','auxpass',\n",
        "     'case','cc','ccomp','compound','conj','det','dobj',\n",
        "     'nmod','nsubj','nsubjpass',\n",
        "     'pcomp','pobj','prep','poss','ROOT','xcomp'\n",
        "}\n",
        "\n",
        "def has_required_dependencies(doc, allowed_dependencies):\n",
        "    if not {token.dep_ for token in doc}.issubset(allowed_dependencies):\n",
        "        return False\n",
        "\n",
        "    return (\"is a\" in doc.text.lower() or \"is an\" in doc.text.lower()) or \\\n",
        "            (any(token.dep_ == 'ROOT' for token in doc) and \\\n",
        "            any(token.dep_ in {'nsubj', 'nsubjpass'} for token in doc) and \\\n",
        "            any(token.dep_ in {'dobj', 'pobj'} for token in doc))\n",
        "\n",
        "nlp = load(\"en_core_web_sm\")\n",
        "unhandled_sentences=set()\n",
        "all_graphs = []\n",
        "with open(\"DSA_knowledge.txt\", \"r\") as file:\n",
        "    sentences = file.read()\n",
        "\n",
        "for sentence in [s.strip().rstrip(string.punctuation) for s in sentences.strip().split('\\n') if s.strip()]:\n",
        "  doc = nlp(sentence)\n",
        "  # displacy.render(doc, style=\"dep\", jupyter=True, options={'distance': 90})\n",
        "  if not has_required_dependencies(doc, allowed_dependencies):\n",
        "      unhandled_sentences.add(sentence)\n",
        "      continue\n",
        "\n",
        "  try:\n",
        "    temp_graph = {\n",
        "        \"nodes\": {},  # {'nodes': {0: {'pos': 0, 'label': 'X', 'dep': 'nsubj'}, 4: {'pos': 4, 'label': 'Y', 'dep': 'pobj'}},\n",
        "        \"edges\": [],  # 'edges': [(0, 4, 'is subclass of')]}\n",
        "        \"sentence\": sentence\n",
        "    }\n",
        "\n",
        "    edge_mapping = {\n",
        "        'subject_nodes': {},  # {1: {0}} # multiple subject nodes possible\n",
        "        'object_nodes': {},   # {1: 4}\n",
        "        'edge_ids': set()     # {1}\n",
        "    }\n",
        "\n",
        "    temp_graph[\"nodes\"] = {token['id']: {\"pos\": token['id'], \"label\": doc.text, \"dep\": token['dep']}\n",
        "                          for token, doc in zip(doc.to_json()['tokens'], doc)}\n",
        "\n",
        "    temp_graph[\"edges\"] = [(token['head'], token['id'], token['dep'])\n",
        "                            for token in doc.to_json()['tokens'] if token['head'] != token['id']]\n",
        "\n",
        "    root_node = list(filter(lambda node: temp_graph[\"nodes\"][node]['dep'] == 'ROOT', temp_graph[\"nodes\"]))[0]\n",
        "    stopping = False\n",
        "    while not stopping:\n",
        "      for edge in sorted(temp_graph[\"edges\"], key=lambda x: abs(x[0] - x[1])):\n",
        "\n",
        "        source_pos, target_pos, meta = edge\n",
        "\n",
        "        if source_pos not in temp_graph[\"nodes\"] or target_pos not in temp_graph[\"nodes\"]:\n",
        "            continue\n",
        "        #print(edge)\n",
        "        source_metadata = temp_graph[\"nodes\"][source_pos]\n",
        "        target_metadata = temp_graph[\"nodes\"][target_pos]\n",
        "        try:\n",
        "            match (source_metadata, meta, target_metadata):\n",
        "                case {'label': s, **source}, 'compound' | 'amod' | 'aux' |'auxpass' | 'advmod', {'label': t, **target}:\n",
        "                    source_metadata['label'] = f\"{t} {s}\"\n",
        "                    temp_graph['edges'] = list(filter(lambda edge: not (edge[0] == source_pos and edge[1] == target_pos), temp_graph['edges']))\n",
        "                    del temp_graph['nodes'][target_pos]\n",
        "                    continue\n",
        "\n",
        "                case {'label': s, **source}, 'agent', {'label': t, **target}:\n",
        "                    source_metadata['label'] = f\"{s} {t}\"\n",
        "                    next_node = next((n for src, n, label in temp_graph[\"edges\"] if src == target_pos and label == 'pobj'), None)\n",
        "                    edge_mapping['edge_ids'].add(source_pos)\n",
        "                    edge_mapping['object_nodes'][source_pos] = next_node\n",
        "                    temp_graph['edges'].append((source_pos, next_node, 'pobj'))\n",
        "                    temp_graph['edges'] = list(filter(lambda edge: not (edge[0] == source_pos and edge[1] == target_pos), temp_graph['edges']))\n",
        "                    temp_graph['edges'] = list(filter(lambda edge: not (edge[0] == target_pos and edge[1] == next_node), temp_graph['edges']))\n",
        "                    del temp_graph['nodes'][target_pos]\n",
        "                    continue\n",
        "\n",
        "                case {'label': s, **source}, 'case' | 'cc', {'label': t, **target}:\n",
        "                    temp_graph['edges'] = list(filter(lambda edge: not (edge[0] == source_pos and edge[1] == target_pos), temp_graph['edges']))\n",
        "                    del temp_graph['nodes'][target_pos]\n",
        "                    continue\n",
        "\n",
        "                case {'label': s, **source}, 'det', {'label': t, **target}:\n",
        "                    temp_graph['nodes'][source_pos]['det'] = t\n",
        "                    temp_graph['edges'] = list(filter(lambda edge: not (edge[0] == source_pos and edge[1] == target_pos), temp_graph['edges']))\n",
        "                    del temp_graph['nodes'][target_pos]\n",
        "                    continue\n",
        "\n",
        "                case {'label': s, **source}, 'attr'|'acomp', {'label': 'subclass'|'attribute'|'dimension'|'kind'|'threat'|'result'|'type'|'equal'|'form', **target}: #is(head)--attr--subclass(tail)--prep--of(child)--pobj--Risk\n",
        "                    next_node = next((n for src, n, label in temp_graph[\"edges\"] if src == target_pos and label == 'prep'), None)\n",
        "                    obj_node = next((n for src, n, label in temp_graph[\"edges\"] if src == next_node and label == 'pobj'), None)\n",
        "                    source_metadata['label'] = f\"{s} {target_metadata['label']} {temp_graph['nodes'][next_node]['label']}\" #is-->issubclassof\n",
        "                    edge_mapping['edge_ids'].add(source_pos)\n",
        "                    edge_mapping['object_nodes'][source_pos] = obj_node\n",
        "                    temp_graph['edges'] = list(filter(lambda edge: not (edge[0] == source_pos and edge[1] == target_pos), temp_graph['edges'])) # remove edge: is--subclass\n",
        "                    temp_graph['edges'] = list(filter(lambda edge: not (edge[0] == target_pos and edge[1] == next_node), temp_graph['edges'])) # remove edge: subclass--of\n",
        "                    temp_graph['edges'] = list(filter(lambda edge: not (edge[0] == next_node and edge[1] == obj_node), temp_graph['edges'])) # remove edge: of--Y\n",
        "                    temp_graph['edges'].append((source_pos, obj_node, 'pobj')) #connect edge from 'is' node to obj node\n",
        "                    del temp_graph['nodes'][target_pos] # remove node: 'subclass'\n",
        "                    del temp_graph['nodes'][next_node]  # remove node: 'of'\n",
        "                    continue\n",
        "\n",
        "                case {'label': s, **source}, 'attr'|'acomp', {'label': t, **target}: #is-attr-Y\n",
        "                    edge_mapping['edge_ids'].add(source_pos)\n",
        "                    edge_mapping['object_nodes'][source_pos] = target_pos\n",
        "                    continue\n",
        "\n",
        "                case {'dep': 'ROOT', 'label': s, **source}, 'prep'|'xcomp', {'label': t, **target}: #attributes(ROOT)--prep--to #helps--xcomp--see--pobj--X\n",
        "                    source_metadata['label'] = f\"{s} {t}\"\n",
        "                    next_node = next((n for src, n, label in temp_graph[\"edges\"] if src == target_pos and label in {'pobj', 'dobj'}), None)\n",
        "                    if next_node:\n",
        "                      temp_graph['edges'].append((source_pos, next_node, 'pobj'))\n",
        "                      temp_graph['edges'] = list(filter(lambda edge: not (edge[0] == source_pos and edge[1] == target_pos), temp_graph['edges']))\n",
        "                      temp_graph['edges'] = list(filter(lambda edge: not (edge[0] == target_pos and edge[1] == next_node), temp_graph['edges']))\n",
        "                      del temp_graph['nodes'][target_pos]\n",
        "                    continue\n",
        "\n",
        "                case {'label': s, **source}, 'prep', {'label': t, **target}: #*-dobj-assessment--prep--of|*-attr-(a)dimension-prep-of\n",
        "                    if next((n for src, n, label in temp_graph[\"edges\"] if n == source_pos and label == 'attr'), None) is None:\n",
        "                      next_node = next((n for src, n, label in temp_graph[\"edges\"] if src == target_pos and label in {'pobj'}), None)\n",
        "                      if next_node: #Date-prep-of-pobj-birth\n",
        "                        source_metadata['label'] = f\"{s} {t} {temp_graph['nodes'][next_node]['label']}\"\n",
        "                        temp_graph['edges'] = list(filter(lambda edge: not (edge[0] == source_pos and edge[1] == target_pos), temp_graph['edges']))\n",
        "                        temp_graph['edges'] = list(filter(lambda edge: not (edge[0] == target_pos and edge[1] == next_node), temp_graph['edges']))\n",
        "                        del temp_graph['nodes'][target_pos]\n",
        "                        del temp_graph['nodes'][next_node]\n",
        "                      else:\n",
        "                        edge_mapping['edge_ids'].add(target_pos)\n",
        "                        edge_mapping['subject_nodes'].setdefault(target_pos, set()).add(source_pos)\n",
        "                        temp_graph['edges'] = list(filter(lambda edge: not (edge[0] == source_pos and edge[1] == target_pos), temp_graph['edges']))\n",
        "                    continue\n",
        "\n",
        "                case {'label': s, **source}, 'poss', {'label': t, **target}:\n",
        "                    next_node = next((n for src, n, label in temp_graph[\"edges\"] if src == source_pos and label == 'conj'), None)\n",
        "                    if next_node:\n",
        "                      temp_graph['edges'].append((temp_graph[\"nodes\"][next_node]['label'],\n",
        "                                                  temp_graph[\"nodes\"][target_pos]['label'],\n",
        "                                                  'of'))\n",
        "                    continue\n",
        "\n",
        "                case {'label': s, **source}, 'nmod', {'label': t, **target}:\n",
        "                    source_metadata['label'] = f\"{t} {s}\"\n",
        "                    incoming_node = next((src for src, n, label in temp_graph[\"edges\"] if target == source_pos and label == 'nsubj'), None)\n",
        "                    if 'conj' in target:\n",
        "                        target['conj']['nodeId'] = target['conj']['text'] + f\" {s}\"\n",
        "                        edge_mapping['subject_nodes'][incoming_node].add(target['conj']['nodeId'])\n",
        "                    temp_graph['edges'] = list(filter(lambda edge: not (edge[0] == source_pos and edge[1] == target_pos), temp_graph['edges']))\n",
        "                    del temp_graph['nodes'][target_pos]\n",
        "                    continue\n",
        "\n",
        "                case {'label': s, **source}, 'conj', {'label': t, **target}:\n",
        "                    temp_graph['edges'] = list(filter(lambda edge: not (edge[0] == source_pos and edge[1] == target_pos), temp_graph['edges']))\n",
        "                    temp_graph['nodes'][source_pos]['conj'] = {'text': t, 'nodeId': target_pos}\n",
        "                    continue\n",
        "\n",
        "                case {'label': s, **source}, 'pcomp', {'label': t, **target}: #in--pcomp--explaining--dobj--x\n",
        "                    temp_graph['edges'] = list(filter(lambda edge: not (edge[0] == source_pos and edge[1] == target_pos), temp_graph['edges']))\n",
        "                    next_node = next((n for src, n, label in temp_graph[\"edges\"] if src == target_pos and label in {'pobj', 'dobj'}), None)\n",
        "                    if next_node:\n",
        "                      temp_graph['nodes'][root_node]['label'] += f\" {t}\"  #if not work f\" {temp_graph['nodes'][root_node]['label']} {t}\n",
        "                      temp_graph['edges'].append((root_node, next_node, 'dobj'))\n",
        "                      temp_graph['edges'] = list(filter(lambda edge: not (edge[0] == target_pos and edge[1] == next_node), temp_graph['edges']))\n",
        "                      del temp_graph['nodes'][source_pos]\n",
        "                      del temp_graph['nodes'][target_pos]\n",
        "                    continue\n",
        "\n",
        "                case {'label': s, **source}, 'ccomp', {'label': t, **target}: #Design interface can help users understand AI decisions\n",
        "                    next_node = next((n for src, n, label in temp_graph[\"edges\"] if src == target_pos and label == 'nsubj'), None)\n",
        "                    if next_node:\n",
        "                      edge_mapping['object_nodes'][source_pos] = next_node\n",
        "                    continue\n",
        "\n",
        "                case {'label': s, **source}, 'nsubj' | 'nsubjpass', {'label': t, **target}:\n",
        "                    edge_mapping['edge_ids'].add(source_pos)\n",
        "                    edge_mapping['subject_nodes'].setdefault(source_pos, set()).add(target_pos)\n",
        "                    if 'conj' in target:\n",
        "                      edge_mapping['subject_nodes'][source_pos].add(target_metadata['nodeId'])\n",
        "                    continue\n",
        "\n",
        "                case {'label': s, **source}, 'dobj' | 'pobj', {'label': t, **target}:\n",
        "                    if next((src for src, n, label in temp_graph[\"edges\"] if src == source_pos and label == 'prep'), None):\n",
        "                      source_metadata['label'] = f\"{s} {t}\"\n",
        "                      temp_graph['edges'] = list(filter(lambda edge: not (edge[0] == source_pos and edge[1] == target_pos), temp_graph['edges']))\n",
        "                      del temp_graph['nodes'][target_pos]\n",
        "                    #assign object outside loop\n",
        "                    continue\n",
        "\n",
        "                case another:\n",
        "                    print(\"another:\", edge)\n",
        "                    unhandled_sentences.add(sentence)\n",
        "                    stopping = True\n",
        "                    continue\n",
        "\n",
        "        except Exception as e:\n",
        "                print(f\"Error occurred in sentence: {sentence} with edge: {edge}, error: {e}\")\n",
        "                unhandled_sentences.add(sentence) # throw error\n",
        "                stopping = True\n",
        "                continue\n",
        "      else:\n",
        "          break\n",
        "\n",
        "    # Update object nodes\n",
        "    edge_mapping['object_nodes'].update({\n",
        "        edge_id: next((tail for head, tail, meta in temp_graph['edges']\n",
        "                      if meta in {'dobj', 'pobj'}), None)\n",
        "        for edge_id in edge_mapping['edge_ids']\n",
        "        if edge_id not in edge_mapping['object_nodes']\n",
        "    })\n",
        "\n",
        "    for edge_id, obj_node in edge_mapping['object_nodes'].items():\n",
        "        if obj_node is None:\n",
        "            print(f\"Missing object node for edge ID: {edge_id}\")\n",
        "\n",
        "    # create final mapping\n",
        "    for edge_id in edge_mapping['edge_ids']:\n",
        "        subject_nodes = edge_mapping['subject_nodes'][edge_id]\n",
        "        object_node = edge_mapping['object_nodes'][edge_id]\n",
        "        edge_node = temp_graph['nodes'][edge_id]\n",
        "        for subject_node in subject_nodes:\n",
        "            #temp_graph['edges'].append((subject_node, object_node, edge_node['label']))\n",
        "            temp_graph['edges'].append((temp_graph[\"nodes\"][subject_node]['label'],\n",
        "                                        temp_graph[\"nodes\"][object_node]['label'],\n",
        "                                        edge_node['label']))\n",
        "            temp_graph['edges'] = list(filter(lambda edge: not (edge[0] == edge_id and edge[1] == subject_node), temp_graph['edges']))\n",
        "\n",
        "        temp_graph['edges'] = list(filter(lambda edge: not (edge[0] == edge_id and edge[1] == object_node), temp_graph['edges']))\n",
        "        del temp_graph['nodes'][edge_id]\n",
        "\n",
        "    temp_graph['edges'] = list(set(temp_graph['edges'])-set([edge for edge in temp_graph['edges'] if edge[2] in allowed_dependencies]))\n",
        "\n",
        "    # all_graphs.append(temp_graph) # we don't need nodes to be pat of json, as edges have them\n",
        "    all_graphs.append({\n",
        "      \"edges\": temp_graph[\"edges\"],\n",
        "      \"sentence\": temp_graph[\"sentence\"]\n",
        "    })\n",
        "\n",
        "    for source, target, edge in temp_graph['edges']:\n",
        "        kg.query(\n",
        "          \"\"\"\n",
        "          CALL apoc.merge.node([$source_label], {label:$source_label}) YIELD node AS s\n",
        "          CALL apoc.merge.node([$target_label], {label:$target_label}) YIELD node AS t\n",
        "          CALL apoc.merge.relationship(s, $edge_label, {sentence: $sentence}, {}, t, {}) YIELD rel\n",
        "          RETURN s,t,rel\n",
        "          \"\"\"\n",
        "          , params={\n",
        "          'source_label': temp_graph['nodes'][source]['label'].replace(\" \", \"_\"),\n",
        "          'target_label': temp_graph['nodes'][target]['label'].replace(\" \", \"_\"),\n",
        "          'edge_label': edge.replace(\" \", \"_\"),\n",
        "          'sentence': sentence\n",
        "        })\n",
        "\n",
        "  except Exception as e:\n",
        "    print(f\"Failed to process sentence: {sentence}, error: {e}\")\n",
        "    unhandled_sentences.add(sentence)\n",
        "\n",
        "\n",
        "with open(\"unhandled_sentences.txt\", \"w\") as file:\n",
        "  for unhandled in unhandled_sentences:\n",
        "    file.write(unhandled + \"\\n\")\n",
        "\n",
        "# Training data - for fine tuning the HF model:\n",
        "with open('graph_data.json', 'w') as json_file:\n",
        "    json.dump(all_graphs, json_file, indent=4, ensure_ascii=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOfJVtH9MrhI"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "# load JSON file\n",
        "def load_json(path):\n",
        "  with open(path, 'r') as file:\n",
        "    return json.load(file)\n",
        "\n",
        "import textwrap\n",
        "# Prints the text with lines wrapped to a maximum width of 80 characters\n",
        "def clean_print(text):\n",
        "    return print(textwrap.fill(text, width=80))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Query graph database"
      ],
      "metadata": {
        "id": "-34SEPp_lGq6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGofGtCqZjrG"
      },
      "outputs": [],
      "source": [
        "CYPHER_GENERATION_TEMPLATE = \"\"\"Task:\n",
        "Generate Cypher statement to query a Neo4j graph database.\n",
        "\n",
        "Instructions:\n",
        "* Only use the provided relationship types, node labels, and properties in the schema.\n",
        "* Do not use any other relationship types, properties, or node labels that are not provided.\n",
        "* Always follow the correct relationship direction.\n",
        "* Ensure that the query follows the correct Cypher syntax.\n",
        "\n",
        "Schema:\n",
        "{schema}\n",
        "\n",
        "Examples:\n",
        "Here are a few examples of generated Cypher statements for particular questions:\n",
        "\n",
        "# What is Bias?\n",
        "    MATCH (s:Bias)-[r]-(t)\n",
        "    RETURN s,r,t\n",
        "\n",
        "# What might introduce Bias?\n",
        "    MATCH (s)-[r:might_introduce]->(t:Bias)\n",
        "    RETURN s,r,t\n",
        "\n",
        "The question is:\n",
        "{question}\n",
        "\n",
        "The generated Cypher statement:\"\"\"\n",
        "\n",
        "# Define schema\n",
        "schema = \"\"\"Node: Risk\n",
        "Properties: label\n",
        "Relationships: (Bias)-[:is]->(Risk)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bfEiuatJTof"
      },
      "outputs": [],
      "source": [
        "# build the query prompt template\n",
        "from langchain.prompts.prompt import PromptTemplate\n",
        "CYPHER_GENERATION_PROMPT = PromptTemplate(\n",
        "    input_variables=[\"schema\", \"question\"],\n",
        "    template=CYPHER_GENERATION_TEMPLATE\n",
        ")\n",
        "\n",
        "# initilize the chain\n",
        "from langchain.chains import GraphCypherQAChain\n",
        "cypherChain = GraphCypherQAChain.from_llm(\n",
        "    graph=kg,\n",
        "    llm=hf_llm,\n",
        "    #cypher_llm=hf_llm,                    # see intermediate steps\n",
        "    cypher_prompt=CYPHER_GENERATION_PROMPT,   # cypher generation prompt\n",
        "    verbose=True,\n",
        "    allow_dangerous_requests=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuWDDHqrQ4fe",
        "outputId": "97482505-0d79-4506-dd26-5ce89324789a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
            "Generated Cypher:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "MATCH (s:Requirement)-[r]-(t)\n",
            "RETURN s,r,t\n",
            "\u001b[0m\n",
            "Full Context:\n",
            "\u001b[32;1m\u001b[1;3m[{'s': {'label': 'Requirement'}, 'r': ({'label': 'Fairness'}, 'is', {'label': 'Requirement'}), 't': {'label': 'Fairness'}}, {'s': {'label': 'Requirement'}, 'r': ({'label': 'Explainability'}, 'is', {'label': 'Requirement'}), 't': {'label': 'Explainability'}}]\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Question: What is Requirement?\n",
            "Response:  Fairness and Explainability are Requirements.\n"
          ]
        }
      ],
      "source": [
        "# test the chain\n",
        "question = \"What is Requirement?\"\n",
        "res = cypherChain.run(question)\n",
        "clean_print('Question: '+question)\n",
        "clean_print('Response: '+res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5VozjCoN8sy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be06af75-a51d-40ef-ef88-8490bdd4b642"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
            "Generated Cypher:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "MATCH (s:Bias)-[r]-(t)\n",
            "RETURN s,r,t\n",
            "\u001b[0m\n",
            "Full Context:\n",
            "\u001b[32;1m\u001b[1;3m[{'s': {'label': 'Bias'}, 'r': ({'label': 'Bias'}, 'is', {'label': 'Risk'}), 't': {'label': 'Risk'}}, {'s': {'label': 'Bias'}, 'r': ({'label': 'Algorithmic_Bias'}, 'is_subclass_of', {'label': 'Bias'}), 't': {'label': 'Algorithmic_Bias'}}, {'s': {'label': 'Bias'}, 'r': ({'label': 'Historical_Bias'}, 'is_subclass_of', {'label': 'Bias'}), 't': {'label': 'Historical_Bias'}}, {'s': {'label': 'Bias'}, 'r': ({'label': 'TrainTestSplit'}, 'might_introduce', {'label': 'Bias'}), 't': {'label': 'TrainTestSplit'}}, {'s': {'label': 'Bias'}, 'r': ({'label': 'Bias'}, 'is_threat_to', {'label': 'Fairness'}), 't': {'label': 'Fairness'}}]\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Question: What is Bias?\n",
            "Response:  Bias is a type of risk. It can be algorithmic bias, historical bias,\n",
            "or it can be introduced by the train-test split. Bias is a threat to fairness.\n"
          ]
        }
      ],
      "source": [
        "question = \"What is Bias?\"\n",
        "res = cypherChain.run(question)\n",
        "clean_print('Question: '+question)\n",
        "clean_print('Response: '+res)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CYPHER_GENERATION_TEMPLATE = \"\"\"Task:\n",
        "Generate Cypher statement to query a Neo4j graph database.\n",
        "\n",
        "Instructions:\n",
        "* Use only the provided node label: {query}.\n",
        "* Traverse from this node and check if it is connected to a node labeled 'Risk'.\n",
        "* Find all nodes connected by the 'is_subclass_of' relationship and return them, excluding the original node.\n",
        "\n",
        "Cypher Query:\n",
        "MATCH (tts:{query})-[*]-(risk:Risk)\n",
        "WHERE risk IS NOT NULL\n",
        "WITH tts\n",
        "MATCH (tts)-[:is_subclass_of]->(parent)\n",
        "MATCH (source)-[:is_subclass_of]->(parent)\n",
        "WHERE source <> tts\n",
        "RETURN source\n",
        "\"\"\"\n",
        "\n",
        "from langchain.prompts.prompt import PromptTemplate\n",
        "\n",
        "CYPHER_GENERATION_PROMPT = PromptTemplate(\n",
        "    input_variables=[\"query\"],\n",
        "    template=CYPHER_GENERATION_TEMPLATE\n",
        ")\n",
        "\n",
        "node_name = \"TrainTestSplit\"\n",
        "\n",
        "cypherChain = GraphCypherQAChain.from_llm(\n",
        "    graph=kg,\n",
        "    llm=hf_llm,\n",
        "    cypher_prompt=CYPHER_GENERATION_PROMPT,\n",
        "    verbose=True,\n",
        "    allow_dangerous_requests=True\n",
        ")\n",
        "\n",
        "# Run the query and get the response\n",
        "res = cypherChain.run({\"query\": node_name})\n",
        "\n",
        "# Display the response\n",
        "clean_print(f'Question: {node_name}')\n",
        "clean_print(f'Response: {res}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 710
        },
        "collapsed": true,
        "id": "xzwg5HqOrbn5",
        "outputId": "8be72ede-5de9-4d47-9e7b-bc4b4fdf64e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
            "Generated Cypher:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Note: This Cypher query traverses the graph from the TrainTestSplit node, checks if it is connected to a Risk node, and then finds all nodes connected by the is_subclass_of relationship, excluding the original TrainTestSplit node. It returns those found nodes.\n",
            "\n",
            "For a more specific example, let's assume we have a graph with the following nodes and relationships:\n",
            "\n",
            "```\n",
            "(a:TrainTestSplit {name: 'TrainSet'})\n",
            "(b:TrainTestSplit {name: 'TestSet'})\n",
            "(c:Risk {name: 'HighRisk'})\n",
            "(d:TrainTestSplit {name: 'Train\u001b[0m\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "CypherSyntaxError",
          "evalue": "{code: Neo.ClientError.Statement.SyntaxError} {message: Invalid input 'Note': expected 'FOREACH', 'ALTER', 'ORDER BY', 'CALL', 'USING PERIODIC COMMIT', 'CREATE', 'LOAD CSV', 'START DATABASE', 'STOP DATABASE', 'DEALLOCATE', 'DELETE', 'DENY', 'DETACH', 'DROP', 'DRYRUN', 'FINISH', 'GRANT', 'INSERT', 'LIMIT', 'MATCH', 'MERGE', 'NODETACH', 'OFFSET', 'OPTIONAL', 'REALLOCATE', 'REMOVE', 'RENAME', 'RETURN', 'REVOKE', 'ENABLE SERVER', 'SET', 'SHOW', 'SKIP', 'TERMINATE', 'UNWIND', 'USE' or 'WITH' (line 2, column 1 (offset: 1))\n\"Note: This Cypher query traverses the graph from the TrainTestSplit node, checks if it is connected to a Risk node, and then finds all nodes connected by the is_subclass_of relationship, excluding the original TrainTestSplit node. It returns those found nodes.\"\n ^}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCypherSyntaxError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-74c11b499928>\u001b[0m in \u001b[0;36m<cell line: 37>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# Run the query and get the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcypherChain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"query\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnode_name\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# Display the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py\u001b[0m in \u001b[0;36mwarning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0mwarned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0memit_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mawarning_emitting_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    604\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"`run` supports only one positional argument.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m             return self(args[0], callbacks=callbacks, tags=tags, metadata=metadata)[\n\u001b[0m\u001b[1;32m    607\u001b[0m                 \u001b[0m_output_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m             ]\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py\u001b[0m in \u001b[0;36mwarning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0mwarned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0memit_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mawarning_emitting_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    387\u001b[0m         }\n\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m         return self.invoke(\n\u001b[0m\u001b[1;32m    390\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m             \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRunnableConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             outputs = (\n\u001b[0;32m--> 160\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/chains/graph_qa/cypher.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;31m# Generated Cypher be null if query corrector identifies invalid schema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgenerated_cypher\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_cypher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/graphs/neo4j_graph.py\u001b[0m in \u001b[0;36mquery\u001b[0;34m(self, query, params)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m             data, _, _ = self._driver.execute_query(\n\u001b[0m\u001b[1;32m    432\u001b[0m                 \u001b[0mQuery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m                 \u001b[0mdatabase_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_database\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/neo4j/_sync/driver.py\u001b[0m in \u001b[0;36mexecute_query\u001b[0;34m(self, query_, parameters_, routing_, database_, impersonated_user_, bookmark_manager_, auth_, result_transformer_, **kwargs)\u001b[0m\n\u001b[1;32m    969\u001b[0m                 )\n\u001b[1;32m    970\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pipelined_begin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 971\u001b[0;31m                 return session._run_transaction(\n\u001b[0m\u001b[1;32m    972\u001b[0m                     \u001b[0maccess_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m                     \u001b[0mTelemetryAPI\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDRIVER\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/neo4j/_sync/work/session.py\u001b[0m in \u001b[0;36m_run_transaction\u001b[0;34m(self, access_mode, api, transaction_function, args, kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m                 \u001b[0mtx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transaction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransaction_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCancelledError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m                     \u001b[0;31m# if cancellation callback has not been called yet:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/neo4j/_work/query.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mwrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/neo4j/_sync/driver.py\u001b[0m in \u001b[0;36m_work\u001b[0;34m(tx, query, parameters, transformer)\u001b[0m\n\u001b[1;32m   1305\u001b[0m     \u001b[0mtransformer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mResult\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_T\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m ) -> _T:\n\u001b[0;32m-> 1307\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1308\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/neo4j/_sync/work/transaction.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, query, parameters, **kwparameters)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tx_ready_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/neo4j/_sync/work/result.py\u001b[0m in \u001b[0;36m_tx_ready_run\u001b[0;34m(self, query, parameters)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;31m# BEGIN {extra}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;31m# RUN \"query\" {parameters} {extra}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     def _run(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/neo4j/_sync/work/result.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, query, parameters, db, imp_user, access_mode, bookmarks, notifications_min_severity, notifications_disabled_classifications)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_pull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/neo4j/_sync/work/result.py\u001b[0m in \u001b[0;36m_attach\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exhausted\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attached\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/neo4j/_sync/io/_common.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                     \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mNeo4jError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mServiceUnavailable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSessionExpired\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                     \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miscoroutinefunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__on_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/neo4j/_sync/io/_bolt.py\u001b[0m in \u001b[0;36mfetch_message\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    975\u001b[0m             \u001b[0mhydration_hooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhydration_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m         )\n\u001b[0;32m--> 977\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midle_since\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/neo4j/_sync/io/_bolt5.py\u001b[0m in \u001b[0;36m_process_message\u001b[0;34m(self, tag, fields)\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_server_state_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbolt_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFAILED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_failure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary_metadata\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mServiceUnavailable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatabaseUnavailable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/neo4j/_sync/io/_common.py\u001b[0m in \u001b[0;36mon_failure\u001b[0;34m(self, metadata)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0mhandler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandlers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"on_summary\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0mUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNeo4jError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhydrate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_ignored\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCypherSyntaxError\u001b[0m: {code: Neo.ClientError.Statement.SyntaxError} {message: Invalid input 'Note': expected 'FOREACH', 'ALTER', 'ORDER BY', 'CALL', 'USING PERIODIC COMMIT', 'CREATE', 'LOAD CSV', 'START DATABASE', 'STOP DATABASE', 'DEALLOCATE', 'DELETE', 'DENY', 'DETACH', 'DROP', 'DRYRUN', 'FINISH', 'GRANT', 'INSERT', 'LIMIT', 'MATCH', 'MERGE', 'NODETACH', 'OFFSET', 'OPTIONAL', 'REALLOCATE', 'REMOVE', 'RENAME', 'RETURN', 'REVOKE', 'ENABLE SERVER', 'SET', 'SHOW', 'SKIP', 'TERMINATE', 'UNWIND', 'USE' or 'WITH' (line 2, column 1 (offset: 1))\n\"Note: This Cypher query traverses the graph from the TrainTestSplit node, checks if it is connected to a Risk node, and then finds all nodes connected by the is_subclass_of relationship, excluding the original TrainTestSplit node. It returns those found nodes.\"\n ^}"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-KtvrZ7b8jI"
      },
      "source": [
        "#### Fine tune Mistral-7B Lora"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Libraries\n",
        "!pip install -q streamlit                                                                             # For Deploying apps\n",
        "!pip install -q transformers>=4.32.0 datasets evaluate                                                # Comes from HuggingFace\n",
        "!pip install -q auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/   # Use cu117 if on CUDA 11.7\n",
        "!pip install -q optimum                                                                               # For GPTQ Optimization\n",
        "!pip install -q -U bitsandbytes                                                                       # For quantization\n",
        "!pip install -q -U peft                                                                               # Parameter-efficient Fine-tuning\n",
        "!pip install -q -U accelerate                                                                         # Loading models across GPUs/CPU/disk\n",
        "!pip install -q trl==0.7.1"
      ],
      "metadata": {
        "collapsed": true,
        "id": "wslKAWxK30mM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Loading the Model"
      ],
      "metadata": {
        "id": "gPUwUOpGUs7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer"
      ],
      "metadata": {
        "id": "QD8ROSsy4JRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name_or_path = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name_or_path,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    revision=\"main\",\n",
        "    token=HF_API_KEY)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)"
      ],
      "metadata": {
        "id": "RPHMqhcr4Qsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch                                # Deep Learning Framework\n",
        "import time                                 # Measure inference time\n",
        "import pandas as pd                         # For table dataset structure\n",
        "import numpy as np                          # Numerical operations on the CPU\n",
        "from datasets import load_dataset           # Loading the dataseet\n",
        "import random"
      ],
      "metadata": {
        "id": "l6KOn8Uk4ujI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Loading the Quantized Model"
      ],
      "metadata": {
        "id": "6ppMfiwPUhoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "# 1. Setup the quantization configuarion\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,                     # Q = 4 bits\n",
        "    bnb_4bit_use_double_quant=True,        # double quantization, quantizing the quantization constants for saving an additional 0.4 bits per parameter\n",
        "    bnb_4bit_quant_type=\"nf4\",             # 4-bit NormalFloat Quantization (optimal for normal weights; enforces w âˆˆ [-1,1])\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16  # Dequantize to 16-bits before computations (as in the paper)\n",
        ")\n",
        "# 2. Pass it while using the model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name_or_path,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "ca189e831a2b4694a276f9324504c0e5",
            "8420489150824a1b9165babf8feff934",
            "6c4c236462574357ab7421efe325a9a9",
            "215698777d0848639f8d4f6e66908b81",
            "9f1201b9b7814a97a13a45fcb2e6380d",
            "0b598bbbae4045b6b5dab9567ef6175b",
            "41c087371c554945a4bd24a2964b17a5",
            "9ed3ea3a6a4e42389757f2052f498c8e",
            "ae94f9fd759c47d3b8db970abcfd1a41",
            "7b1fb9f04270484585e6599da952e0c7",
            "86c56f26f32a40269e80fb0c9fc9a93e"
          ]
        },
        "id": "ZbNZDyLw4zf4",
        "outputId": "eb96c878-4723-489c-982b-768436947c4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ca189e831a2b4694a276f9324504c0e5"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Loading the Tokenizer"
      ],
      "metadata": {
        "id": "lk7HCW6cUlNq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ],
      "metadata": {
        "id": "JqTjVfbI9owl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "How are you, Llama! Tell me about yourself.\n",
        "\"\"\"\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors='pt')\n",
        "output_tokens = model.generate(inputs[\"input_ids\"], max_new_tokens=100,)[0]     # batch of tokens with one sequence\n",
        "output = tokenizer.decode(output_tokens, skip_special_tokens=True)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "pWb29G-B9ryA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "import json\n",
        "# Load your JSON file\n",
        "with open('graph_data.json', 'r') as f:\n",
        "    graph_data = json.load(f)\n",
        "\n",
        "# Prepare the dataset (input/output pairs) from your JSON data\n",
        "# Assuming the JSON file has the 'sentence' and 'edges' structure\n",
        "\n",
        "train_data_dict = {\"input\": [], \"output\": []}  # Initialize an empty dictionary with the desired columns\n",
        "for graph in graph_data:\n",
        "    train_data_dict[\"input\"].append(graph.get(\"sentence\", \"\"))\n",
        "    train_data_dict[\"output\"].append(f\"Edges: {graph['edges']}\")\n",
        "\n",
        "# Convert the dictionary into a Hugging Face Dataset\n",
        "dataset = Dataset.from_dict(train_data_dict)\n",
        "\n",
        "train_dataset, val_dataset = dataset.train_test_split(test_size=0.2).values()\n",
        "\n",
        "# Preprocessing function for dataset\n",
        "def preprocess_function(examples):\n",
        "    inputs = examples[\"input\"]\n",
        "    outputs = examples[\"output\"]\n",
        "\n",
        "    model_inputs = tokenizer(inputs, truncation=True, padding=True, max_length=256)\n",
        "    labels = tokenizer(outputs, truncation=True, padding=True, max_length=256)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# Preprocess the dataset\n",
        "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
        "val_dataset = val_dataset.map(preprocess_function, batched=True)\n",
        "print(f\"Training dataset size: {len(train_dataset)}\")\n",
        "print(f\"Validation dataset size: {len(val_dataset)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115,
          "referenced_widgets": [
            "3eebf3da54f94374846d40b266f5227d",
            "a85287f4b6b749c088305ff41dcb6913",
            "402151eec95b4fbc921508a7b626ca69",
            "933a97ade9554484beaf8027feec2e22",
            "fae0ded9eeda4735983b8610870f64b6",
            "def83d3d7aa44eeabcbba9eaca53564f",
            "e7b010d375ef4cbb9ea7b3c87a5a8a73",
            "ac0c06f242004b1ebbf1438cab35914b",
            "6abe4380b31d4073b62a1d4a23b5a8d2",
            "e317bd384f6340829bd0562930b88374",
            "7f452df6c8524dd9b273d5d02fea0ac0",
            "0909b187346e471a8aea7c85de606c18",
            "9916efc5089a47f1ade900d381446176",
            "8ba9c2d95bdf4d6f93588016eef3e56f",
            "91bc2f38d06847528b5770b97b24d624",
            "b14ce7ca8d3a48728d314cafffc14ecb",
            "b7429902ba7d4016a68f83bb9d5d4b67",
            "60f2ec7ee45946b1aae83f3c5294fce4",
            "a2973c4ceb064f999426a896d24e861e",
            "1245ba36c9074a29b832cd069bcf7cea",
            "027740da05424e3fb7a8bf127b7f67e6",
            "b518f7fa98ae49e68e851b1e8d658bd8"
          ]
        },
        "id": "FzedM_Y5BTMX",
        "outputId": "0d61f702-b7b8-416e-adf7-51b4fe5ff726"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/206 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3eebf3da54f94374846d40b266f5227d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/52 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0909b187346e471a8aea7c85de606c18"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training dataset size: 206\n",
            "Validation dataset size: 52\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data[0])"
      ],
      "metadata": {
        "id": "XeeJycYQEcS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "WTmvMJ_bCIDD"
      },
      "outputs": [],
      "source": [
        "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
        "# Prepare the model for LoRA fine-tuning\n",
        "lora_config = LoraConfig(\n",
        "    r=16,  # rank of the low-rank matrices\n",
        "    lora_alpha=64,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"  # LoRA fine-tuning for causal language modeling task\n",
        ")\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "model = get_peft_model(model, lora_config)\n",
        "#print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Train the Model"
      ],
      "metadata": {
        "id": "cmlivAumUMcJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_arguments = TrainingArguments(\n",
        "    fp16=True,                           # Training computations in 16 bits\n",
        "    # batch-related\n",
        "    per_device_train_batch_size=2,       # Batch Size\n",
        "    gradient_accumulation_steps=4,       # Batch Size (Mathematically)\n",
        "    # optimizer-related\n",
        "    optim=\"paged_adamw_32bit\",           # Variant of AdamW designed to be more efficient on 32-bit GPUs\n",
        "    learning_rate=1e-4,                  # Learning Rate\n",
        "    warmup_ratio=0.05,                   # After 5% of the data, learning rate has linearly  from 0 to 1e-4\n",
        "    lr_scheduler_type=\"cosine\",          # Adjust learning rate sinusoidally\n",
        "    max_grad_norm=0.3,                   # Clip gradients if less than 0.3 (prevent gradient explosion)\n",
        "    # epochs and saving\n",
        "    num_train_epochs=2,                  # Number of Epochs\n",
        "    save_strategy=\"epoch\",               # Save after each epoch\n",
        "    output_dir=\"epoch-finetuned\",        # Where to save the model\n",
        "    # validation\n",
        "    evaluation_strategy=\"steps\",         # For the next argument\n",
        "    eval_steps=0.2,                      # Evaluate after 20% of training steps\n",
        "    # logging-related\n",
        "    logging_steps=1,                     # Number of update steps between two logs\n",
        "    group_by_length=True,                # Minimize padding by grouping sentences of similar length\n",
        "    seed=42,                             # For consistent results\n",
        ")\n",
        "model.gradient_checkpointing_enable()    # Store less activations and recompute later\n",
        "model.config.use_cache = False           # Disable using attention output cache. Should be enabled in inference."
      ],
      "metadata": {
        "id": "qqXtJtGsK5jL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade trl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "UwllVzi6Mo-D",
        "outputId": "9323f0ba-807c-472b-cbdb-26053ac85a1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: trl in /usr/local/lib/python3.10/dist-packages (0.7.1)\n",
            "Collecting trl\n",
            "  Downloading trl-0.11.4-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from trl) (2.5.0+cu121)\n",
            "Requirement already satisfied: transformers>=4.40.0 in /usr/local/lib/python3.10/dist-packages (from trl) (4.44.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from trl) (1.0.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from trl) (3.0.2)\n",
            "Collecting tyro>=0.5.11 (from trl)\n",
            "  Downloading tyro-0.8.14-py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.2 in /usr/local/lib/python3.10/dist-packages (from trl) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (2024.6.1)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.4.0->trl) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.40.0->trl) (0.24.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.40.0->trl) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.40.0->trl) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.40.0->trl) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.40.0->trl) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.40.0->trl) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.40.0->trl) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.40.0->trl) (4.66.5)\n",
            "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (0.16)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (13.9.3)\n",
            "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl)\n",
            "  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->trl) (5.9.5)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (16.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (3.10.10)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (1.16.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.40.0->trl) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.40.0->trl) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.40.0->trl) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.40.0->trl) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.18.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.4.0->trl) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl) (2024.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->trl) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets->trl) (0.2.0)\n",
            "Downloading trl-0.11.4-py3-none-any.whl (316 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m316.6/316.6 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tyro-0.8.14-py3-none-any.whl (109 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m109.8/109.8 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: shtab, tyro, trl\n",
            "  Attempting uninstall: trl\n",
            "    Found existing installation: trl 0.7.1\n",
            "    Uninstalling trl-0.7.1:\n",
            "      Successfully uninstalled trl-0.7.1\n",
            "Successfully installed shtab-1.7.1 trl-0.11.4 tyro-0.8.14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "    # tokenizer and model\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    peft_config=lora_config,\n",
        "    max_seq_length=1024,\n",
        "    args=training_arguments,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "9RvhU8jtLKv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Save Trained Model"
      ],
      "metadata": {
        "id": "eJ2YGGleUFsC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "peft_model_path =os.path.expanduser(\"~/Downloads/fine-tuned-mistral\")\n",
        "os.makedirs(peft_model_path, exist_ok=True)\n",
        "trainer.model.save_pretrained(peft_model_path)\n",
        "tokenizer.save_pretrained(peft_model_path)"
      ],
      "metadata": {
        "id": "LkFvzmRlMrtj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import AutoPeftModelForCausalLM\n",
        "from transformers import AutoTokenizer\n",
        "tuned_model = AutoPeftModelForCausalLM.from_pretrained(peft_model_path, low_cpu_mem_usage=True, torch_dtype=torch.float16, load_in_4bit=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(peft_model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "562a2edaa8ba461e8617515e7af06814",
            "99e5ae044bf34a1c9092d8dc6a9a6461",
            "e732ecd79ad04c54aba93e967ef13ec0",
            "157614047bdc4587b789a5d418bcd381",
            "ff27741944af4f29aae9e1dc1a421d96",
            "e43baad4de354b14bf583ab54e312d50",
            "02f7c30e892c4bceb203c11aff42e067",
            "b6e3e96935224d368e53ccf1dbfa4049",
            "f7053e1ae0fa4422a468a1d42ef44149",
            "5e35161adb8b4408900e98c8336cf82d",
            "30af5fcd2f524a1b9fba3606795a0d46"
          ]
        },
        "id": "5qocwGrLSiEq",
        "outputId": "0e390316-3526-424a-dbb2-110dd27db497"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "562a2edaa8ba461e8617515e7af06814"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Inference"
      ],
      "metadata": {
        "id": "SRw0kKFDUAS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example single sentence for the model to process\n",
        "sentence = \"X is subclass of Y.\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Extract the nodes and relationships from the following sentence.\n",
        "\n",
        "### Input:\n",
        "\"{sentence}\"\n",
        "\n",
        "### Nodes and Relationships:\n",
        "\"\"\"\n",
        "\n",
        "# Encode the prompt\n",
        "input_ids = tokenizer(prompt, return_tensors='pt', truncation=True).input_ids.cuda()\n",
        "outputs = tuned_model.generate(input_ids=input_ids, max_new_tokens=150)\n",
        "output = tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]\n",
        "\n",
        "# Print the generated nodes and relationships\n",
        "print(f'TRAINED MODEL GENERATED RESULT :\\n{output}')"
      ],
      "metadata": {
        "id": "x6jK6uV-OyEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example single sentence for the model to process\n",
        "sentence = \"Re-weighting mitigates Algorithmic Bias\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Extract the nodes and relationships from the following sentence.\n",
        "\n",
        "### Input:\n",
        "\"{sentence}\"\n",
        "\n",
        "### Nodes and Relationships:\n",
        "\"\"\"\n",
        "\n",
        "# Encode the prompt\n",
        "input_ids = tokenizer(prompt, return_tensors='pt', truncation=True).input_ids.cuda()\n",
        "outputs = tuned_model.generate(input_ids=input_ids, max_new_tokens=150)\n",
        "output = tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]\n",
        "\n",
        "# Print the generated nodes and relationships\n",
        "print(f'TRAINED MODEL GENERATED RESULT :\\n{output}')"
      ],
      "metadata": {
        "id": "axRxWCdLSb_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example single sentence for the model to process\n",
        "sentence = \"Societal and environmental well-being Includes sustainability\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Extract the nodes and relationships from the following sentence.\n",
        "\n",
        "### Input:\n",
        "\"{sentence}\"\n",
        "\n",
        "### Nodes and Relationships:\n",
        "\"\"\"\n",
        "\n",
        "# Encode the prompt\n",
        "input_ids = tokenizer(prompt, return_tensors='pt', truncation=True).input_ids.cuda()\n",
        "outputs = tuned_model.generate(input_ids=input_ids, max_new_tokens=150)\n",
        "output = tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]\n",
        "\n",
        "# Print the generated nodes and relationships\n",
        "print(f'TRAINED MODEL GENERATED RESULT :\\n{output}')"
      ],
      "metadata": {
        "id": "crBRXLDgTP5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example single sentence for the model to process\n",
        "sentence = \"AI systemsâ€™ resource usage and energy consumption need to be monitored\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Extract the nodes and relationships from the following sentence.\n",
        "\n",
        "### Input:\n",
        "\"{sentence}\"\n",
        "\n",
        "### Nodes and Relationships:\n",
        "\"\"\"\n",
        "\n",
        "# Encode the prompt\n",
        "input_ids = tokenizer(prompt, return_tensors='pt', truncation=True).input_ids.cuda()\n",
        "outputs = tuned_model.generate(input_ids=input_ids, max_new_tokens=150)\n",
        "output = tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]\n",
        "\n",
        "# Print the generated nodes and relationships\n",
        "print(f'TRAINED MODEL GENERATED RESULT :\\n{output}')"
      ],
      "metadata": {
        "id": "QNOobaBwTfCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WSkxxyukTsaV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "6r2T3wQTVCYi",
        "8ekpkRjo3KxM",
        "-34SEPp_lGq6",
        "6-KtvrZ7b8jI",
        "gPUwUOpGUs7x",
        "6ppMfiwPUhoQ",
        "lk7HCW6cUlNq",
        "cmlivAumUMcJ",
        "eJ2YGGleUFsC",
        "SRw0kKFDUAS2"
      ],
      "gpuType": "L4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ca189e831a2b4694a276f9324504c0e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8420489150824a1b9165babf8feff934",
              "IPY_MODEL_6c4c236462574357ab7421efe325a9a9",
              "IPY_MODEL_215698777d0848639f8d4f6e66908b81"
            ],
            "layout": "IPY_MODEL_9f1201b9b7814a97a13a45fcb2e6380d"
          }
        },
        "8420489150824a1b9165babf8feff934": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b598bbbae4045b6b5dab9567ef6175b",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_41c087371c554945a4bd24a2964b17a5",
            "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
          }
        },
        "6c4c236462574357ab7421efe325a9a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9ed3ea3a6a4e42389757f2052f498c8e",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ae94f9fd759c47d3b8db970abcfd1a41",
            "value": 3
          }
        },
        "215698777d0848639f8d4f6e66908b81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b1fb9f04270484585e6599da952e0c7",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_86c56f26f32a40269e80fb0c9fc9a93e",
            "value": "â€‡3/3â€‡[00:10&lt;00:00,â€‡â€‡3.54s/it]"
          }
        },
        "9f1201b9b7814a97a13a45fcb2e6380d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b598bbbae4045b6b5dab9567ef6175b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41c087371c554945a4bd24a2964b17a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9ed3ea3a6a4e42389757f2052f498c8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae94f9fd759c47d3b8db970abcfd1a41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7b1fb9f04270484585e6599da952e0c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86c56f26f32a40269e80fb0c9fc9a93e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3eebf3da54f94374846d40b266f5227d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a85287f4b6b749c088305ff41dcb6913",
              "IPY_MODEL_402151eec95b4fbc921508a7b626ca69",
              "IPY_MODEL_933a97ade9554484beaf8027feec2e22"
            ],
            "layout": "IPY_MODEL_fae0ded9eeda4735983b8610870f64b6"
          }
        },
        "a85287f4b6b749c088305ff41dcb6913": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_def83d3d7aa44eeabcbba9eaca53564f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e7b010d375ef4cbb9ea7b3c87a5a8a73",
            "value": "Map:â€‡100%"
          }
        },
        "402151eec95b4fbc921508a7b626ca69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac0c06f242004b1ebbf1438cab35914b",
            "max": 206,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6abe4380b31d4073b62a1d4a23b5a8d2",
            "value": 206
          }
        },
        "933a97ade9554484beaf8027feec2e22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e317bd384f6340829bd0562930b88374",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_7f452df6c8524dd9b273d5d02fea0ac0",
            "value": "â€‡206/206â€‡[00:00&lt;00:00,â€‡3578.21â€‡examples/s]"
          }
        },
        "fae0ded9eeda4735983b8610870f64b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "def83d3d7aa44eeabcbba9eaca53564f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7b010d375ef4cbb9ea7b3c87a5a8a73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ac0c06f242004b1ebbf1438cab35914b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6abe4380b31d4073b62a1d4a23b5a8d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e317bd384f6340829bd0562930b88374": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f452df6c8524dd9b273d5d02fea0ac0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0909b187346e471a8aea7c85de606c18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9916efc5089a47f1ade900d381446176",
              "IPY_MODEL_8ba9c2d95bdf4d6f93588016eef3e56f",
              "IPY_MODEL_91bc2f38d06847528b5770b97b24d624"
            ],
            "layout": "IPY_MODEL_b14ce7ca8d3a48728d314cafffc14ecb"
          }
        },
        "9916efc5089a47f1ade900d381446176": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b7429902ba7d4016a68f83bb9d5d4b67",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_60f2ec7ee45946b1aae83f3c5294fce4",
            "value": "Map:â€‡100%"
          }
        },
        "8ba9c2d95bdf4d6f93588016eef3e56f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a2973c4ceb064f999426a896d24e861e",
            "max": 52,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1245ba36c9074a29b832cd069bcf7cea",
            "value": 52
          }
        },
        "91bc2f38d06847528b5770b97b24d624": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_027740da05424e3fb7a8bf127b7f67e6",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_b518f7fa98ae49e68e851b1e8d658bd8",
            "value": "â€‡52/52â€‡[00:00&lt;00:00,â€‡2046.27â€‡examples/s]"
          }
        },
        "b14ce7ca8d3a48728d314cafffc14ecb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7429902ba7d4016a68f83bb9d5d4b67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60f2ec7ee45946b1aae83f3c5294fce4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a2973c4ceb064f999426a896d24e861e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1245ba36c9074a29b832cd069bcf7cea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "027740da05424e3fb7a8bf127b7f67e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b518f7fa98ae49e68e851b1e8d658bd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "562a2edaa8ba461e8617515e7af06814": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_99e5ae044bf34a1c9092d8dc6a9a6461",
              "IPY_MODEL_e732ecd79ad04c54aba93e967ef13ec0",
              "IPY_MODEL_157614047bdc4587b789a5d418bcd381"
            ],
            "layout": "IPY_MODEL_ff27741944af4f29aae9e1dc1a421d96"
          }
        },
        "99e5ae044bf34a1c9092d8dc6a9a6461": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e43baad4de354b14bf583ab54e312d50",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_02f7c30e892c4bceb203c11aff42e067",
            "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
          }
        },
        "e732ecd79ad04c54aba93e967ef13ec0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6e3e96935224d368e53ccf1dbfa4049",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f7053e1ae0fa4422a468a1d42ef44149",
            "value": 3
          }
        },
        "157614047bdc4587b789a5d418bcd381": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e35161adb8b4408900e98c8336cf82d",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_30af5fcd2f524a1b9fba3606795a0d46",
            "value": "â€‡3/3â€‡[00:09&lt;00:00,â€‡â€‡3.24s/it]"
          }
        },
        "ff27741944af4f29aae9e1dc1a421d96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e43baad4de354b14bf583ab54e312d50": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02f7c30e892c4bceb203c11aff42e067": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b6e3e96935224d368e53ccf1dbfa4049": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7053e1ae0fa4422a468a1d42ef44149": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5e35161adb8b4408900e98c8336cf82d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30af5fcd2f524a1b9fba3606795a0d46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}