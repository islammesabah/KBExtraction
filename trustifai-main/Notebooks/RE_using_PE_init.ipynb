{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74bSNuM04Stm"
      },
      "source": [
        "Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain_huggingface\n",
        "!pip install -q langchain-community\n",
        "!pip install -q \"unstructured[local-inference]\"\n",
        "!pip install -q pymupdf"
      ],
      "metadata": {
        "id": "NgK7vCWO5XRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Connect to the models"
      ],
      "metadata": {
        "id": "T3ZWlkYi0NwD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "# get your HF TOKEN from https://huggingface.co/settings/tokens/new?tokenType=read\n",
        "HF_API_KEY = userdata.get('HF_API_KEY')"
      ],
      "metadata": {
        "id": "xaBOR28L7oSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WaEwbwSi4Eio"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "\n",
        "# for now mistral model works but we will need to update\n",
        "# define huggingface generation endpoint\n",
        "hf_llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\", # Model Name\n",
        "    task=\"text-generation\",                       # task as generating a text response\n",
        "    max_new_tokens=200,                           # maximum numbers of generated tokens\n",
        "    do_sample=False,                              # disables sampling\n",
        "    huggingfacehub_api_token=HF_API_KEY           # ðŸ¤— huggingface API token\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# use LLaMA model internal server\n",
        "from openai import OpenAI\n",
        "#set the base url to the local server\n",
        "# client = OpenAI(base_url=\"WILL PROVIDE THE URL\",\n",
        "                # api_key=\"\") #setting an api key is required from the openai framework, but the server itself does not use it\n",
        "\n",
        "#get a list of models hosted on the local server\n",
        "# print(client.models.list())\n",
        "\n",
        "# Call the ChatCompletion API\n",
        "# completion = client.chat.completions.create(\n",
        "#       model=\"meta-llama-3.1-70b-instruct-fp8\",\n",
        "#       messages=[\n",
        "#           {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "#           {\"role\": \"user\", \"content\": \"Hello, are you there?\"}\n",
        "#           ])\n",
        "\n",
        "#print the response\n",
        "\n",
        "# print(completion.choices[0].message.content)"
      ],
      "metadata": {
        "id": "1loEPS9B0ovI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the data"
      ],
      "metadata": {
        "id": "Eunw-Iw42ZaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_data_file = 'sentence_data.txt'\n",
        "pdf_path = '20241015_MISSION_KI_Glossar_v1.0 en.pdf'\n",
        "\n",
        "# load the sentences data and split it for first part\n",
        "###   UPDATE THIS part to load the snetences  ###\n",
        "\n",
        "# for chunks load the pdf file and do the chunking and select rondom thirty chunks\n",
        "\n",
        "from langchain.document_loaders import UnstructuredPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "loader = UnstructuredPDFLoader(pdf_path)\n",
        "data = loader.load()\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=70)\n",
        "documents = text_splitter.split_documents(data)\n",
        "import random\n",
        "random.seed(42)\n",
        "num_chunks_to_select = 15  # Number of random chunks you want\n",
        "random_chunks = random.sample(documents, num_chunks_to_select)"
      ],
      "metadata": {
        "id": "-scWCtcv2ZJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Few shot examples"
      ],
      "metadata": {
        "id": "sCsoI1kV17cn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_examples = [\n",
        "    {\n",
        "        \"sentence\":\"\"\"Transparency\n",
        "Property of an â†’KI system that is explainable and comprehensible. In the context of this quality\n",
        "standard, \"transparency\" also includes documentation of the properties of the â†’KI system.\"\"\",\n",
        "        \"output\":\"\"\"Transparency is a property of KI system\n",
        "        Transparency is explainable\n",
        "        Transparency is comprehensible\n",
        "        Transparency includes documentation of properties of KI system\"\"\"\n",
        "    },\n",
        "    {\n",
        "        \"sentence\":\"\"\"opacity\n",
        "opaqueness:\n",
        "Property of a system that appropriate information about the system is unavailable to relevant stakeholders.\"\"\",\n",
        "        \"output\":\"\"\"Opacity also called Opaqueness\n",
        "        Opacity is a Property of a system\n",
        "        Property of a system has characteristic Information unavailable to stakeholders\"\"\"\n",
        "    },\n",
        "    {\n",
        "        \"sentence\":\"\"\"This requirement is closely linked with the principle of explicability and encompasses transparency of elements relevant to an AI system: the data, the system and the business models.\"\"\",\n",
        "        \"output\":\"\"\"Requirement is linked with principle of explicability\n",
        "        Requirement encompasses Transparency of elements\n",
        "        Transparency is relevant to AI system\n",
        "        Elements include Data\n",
        "        Elements include System\n",
        "        Elements include Business models\"\"\"\n",
        "    },\n",
        "]\n",
        "\n",
        "chunks_examples = [\n",
        "    {\n",
        "        \"chunk\":\"\"\"behavior and/or functioning of this â†’AI system in principle and during operation and, if necessary, to\n",
        "terminate it.\n",
        "Monitoring\n",
        "Procedure in which deviations between observable actual states and the desired target states are detected\n",
        "during the operation of an â†’KI system.\n",
        "Non-discrimination\n",
        "Characteristic of an open process carried out by an â†’KI system if, in the course of this process, several\n",
        "human individuals are treated in comparison with each other and this process is carried out in an open\n",
        "process.\n",
        "is legally free from the mistreatment of a human individual on the basis of a legally protected\n",
        "characteristic.\n",
        "User information\"\"\",\n",
        "        \"output\":\"\"\"AI system has property functioning in principle and during operation\n",
        "        Monitoring is deviations between observable actual states and desired target states\n",
        "        Deviations occur during operation of an KI system\n",
        "        Monitoring is a procedure for detecting deviations during operation (Optional, better to have)\n",
        "        Non-discrimination is a characteristic of an open process by an KI system\n",
        "        Open process treats several human individuals in comparison with each other\n",
        "        Non-discrimination ensures no mistreatment of a human individual\n",
        "        Mistreatment is based on legally protected characteristics\"\"\"\n",
        "    },\n",
        "    {\n",
        "        \"chunk\":\"\"\"characteristic.\n",
        "User information\n",
        "Characteristics of an â†’AI system with regard to the quality of information, interaction and operation by a\n",
        "user, including knowledge of the involvement of AI, barriers, and the quality of the user experience.\n",
        "freedom and with a view to preventing nudging.\n",
        "Robustness\n",
        "Ability of an â†’AI system to maintain its regular and usual behavior and functioning in the best possible\n",
        "way even in the event of non-malicious, adverse, disruptive or faulty inputs or external influences.\n",
        "to keep.\n",
        "Traceability\n",
        "Property of an â†’KI system with regard to the ability to record the consecutive sequence of all decisions\"\"\",\n",
        "        \"output\":\"\"\"AI system has characteristic User information\n",
        "        User information relates to quality of information, interaction and operation by a user\n",
        "        AI system includes Knowledge of AI involvement\n",
        "        AI system includes barriers\n",
        "        AI system includes quality of the user experience\n",
        "        AI system has ability Robustness\n",
        "        Robustness allows regular and usual behavior in adverse conditions\n",
        "        AI system maintains behavior even under faulty inputs or external influences\n",
        "        AI system has property Traceability\n",
        "        Traceability relates to recording consecutive sequence of all decisions\"\"\"\n",
        "    },\n",
        "    {\n",
        "        \"chunk\":\"\"\"that enter or have entered an â†’KI system along the entire life cycle.\n",
        "Transparency\n",
        "Property of an â†’KI system that is explainable and comprehensible. In the context of this quality\n",
        "standard, \"transparency\" also includes documentation of the properties of the â†’KI system.\"\"\",\n",
        "        \"output\":\"\"\"Transparency is a property of KI system\n",
        "        Transparency is explainable\n",
        "        Transparency is comprehensible\n",
        "        Transparency includes documentation of properties of the KI system\"\"\"\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "5yiH3gzE1-Aa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "output parsing into list of strings"
      ],
      "metadata": {
        "id": "B3TPKfWY-bEg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from langchain.schema import BaseOutputParser\n",
        "\n",
        "class NewLineSeparatedOutputParser(BaseOutputParser):\n",
        "    def parse(self, text: str) -> List[str]:\n",
        "        return text.strip().split('\\n')"
      ],
      "metadata": {
        "id": "ne71uZ0M-axw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtMP2UrSDRHs"
      },
      "source": [
        "prompt engineering"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# please keep a record for all prompts you try\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "user_query = \"HERE\"\n",
        "\n",
        "# prepare prompt\n",
        "###   UPDATE THIS PROMPT  ###\n",
        "template = \"\"\"BUILD YOUR PROMPT ADDING THE USER_QUERY AND EXAMPLE\"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "simple_rag_chain = (\n",
        " prompt                                   # build the prompt\n",
        " | hf_llm                                 # llm for generation\n",
        " | NewLineSeparatedOutputParser()         # collect the response into list\n",
        ")\n",
        "\n",
        "# Display response\n",
        "print(\"Generated Response List:\")\n",
        "print(simple_rag_chain.invoke(user_query))"
      ],
      "metadata": {
        "id": "Wirp8mGHoUv0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}