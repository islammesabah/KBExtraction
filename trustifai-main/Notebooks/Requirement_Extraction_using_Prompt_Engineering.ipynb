{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74bSNuM04Stm"
      },
      "source": [
        "##### Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "gMPIkr27ZGCj"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "%pip install -U langchain huggingface_hub transformers sentence_transformers\n",
        "%pip install -U langchain-community\n",
        "%pip install -U langchain-huggingface\n",
        "%pip install -U langchain_experimental\n",
        "%pip install -U langchain_openai\n",
        "%pip install -U langchain-chroma\n",
        "\n",
        "%pip install -U chromadb\n",
        "\n",
        "%pip install -U unstructured\n",
        "%pip install -U sentence-transformers\n",
        "%pip install -U nltk\n",
        "%pip install -U spacy\n",
        "%pip install -U --upgrade pymupdf\n",
        "%pip install -U transformers torch\n",
        "\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "# Warning control\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "# Define the embedding model and configuration\n",
        "embeddings_model_name = 'dunzhang/stella_en_1.5B_v5'\n",
        "embeddings_model_kwargs = {\"device\":'cpu',                 # Load the embedding on CPU\n",
        "                'trust_remote_code':True}       # As the model not official code trust it\n",
        "\n",
        "## @Islam, please fix (To-do)\n",
        "# Load the embedding model locally\n",
        "hf_embeddings = HuggingFaceEmbeddings(     # use huggingFace embedding class\n",
        "    model_name = embeddings_model_name,\n",
        "    model_kwargs = embeddings_model_kwargs\n",
        ")\n",
        "\n",
        "vect_embed = hf_embeddings.embed_query(\"Hello world!\")\n",
        "print(\"Embedding vector type: \", type(vect_embed))\n",
        "print(\"Embedding vector length: \", len(vect_embed))\n"
      ],
      "metadata": {
        "id": "kktgKTgOz21J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You could use https://huggingface.co/settings/tokens/new?tokenType=read to obtain the API token with read permission after creating HF account."
      ],
      "metadata": {
        "id": "pKxfKxl_7uRA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "HF_API_KEY = userdata.get('HF_API_KEY')"
      ],
      "metadata": {
        "id": "xaBOR28L7oSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WaEwbwSi4Eio"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "\n",
        "# for now mistral model works but we will need to update\n",
        "# define huggingface generation endpoint\n",
        "hf_llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\", # Model Name\n",
        "    task=\"text-generation\",                       # task as generating a text response\n",
        "    max_new_tokens=150,                           # maximum numbers of generated tokens\n",
        "    do_sample=False,                              # disables sampling\n",
        "    huggingfacehub_api_token=HF_API_KEY           # ðŸ¤— huggingface API token\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rz6gIqVs4ZPU"
      },
      "source": [
        "##### Indexing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "sgPFGFS0Z1XU"
      },
      "outputs": [],
      "source": [
        "import fitz  # PyMuPDF\n",
        "import re\n",
        "import spacy\n",
        "from langchain.schema import Document\n",
        "import logging\n",
        "import os\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
        "\n",
        "# Initialize SpaCy's English model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def extract_text_with_metadata(pdf_path, page_limit=None):\n",
        "    try:\n",
        "        with fitz.open(pdf_path) as file:\n",
        "            logging.info(\"PDF opened successfully.\")\n",
        "\n",
        "            filename = os.path.basename(pdf_path)\n",
        "            title, _ = os.path.splitext(filename)\n",
        "\n",
        "            #return \"\\n\".join(page.get_text() for page in file)\n",
        "            pages_text = []\n",
        "            for page_num in range(len(file) if page_limit is None else min(len(file), page_limit)):\n",
        "                page = file.load_page(page_num)\n",
        "                text = page.get_text(\"text\").strip()\n",
        "                pages_text.append({\n",
        "                    'page_number': page_num + 1,  # 1-indexed\n",
        "                    'text': text\n",
        "                })\n",
        "            return title, pages_text\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error extracting text from PDF: {e}\")\n",
        "        return \"Untitled Document\", []\n",
        "\n",
        "def clean_extracted_text(raw_text):\n",
        "    cleaned_pages = []\n",
        "\n",
        "    for page in raw_text:\n",
        "        text = page['text']\n",
        "        page_number = page['page_number']\n",
        "        lines = text.split('\\n')\n",
        "        cleaned_lines = []\n",
        "\n",
        "        for line in lines:\n",
        "            # Skip lines with DOIs\n",
        "            if re.search(r'doi:\\s*\\d+\\.\\d+/\\S+', line, re.IGNORECASE):\n",
        "                continue\n",
        "            # Skip lines starting with numbers followed by ':' or '.'\n",
        "            if re.match(r'^\\d+[:.]', line):\n",
        "                continue\n",
        "            # Skip lines containing email addresses\n",
        "            if re.search(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', line):\n",
        "                continue\n",
        "            # Skip lines containing specific keywords\n",
        "            if re.search(r'(Received|revised|accepted|Keywords|Abstract)', line, re.IGNORECASE):\n",
        "                continue\n",
        "            # Skip lines that are standalone numbers\n",
        "            if re.match(r'^\\d+$', line.strip()):\n",
        "                continue\n",
        "            # Optionally skip very short lines\n",
        "            # if len(line.strip()) < 20:\n",
        "                # continue\n",
        "            # Fix hyphenated line breaks (e.g., \"retrieval-\\n augmented\")\n",
        "            line = re.sub(r'(\\w+)-\\s*\\n\\s*(\\w+)', r'\\1\\2', line)\n",
        "            # Replace multiple spaces with a single space\n",
        "            line = re.sub(r'\\s+', ' ', line)\n",
        "            cleaned_lines.append(line.strip())\n",
        "\n",
        "        # Preserve paragraph breaks by joining with double newline\n",
        "        cleaned_text = '\\n'.join(cleaned_lines) # can give \\n\\n\n",
        "        cleaned_pages.append({\n",
        "            'page_number': page_number,\n",
        "            'cleaned_text': cleaned_text\n",
        "        })\n",
        "\n",
        "    return cleaned_pages\n",
        "\n",
        "def tokenize_sentences(cleaned_pages):\n",
        "    tokenized_pages = []\n",
        "\n",
        "    for page in cleaned_pages:\n",
        "        text = page['cleaned_text']\n",
        "        page_number = page['page_number']\n",
        "        doc = nlp(text)\n",
        "        sentences = [sent.text.strip() for sent in doc.sents if len(sent.text.strip()) > 20]\n",
        "        tokenized_pages.append({\n",
        "            'page_number': page_number,\n",
        "            'sentences': sentences\n",
        "        })\n",
        "    return tokenized_pages\n",
        "\n",
        "def create_chunks(tokenized_pages, title):\n",
        "    documents = []\n",
        "    for page in tokenized_pages:\n",
        "        page_number = page['page_number']\n",
        "        for sentence in page['sentences']:\n",
        "            doc = Document(\n",
        "                page_content=sentence,\n",
        "                metadata={\n",
        "                    'page_number': page_number,\n",
        "                    'source': title\n",
        "                }\n",
        "            )\n",
        "            documents.append(doc)\n",
        "    return documents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLmMzZ4cmCPk"
      },
      "outputs": [],
      "source": [
        "pdf_path = '20241015_MISSION_KI_Glossar_v1.0 en.pdf'\n",
        "\n",
        "title, pages_raw_text = extract_text_with_metadata(pdf_path) #2nd argument is for page limit(optional)\n",
        "\n",
        "cleaned_pages = clean_extracted_text(pages_raw_text)\n",
        "\n",
        "# Tokenize into sentences\n",
        "tokenized_pages = tokenize_sentences(cleaned_pages)\n",
        "\n",
        "# Structure sentences using LangChain's Document with metadata\n",
        "chunks = create_chunks(tokenized_pages, title)\n",
        "\n",
        "print(f\"Total #chunks: {len(chunks)}\")\n",
        "\n",
        "print(\"\\nFirst 5 and Last 3 Chunks with Metadata:\")\n",
        "for idx, chunk in enumerate(chunks[:5] + chunks[-3:], 1):\n",
        "    print(f\"{idx}: \")\n",
        "    print('===================')\n",
        "    print(chunk.page_content)\n",
        "    print('===================')\n",
        "    print(f\"   Metadata: Title='{chunk.metadata['source']}', Page Number={chunk.metadata.get('page_number', 'N/A')}, Start Index={chunk.metadata.get('start_index', 'N/A')}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTHf067U2_a8"
      },
      "source": [
        "##### Embed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLyW9og61JFW"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from langchain_chroma import Chroma                    # This is the database we will use to store the embeddings\n",
        "\n",
        "vectorstore = None\n",
        "Chroma().delete_collection()\n",
        "for split in tqdm(chunks, colour=\"green\"):\n",
        "    if vectorstore:\n",
        "      vectorstore.add_documents([split])                # Add new split to the vectorstore\n",
        "    else:\n",
        "      vectorstore = Chroma.from_documents(              # Generate the vectorstore with first split\n",
        "                documents=[split],\n",
        "                embedding=hf_embeddings,\n",
        "                collection_metadata={\"hnsw:space\": \"cosine\"}     # by default L2 distance measured\n",
        "                )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Retrieval"
      ],
      "metadata": {
        "id": "zCoOVV0-Ajl_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# as we need to make sure that we will not use the full context of the model\n",
        "# we use k but we can use high numbers as sentence is small\n",
        "retriever = vectorstore.as_retriever(\n",
        "      search_type=\"similarity_score_threshold\",           # similarity function\n",
        "      search_kwargs={\"score_threshold\": 0.5,\n",
        "                     'k':10}                     # number of retrieved relevant documents\n",
        "    )"
      ],
      "metadata": {
        "id": "LBIm20gHAoGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtMP2UrSDRHs"
      },
      "source": [
        "#### using context in prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgl6Ly0wTqFi"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "user_example_input = \"User information\n",
        "Characteristics of an â†’AI system with regard to the quality of information, interaction and operation by a\n",
        "user, including knowledge of the involvement of AI, barriers, and the quality of the user experience\"\n",
        "user_example_output = 'User Information is part AI system, \"\n",
        "\n",
        "context = \"Interpretability\n",
        "Property of an â†’ AI model that its model parameters, weights or other (mathematical) properties are as\n",
        "directly comprehensible as possible and directly understandable for specialist personnel.\"\n",
        "\n",
        "# prepare prompt\n",
        "template = \"\"\"Use the following 3 pieces of context to answer the question at the end.\n",
        "Use three sentences maximum and keep the answer as concise as possible.\n",
        "Say I don't know when you need to.\n",
        "{context}\n",
        "Question: {question}\n",
        "Helpful Answer:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "# Function to format chunks into context\n",
        "def format_docs(top_k_chunks):\n",
        "    return \"\\n\\n\".join(chunk.page_content for chunk in top_k_chunks)\n",
        "\n",
        "additional_context = \"Requirement refers to concepts such as Fairness and Explainability.\"\n",
        "\n",
        "def add_context(retrived_docs):\n",
        "    additional_context = \"Requirement refers to concepts such as Fairness and Explainability.\"\n",
        "\n",
        "    return additional_context + '\\n\\n---------\\n\\n' + retrived_docs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZ4_Y9iiuGNd"
      },
      "outputs": [],
      "source": [
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.schema import StrOutputParser\n",
        "\n",
        "simple_rag_chain = (\n",
        " {  \"context\": add_context | retriever | format_docs | add_context,\n",
        "    \"question\":RunnablePassthrough()}\n",
        " | prompt                                 # build the prompt\n",
        " | hf_llm                                 # llm for generation\n",
        " | StrOutputParser()                      # collect the response text\n",
        ")\n",
        "\n",
        "# Display response\n",
        "print(\"Generated Response:\")\n",
        "print(simple_rag_chain.invoke(user_query))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEeldQWzhGqh"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# prepare prompt\n",
        "template = \"\"\"Use the following 3 pieces of context to answer the question at the end.\n",
        "Use three sentences maximum and keep the answer as concise as possible.\n",
        "Say I don't know when you need to.\n",
        "{context}\n",
        "Question: {question}\n",
        "new_context: {additional_context}\n",
        "Helpful Answer:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "# Function to format chunks into context\n",
        "def format_docs(top_k_chunks):\n",
        "    return \"\\n\\n\".join(chunk.page_content for chunk in top_k_chunks)\n",
        "\n",
        "additional_context = \"Requirement refers to concepts such as Fairness and Explainability.\"\n",
        "\n",
        "def add_context(additional_context):\n",
        "    return \"Requirement refers to concepts such as Fairness and Explainability.\"\n",
        "\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.schema import StrOutputParser\n",
        "\n",
        "simple_rag_chain = (\n",
        " {  \"context\": retriever | format_docs,\n",
        "    \"question\":RunnablePassthrough()}\n",
        " | prompt                                 # build the prompt\n",
        " | hf_llm                                 # llm for generation\n",
        " | StrOutputParser()                      # collect the response text\n",
        ")\n",
        "\n",
        "# Display response\n",
        "print(\"Generated Response:\")\n",
        "print(simple_rag_chain.invoke(user_query, additional_context))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}